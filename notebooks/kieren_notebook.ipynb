{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44cdc368",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35f98a0a-506d-4df1-b671-93a96a9cb7de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T09:22:11.340871Z",
     "iopub.status.busy": "2025-08-25T09:22:11.340302Z",
     "iopub.status.idle": "2025-08-25T09:22:10.413365Z",
     "shell.execute_reply": "2025-08-25T09:22:10.412175Z",
     "shell.execute_reply.started": "2025-08-25T09:22:11.340823Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root (parent of \"electricity\") to sys.path\n",
    "\n",
    "sys.path.append(\"..\")   # go up one level to project root\n",
    "\n",
    "\n",
    "from electricity.load import Loading\n",
    "from electricity.preprocessing import Preprocessor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8b1ec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Loading(filepath=\"complete_dataset.csv\", return_X_y=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56494b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load the dataframe\n",
    "df = loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4fcf42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RRP</th>\n",
       "      <th>school_day</th>\n",
       "      <th>holiday</th>\n",
       "      <th>RRP_t_minus_1</th>\n",
       "      <th>demand_t_minus_1</th>\n",
       "      <th>min_temperature_t_minus_1</th>\n",
       "      <th>max_temperature_t_minus_1</th>\n",
       "      <th>solar_exposure_t_minus_1</th>\n",
       "      <th>rainfall_t_minus_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-02</th>\n",
       "      <td>33.138988</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.633696</td>\n",
       "      <td>99635.030</td>\n",
       "      <td>13.3</td>\n",
       "      <td>26.9</td>\n",
       "      <td>23.6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03</th>\n",
       "      <td>34.564855</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.138988</td>\n",
       "      <td>129606.010</td>\n",
       "      <td>15.4</td>\n",
       "      <td>38.8</td>\n",
       "      <td>26.8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-04</th>\n",
       "      <td>25.005560</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34.564855</td>\n",
       "      <td>142300.540</td>\n",
       "      <td>20.0</td>\n",
       "      <td>38.2</td>\n",
       "      <td>26.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-05</th>\n",
       "      <td>26.724176</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.005560</td>\n",
       "      <td>104330.715</td>\n",
       "      <td>16.3</td>\n",
       "      <td>21.4</td>\n",
       "      <td>25.2</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-06</th>\n",
       "      <td>31.282311</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.724176</td>\n",
       "      <td>118132.200</td>\n",
       "      <td>15.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>30.7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-02</th>\n",
       "      <td>-6.076028</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34.654671</td>\n",
       "      <td>106641.790</td>\n",
       "      <td>9.4</td>\n",
       "      <td>19.5</td>\n",
       "      <td>21.2</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-03</th>\n",
       "      <td>-1.983471</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.076028</td>\n",
       "      <td>99585.835</td>\n",
       "      <td>12.8</td>\n",
       "      <td>26.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-04</th>\n",
       "      <td>25.008614</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.983471</td>\n",
       "      <td>92277.025</td>\n",
       "      <td>17.4</td>\n",
       "      <td>29.4</td>\n",
       "      <td>19.8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-05</th>\n",
       "      <td>36.764701</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.008614</td>\n",
       "      <td>94081.565</td>\n",
       "      <td>13.5</td>\n",
       "      <td>29.5</td>\n",
       "      <td>8.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-06</th>\n",
       "      <td>75.771059</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36.764701</td>\n",
       "      <td>113610.030</td>\n",
       "      <td>9.1</td>\n",
       "      <td>12.7</td>\n",
       "      <td>7.3</td>\n",
       "      <td>12.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2105 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  RRP  school_day  holiday  RRP_t_minus_1  demand_t_minus_1  \\\n",
       "date                                                                          \n",
       "2015-01-02  33.138988           0        0      25.633696         99635.030   \n",
       "2015-01-03  34.564855           0        0      33.138988        129606.010   \n",
       "2015-01-04  25.005560           0        0      34.564855        142300.540   \n",
       "2015-01-05  26.724176           0        0      25.005560        104330.715   \n",
       "2015-01-06  31.282311           0        0      26.724176        118132.200   \n",
       "...               ...         ...      ...            ...               ...   \n",
       "2020-10-02  -6.076028           0        0      34.654671        106641.790   \n",
       "2020-10-03  -1.983471           0        0      -6.076028         99585.835   \n",
       "2020-10-04  25.008614           0        0      -1.983471         92277.025   \n",
       "2020-10-05  36.764701           0        0      25.008614         94081.565   \n",
       "2020-10-06  75.771059           0        0      36.764701        113610.030   \n",
       "\n",
       "            min_temperature_t_minus_1  max_temperature_t_minus_1  \\\n",
       "date                                                               \n",
       "2015-01-02                       13.3                       26.9   \n",
       "2015-01-03                       15.4                       38.8   \n",
       "2015-01-04                       20.0                       38.2   \n",
       "2015-01-05                       16.3                       21.4   \n",
       "2015-01-06                       15.0                       22.0   \n",
       "...                               ...                        ...   \n",
       "2020-10-02                        9.4                       19.5   \n",
       "2020-10-03                       12.8                       26.0   \n",
       "2020-10-04                       17.4                       29.4   \n",
       "2020-10-05                       13.5                       29.5   \n",
       "2020-10-06                        9.1                       12.7   \n",
       "\n",
       "            solar_exposure_t_minus_1  rainfall_t_minus_1  \n",
       "date                                                      \n",
       "2015-01-02                      23.6                 0.0  \n",
       "2015-01-03                      26.8                 0.0  \n",
       "2015-01-04                      26.5                 0.0  \n",
       "2015-01-05                      25.2                 4.2  \n",
       "2015-01-06                      30.7                 0.0  \n",
       "...                              ...                 ...  \n",
       "2020-10-02                      21.2                 1.8  \n",
       "2020-10-03                      22.0                 0.0  \n",
       "2020-10-04                      19.8                 0.0  \n",
       "2020-10-05                       8.4                 0.0  \n",
       "2020-10-06                       7.3                12.8  \n",
       "\n",
       "[2105 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf41c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4e7b283",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=\"RRP\")\n",
    "y = df[\"RRP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a3c7e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2105, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec3ea2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test: (N, T, F) windows for your RNN\n",
    "# date_array: shape (N, T) with np.datetime64 for each timestep (preferred)\n",
    "pre = Preprocessor(filepath=\".\", date_col=\"date\", target_col=\"RRP\")\n",
    "   # OR: pre.set_rnn_dates(date_feature_index=idx)\n",
    "pipe = pre.build_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3e31636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    font-family: monospace;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td.value pre {\n",
       "    color:rgb(255, 94, 0) !important;\n",
       "    background-color: transparent !important;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;date_features&#x27;, DateCyclicalFeatures()),\n",
       "                (&#x27;pre&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;pipeline-1&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;simpleimputer&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;median&#x27;)),\n",
       "                                                                  (&#x27;robustscaler&#x27;,\n",
       "                                                                   RobustScaler())]),\n",
       "                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7bf857c606a0&gt;),\n",
       "                                                 (&#x27;pipeline-2&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;simpleimputer&#x27;,\n",
       "                                                                   SimpleImputer(fill_value=&#x27;missing&#x27;,\n",
       "                                                                                 strategy=&#x27;constant&#x27;)),\n",
       "                                                                  (&#x27;onehotencoder&#x27;,\n",
       "                                                                   OneHotEncoder(drop=&#x27;first&#x27;,\n",
       "                                                                                 handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                                 sparse_output=False))]),\n",
       "                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7bf857c608e0&gt;)])),\n",
       "                (&#x27;corr_prune&#x27;,\n",
       "                 CorrelationSelector(keep_always=[&#x27;month_sin&#x27;, &#x27;month_cos&#x27;,\n",
       "                                                  &#x27;week_sin&#x27;, &#x27;week_cos&#x27;,\n",
       "                                                  &#x27;doy_sin&#x27;, &#x27;doy_cos&#x27;]))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>Pipeline</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('steps',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">steps&nbsp;</td>\n",
       "            <td class=\"value\">[(&#x27;date_features&#x27;, ...), (&#x27;pre&#x27;, ...), ...]</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('transform_input',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">transform_input&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('memory',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">memory&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">verbose&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>DateCyclicalFeatures</div></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"date_features__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('date_col',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">date_col&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;date&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>pre: ColumnTransformer</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for pre: ColumnTransformer</span></a></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"pre__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('transformers',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">transformers&nbsp;</td>\n",
       "            <td class=\"value\">[(&#x27;pipeline-1&#x27;, ...), (&#x27;pipeline-2&#x27;, ...)]</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('remainder',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">remainder&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;drop&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('sparse_threshold',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">sparse_threshold&nbsp;</td>\n",
       "            <td class=\"value\">0.3</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_jobs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">n_jobs&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('transformer_weights',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">transformer_weights&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">verbose&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose_feature_names_out',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">verbose_feature_names_out&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('force_int_remainder_cols',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">force_int_remainder_cols&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;deprecated&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>pipeline-1</div></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"pre__pipeline-1__\"><pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7bf857c606a0&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>SimpleImputer</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"pre__pipeline-1__simpleimputer__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('missing_values',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">missing_values&nbsp;</td>\n",
       "            <td class=\"value\">nan</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('strategy',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">strategy&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;median&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('fill_value',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">fill_value&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('copy',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">copy&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('add_indicator',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">add_indicator&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('keep_empty_features',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">keep_empty_features&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>RobustScaler</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.preprocessing.RobustScaler.html\">?<span>Documentation for RobustScaler</span></a></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"pre__pipeline-1__robustscaler__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('with_centering',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">with_centering&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('with_scaling',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">with_scaling&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('quantile_range',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">quantile_range&nbsp;</td>\n",
       "            <td class=\"value\">(25.0, ...)</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('copy',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">copy&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('unit_variance',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">unit_variance&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>pipeline-2</div></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"pre__pipeline-2__\"><pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7bf857c608e0&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>SimpleImputer</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"pre__pipeline-2__simpleimputer__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('missing_values',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">missing_values&nbsp;</td>\n",
       "            <td class=\"value\">nan</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('strategy',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">strategy&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;constant&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('fill_value',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">fill_value&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;missing&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('copy',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">copy&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('add_indicator',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">add_indicator&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('keep_empty_features',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">keep_empty_features&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>OneHotEncoder</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"pre__pipeline-2__onehotencoder__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('categories',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">categories&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;auto&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('drop',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">drop&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;first&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('sparse_output',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">sparse_output&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('dtype',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">dtype&nbsp;</td>\n",
       "            <td class=\"value\">&lt;class &#x27;numpy.float64&#x27;&gt;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('handle_unknown',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">handle_unknown&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;ignore&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('min_frequency',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">min_frequency&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_categories',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_categories&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('feature_name_combiner',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">feature_name_combiner&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;concat&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>CorrelationSelector</div></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"corr_prune__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('threshold',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">threshold&nbsp;</td>\n",
       "            <td class=\"value\">0.95</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('keep_always',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">keep_always&nbsp;</td>\n",
       "            <td class=\"value\">[&#x27;month_sin&#x27;, &#x27;month_cos&#x27;, ...]</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">verbose&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling.textContent.trim();\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "</script></body>"
      ],
      "text/plain": [
       "Pipeline(steps=[('date_features', DateCyclicalFeatures()),\n",
       "                ('pre',\n",
       "                 ColumnTransformer(transformers=[('pipeline-1',\n",
       "                                                  Pipeline(steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer(strategy='median')),\n",
       "                                                                  ('robustscaler',\n",
       "                                                                   RobustScaler())]),\n",
       "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x7bf857c606a0>),\n",
       "                                                 ('pipeline-2',\n",
       "                                                  Pipeline(steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer(fill_value='missing',\n",
       "                                                                                 strategy='constant')),\n",
       "                                                                  ('onehotencoder',\n",
       "                                                                   OneHotEncoder(drop='first',\n",
       "                                                                                 handle_unknown='ignore',\n",
       "                                                                                 sparse_output=False))]),\n",
       "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x7bf857c608e0>)])),\n",
       "                ('corr_prune',\n",
       "                 CorrelationSelector(keep_always=['month_sin', 'month_cos',\n",
       "                                                  'week_sin', 'week_cos',\n",
       "                                                  'doy_sin', 'doy_cos']))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d49528e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":mag_right: CorrelationSelector dropped 6 features: ['pipeline-1__week', 'pipeline-1__dayofyear', 'pipeline-1__week_sin', 'pipeline-1__week_cos', 'pipeline-1__doy_sin', 'pipeline-1__doy_cos']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pipe.fit(X)      # -> (N, T, F_out)\n",
    "X_preproc = pipe.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfcb60a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2105, 13)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_preproc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a3841b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = X.join(y)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b20c8ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.6\n",
    "index = round(train_size*df.shape[0])\n",
    "\n",
    "df_train = df.iloc[:index]\n",
    "df_test = df.iloc[index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56f181f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1263, 9)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d90814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple, Sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a79bd31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'RRP'\n",
    "\n",
    "def get_Xi_yi(\n",
    "    dataset: pd.DataFrame,\n",
    "    input_length: int,\n",
    "    output_length: int,\n",
    "    random: bool = True,\n",
    "    start_index: int = None\n",
    ") -> Tuple[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Returns one sequence (X_i, y_i) from the dataset, either randomly or at a specified index.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The dataset to sample from.\n",
    "        input_length (int): Length of input sequence.\n",
    "        output_length (int): Length of output sequence.\n",
    "        random (bool): If True, select randomly. If False, use start_index.\n",
    "        start_index (int, optional): If random is False, use this as the starting index.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame]: A tuple of two dataframes (X_i, y_i)\n",
    "    \"\"\"\n",
    "    first_possible_start = 0\n",
    "    last_possible_start = len(dataset) - (input_length + output_length) + 1\n",
    "\n",
    "    if last_possible_start <= 0:\n",
    "        raise ValueError(\"Not enough data to create a sequence with the given input and output lengths.\")\n",
    "\n",
    "    if random:\n",
    "        random_start = np.random.randint(first_possible_start, last_possible_start)\n",
    "    else:\n",
    "        if start_index is None:\n",
    "            raise ValueError(\"start_index must be provided when random is False.\")\n",
    "        if not (first_possible_start <= start_index < last_possible_start):\n",
    "            raise ValueError(f\"start_index must be in [{first_possible_start}, {last_possible_start-1}]\")\n",
    "        random_start = start_index\n",
    "\n",
    "    X_i = dataset.iloc[random_start:random_start + input_length]\n",
    "    y_i = dataset.iloc[random_start + input_length:\n",
    "                      random_start + input_length + output_length][[TARGET]]\n",
    "\n",
    "    return (X_i, y_i)\n",
    "\n",
    "\n",
    "def get_X_y(\n",
    "    dataset: pd.DataFrame,\n",
    "    input_length: int,\n",
    "    output_length: int,\n",
    "    number_of_sequences: int = None,\n",
    "    random: bool = False\n",
    ") -> Tuple[np.array]:\n",
    "    \"\"\"\n",
    "    Generate X and y based on the number of desired sequences of the given input_length and output_length.\n",
    "    If random is False, sequences are taken sequentially to cover the full dataframe (up to number_of_sequences or max possible).\n",
    "    If number_of_sequences is None, use the maximum possible number of sequences.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): Fold dataframe\n",
    "        input_length (int): Length of each X_i\n",
    "        output_length (int): Length of each y_i\n",
    "        number_of_sequences (int, optional): The number of X_i and y_i pairs to include. If None, use max possible.\n",
    "        random (bool): If True, sample randomly. If False, sample sequentially.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.array]: A tuple of numpy arrays (X, y)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "\n",
    "    first_possible_start = 0\n",
    "    last_possible_start = len(dataset) - (input_length + output_length) + 1\n",
    "\n",
    "    if last_possible_start <= 0:\n",
    "        raise ValueError(\"Not enough data to create sequences with the given input and output lengths.\")\n",
    "\n",
    "    max_possible_sequences = last_possible_start\n",
    "\n",
    "    if number_of_sequences is None:\n",
    "        n_sequences = max_possible_sequences\n",
    "    else:\n",
    "        n_sequences = min(number_of_sequences, max_possible_sequences)\n",
    "\n",
    "    if random:\n",
    "        for i in range(n_sequences):\n",
    "            Xi, yi = get_Xi_yi(dataset, input_length, output_length, random=True)\n",
    "            X.append(Xi)\n",
    "            y.append(yi)\n",
    "    else:\n",
    "        # Sequential, non-overlapping (or overlapping) windows\n",
    "        for idx in range(first_possible_start, first_possible_start + n_sequences):\n",
    "            Xi, yi = get_Xi_yi(dataset, input_length, output_length, random=False, start_index=idx)\n",
    "            X.append(Xi)\n",
    "            y.append(yi)\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db7e4e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = get_X_y(df_train, 60, 1)\n",
    "data_test = get_X_y(df_test, 60, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c873061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train[0]\n",
    "y_train = data_train[1]\n",
    "X_test = data_test[0]\n",
    "y_test = data_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83333eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Notebook Cell 3: Simple model with unified preprocessing (LinearRegression + TS CV) ---\n",
    "from electricity.preprocessing import Preprocessor\n",
    "# Build preprocessing + model pipeline.\n",
    "# Preprocessor will add date/cyclical features, impute/scale/one-hot, and prune highly correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a29a408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "623cb73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-27 14:36:55.101561: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-08-27 14:36:55.102055: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-27 14:36:55.172284: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-27 14:36:57.822353: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-27 14:36:57.823022: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "from electricity.models import _tscv_scores\n",
    "from sklearn.ensemble import RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "806bb095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db8ee3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3657085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "040ae2e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 25.63369643387471, 99635.03, 13.3, 26.9, 23.6, 0.0,\n",
       "        33.13898756122499],\n",
       "       [0, 0, 33.13898756122499, 129606.00999999994, 15.4, 38.8, 26.8,\n",
       "        0.0, 34.56485482908218],\n",
       "       [0, 0, 34.56485482908218, 142300.53999999998, 20.0, 38.2, 26.5,\n",
       "        0.0, 25.00556023842067],\n",
       "       [0, 0, 25.00556023842067, 104330.715, 16.3, 21.4, 25.2, 4.2,\n",
       "        26.72417627793271],\n",
       "       [0, 0, 26.72417627793271, 118132.19999999994, 15.0, 22.0, 30.7,\n",
       "        0.0, 31.282310728612853],\n",
       "       [0, 0, 31.282310728612853, 130672.48499999994, 17.7, 26.0, 31.6,\n",
       "        0.0, 48.31230937899024],\n",
       "       [0, 0, 48.31230937899024, 153514.82, 18.9, 37.4, 20.7, 0.0,\n",
       "        49.11728029878114],\n",
       "       [0, 0, 49.11728029878114, 142015.65500000006, 23.1, 28.2, 13.5,\n",
       "        19.4, 34.490675454596484],\n",
       "       [0, 0, 34.490675454596484, 121801.15499999998, 16.5, 18.0, 3.1,\n",
       "        1.2, 20.229824895097856],\n",
       "       [0, 0, 20.229824895097856, 103043.66000000005, 13.6, 21.7, 5.6,\n",
       "        5.2, 18.23476806038267],\n",
       "       [0, 0, 18.23476806038267, 99865.755, 15.6, 27.5, 29.9, 0.0,\n",
       "        33.69481017437569],\n",
       "       [0, 0, 33.69481017437569, 131261.12500000006, 16.1, 31.3, 31.6,\n",
       "        0.0, 27.537864268645148],\n",
       "       [0, 0, 27.537864268645148, 126527.35999999994, 20.2, 25.6, 4.2,\n",
       "        0.0, 29.018516605170355],\n",
       "       [0, 0, 29.018516605170355, 119741.62, 18.3, 22.7, 15.2, 15.8,\n",
       "        30.936538792523216],\n",
       "       [0, 0, 30.936538792523216, 118411.21999999994, 15.8, 20.0, 14.6,\n",
       "        0.0, 23.01519087693016],\n",
       "       [0, 0, 23.01519087693016, 116690.76499999994, 13.1, 27.3, 30.3,\n",
       "        0.0, 18.99492729239456],\n",
       "       [0, 0, 18.99492729239456, 99371.31, 15.9, 25.0, 27.5, 0.0,\n",
       "        17.008680881521563],\n",
       "       [0, 0, 17.008680881521563, 97728.75, 15.3, 19.5, 23.4, 0.0,\n",
       "        26.927876193873967],\n",
       "       [0, 0, 26.927876193873967, 116883.14999999998, 13.5, 23.6, 29.0,\n",
       "        0.0, 23.07807413769876],\n",
       "       [0, 0, 23.07807413769876, 128968.125, 13.0, 30.4, 19.6, 0.0,\n",
       "        34.44341545994803],\n",
       "       [0, 0, 34.44341545994803, 148702.505, 19.7, 33.1, 25.8, 0.0,\n",
       "        35.230761404105266],\n",
       "       [0, 0, 35.230761404105266, 153232.1, 18.7, 35.8, 23.1, 0.0,\n",
       "        40.60420021586557],\n",
       "       [0, 0, 40.60420021586557, 138095.19999999998, 20.2, 25.0, 28.6,\n",
       "        0.0, 20.922000145472573],\n",
       "       [0, 0, 20.922000145472573, 116310.58999999998, 15.8, 26.3, 30.3,\n",
       "        0.0, 16.541417703813387],\n",
       "       [0, 1, 16.541417703813387, 97959.46000000004, 14.7, 22.0, 25.4,\n",
       "        0.0, 16.936012803090073],\n",
       "       [0, 0, 16.936012803090073, 103769.48000000004, 11.6, 19.8, 19.9,\n",
       "        1.0, 21.20628859561405],\n",
       "       [0, 0, 21.20628859561405, 118393.31, 11.7, 19.1, 17.7, 0.6,\n",
       "        17.02383823700383],\n",
       "       [1, 0, 17.02383823700383, 116763.72499999998, 13.3, 25.4, 27.0,\n",
       "        0.0, 19.624827355577604],\n",
       "       [1, 0, 19.624827355577604, 119313.73, 12.8, 22.2, 28.9, 0.0,\n",
       "        21.029356082124878],\n",
       "       [1, 0, 21.029356082124878, 121018.15, 13.1, 21.0, 27.6, 0.0,\n",
       "        16.538846727752063],\n",
       "       [1, 0, 16.538846727752063, 103822.97, 14.3, 20.2, 26.2, 0.0,\n",
       "        14.578844423632775],\n",
       "       [1, 0, 14.578844423632775, 100178.84, 13.2, 19.0, 17.3, 9.2,\n",
       "        22.27003219492855],\n",
       "       [1, 0, 22.27003219492855, 118694.15999999996, 13.5, 21.1, 23.9,\n",
       "        3.8, 27.00383232545409],\n",
       "       [1, 0, 27.00383232545409, 122880.13, 16.1, 20.0, 22.7, 0.0,\n",
       "        27.809134200122436],\n",
       "       [1, 0, 27.809134200122436, 117398.03000000006, 14.8, 19.0, 21.2,\n",
       "        0.0, 31.38457073747281],\n",
       "       [1, 0, 31.38457073747281, 122279.42500000006, 13.3, 24.2, 28.8,\n",
       "        0.0, 38.84018027030393],\n",
       "       [1, 0, 38.84018027030393, 141837.005, 14.5, 35.7, 28.4, 0.0,\n",
       "        28.761706782507872],\n",
       "       [1, 0, 28.761706782507872, 132138.36500000002, 18.8, 35.0, 16.1,\n",
       "        0.0, 23.97410660449817],\n",
       "       [1, 0, 23.97410660449817, 112108.215, 18.9, 21.5, 16.6, 0.0,\n",
       "        28.90913470554987],\n",
       "       [1, 0, 28.90913470554987, 126813.07500000006, 13.6, 28.3, 28.6,\n",
       "        0.0, 37.522599975546186],\n",
       "       [1, 0, 37.522599975546186, 145948.56499999994, 16.4, 27.8, 27.7,\n",
       "        0.0, 32.655454296177695],\n",
       "       [1, 0, 32.655454296177695, 149322.85, 18.1, 26.8, 24.1, 0.0,\n",
       "        26.576634936173647],\n",
       "       [1, 0, 26.576634936173647, 131376.78000000006, 17.8, 21.9, 23.8,\n",
       "        0.0, 18.736970611657387],\n",
       "       [1, 0, 18.736970611657387, 136070.62, 16.1, 32.4, 14.9, 0.0,\n",
       "        20.24010323118855],\n",
       "       [1, 0, 20.24010323118855, 119692.025, 19.0, 25.1, 20.8, 19.6,\n",
       "        24.58418694100164],\n",
       "       [1, 0, 24.58418694100164, 125287.70999999998, 18.2, 34.4, 25.7,\n",
       "        0.4, 31.949040602087678],\n",
       "       [1, 0, 31.949040602087678, 131832.62999999998, 17.4, 24.6, 26.3,\n",
       "        0.0, 30.25014938549424],\n",
       "       [1, 0, 30.25014938549424, 127666.01, 17.6, 20.6, 8.3, 0.0,\n",
       "        30.23579275091724],\n",
       "       [1, 0, 30.23579275091724, 125972.765, 17.7, 21.2, 7.8, 0.0,\n",
       "        33.59896067918086],\n",
       "       [1, 0, 33.59896067918086, 139399.68999999994, 18.2, 26.9, 24.7,\n",
       "        0.0, 33.538671531902025],\n",
       "       [1, 0, 33.538671531902025, 139465.675, 17.4, 25.3, 24.7, 0.0,\n",
       "        31.25281983934463],\n",
       "       [1, 0, 31.25281983934463, 136283.03, 18.1, 36.1, 20.0, 0.0,\n",
       "        32.40972720732],\n",
       "       [1, 0, 32.40972720732, 138016.34999999998, 20.3, 35.3, 11.7, 1.6,\n",
       "        28.41722912763491],\n",
       "       [1, 0, 28.41722912763491, 135255.815, 21.1, 29.3, 4.7, 0.0,\n",
       "        26.55901189774805],\n",
       "       [1, 0, 26.55901189774805, 120263.51499999994, 15.9, 20.1, 21.4,\n",
       "        3.2, 33.32324958709357],\n",
       "       [1, 0, 33.32324958709357, 130062.75499999998, 14.0, 27.0, 24.1,\n",
       "        0.0, 39.086578343854896],\n",
       "       [1, 0, 39.086578343854896, 131525.85499999998, 16.0, 23.2, 18.5,\n",
       "        0.0, 36.33901155418428],\n",
       "       [1, 0, 36.33901155418428, 131069.4, 14.8, 24.3, 21.7, 0.0,\n",
       "        25.1431307255607],\n",
       "       [1, 0, 25.1431307255607, 116275.73, 17.9, 32.3, 6.9, 0.0,\n",
       "        24.586148145738974],\n",
       "       [1, 0, 24.586148145738974, 103462.78, 15.2, 20.8, 23.3, 15.8,\n",
       "        33.1665262325577]], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a35fae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(np.float32)\n",
    "y_train  = np.asarray(y_train).astype(np.float32)\n",
    "\n",
    "X_test = X_test.astype(np.float32)\n",
    "y_test  = np.asarray(y_test).astype(np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef969509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-27 14:36:59.019818: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 81ms/step - loss: 5069.3252 - mae: 59.6456 - val_loss: 17251.7734 - val_mae: 95.1470\n",
      "Epoch 2/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 5064.4229 - mae: 59.6049 - val_loss: 17245.9258 - val_mae: 95.1163\n",
      "Epoch 3/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 5060.7954 - mae: 59.5742 - val_loss: 17240.1133 - val_mae: 95.0857\n",
      "Epoch 4/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - loss: 5057.1328 - mae: 59.5435 - val_loss: 17234.2637 - val_mae: 95.0550\n",
      "Epoch 5/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - loss: 5053.5352 - mae: 59.5134 - val_loss: 17228.5527 - val_mae: 95.0249\n",
      "Epoch 6/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 5049.8940 - mae: 59.4826 - val_loss: 17222.6777 - val_mae: 94.9940\n",
      "Epoch 7/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - loss: 5046.2803 - mae: 59.4522 - val_loss: 17216.9453 - val_mae: 94.9638\n",
      "Epoch 8/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 5042.6514 - mae: 59.4220 - val_loss: 17211.1875 - val_mae: 94.9335\n",
      "Epoch 9/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 76ms/step - loss: 5039.0391 - mae: 59.3913 - val_loss: 17205.3320 - val_mae: 94.9027\n",
      "Epoch 10/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - loss: 5035.4214 - mae: 59.3608 - val_loss: 17199.5879 - val_mae: 94.8724\n",
      "Epoch 11/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 5031.8115 - mae: 59.3307 - val_loss: 17193.8574 - val_mae: 94.8422\n",
      "Epoch 12/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 5028.2070 - mae: 59.3002 - val_loss: 17188.0723 - val_mae: 94.8117\n",
      "Epoch 13/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 5024.6084 - mae: 59.2697 - val_loss: 17182.2734 - val_mae: 94.7811\n",
      "Epoch 14/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - loss: 5020.9756 - mae: 59.2391 - val_loss: 17176.4785 - val_mae: 94.7505\n",
      "Epoch 15/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 5017.2954 - mae: 59.2081 - val_loss: 17170.5605 - val_mae: 94.7193\n",
      "Epoch 16/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 5013.6221 - mae: 59.1771 - val_loss: 17164.7246 - val_mae: 94.6885\n",
      "Epoch 17/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 5009.9902 - mae: 59.1463 - val_loss: 17158.9102 - val_mae: 94.6578\n",
      "Epoch 18/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 5006.4082 - mae: 59.1160 - val_loss: 17153.1895 - val_mae: 94.6276\n",
      "Epoch 19/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 5002.8574 - mae: 59.0861 - val_loss: 17147.5391 - val_mae: 94.5977\n",
      "Epoch 20/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 4999.2710 - mae: 59.0554 - val_loss: 17141.7109 - val_mae: 94.5669\n",
      "Epoch 21/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4995.6738 - mae: 59.0255 - val_loss: 17136.0977 - val_mae: 94.5372\n",
      "Epoch 22/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4992.0854 - mae: 58.9948 - val_loss: 17130.2266 - val_mae: 94.5061\n",
      "Epoch 23/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 4988.4941 - mae: 58.9644 - val_loss: 17124.5469 - val_mae: 94.4761\n",
      "Epoch 24/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - loss: 4984.9585 - mae: 58.9343 - val_loss: 17118.8066 - val_mae: 94.4457\n",
      "Epoch 25/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 4981.3472 - mae: 58.9033 - val_loss: 17112.9473 - val_mae: 94.4147\n",
      "Epoch 26/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - loss: 4977.7578 - mae: 58.8731 - val_loss: 17107.3047 - val_mae: 94.3848\n",
      "Epoch 27/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4974.1909 - mae: 58.8428 - val_loss: 17101.5469 - val_mae: 94.3543\n",
      "Epoch 28/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 4970.6289 - mae: 58.8126 - val_loss: 17095.8594 - val_mae: 94.3241\n",
      "Epoch 29/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - loss: 4967.0415 - mae: 58.7822 - val_loss: 17090.0957 - val_mae: 94.2936\n",
      "Epoch 30/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 4963.4736 - mae: 58.7513 - val_loss: 17084.2910 - val_mae: 94.2628\n",
      "Epoch 31/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - loss: 4959.8926 - mae: 58.7214 - val_loss: 17078.6934 - val_mae: 94.2331\n",
      "Epoch 32/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 4956.3784 - mae: 58.6913 - val_loss: 17072.9766 - val_mae: 94.2028\n",
      "Epoch 33/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - loss: 4952.7988 - mae: 58.6606 - val_loss: 17067.1738 - val_mae: 94.1720\n",
      "Epoch 34/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 4949.2134 - mae: 58.6301 - val_loss: 17061.4473 - val_mae: 94.1415\n",
      "Epoch 35/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4945.6299 - mae: 58.5997 - val_loss: 17055.7324 - val_mae: 94.1112\n",
      "Epoch 36/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4941.9995 - mae: 58.5686 - val_loss: 17049.8301 - val_mae: 94.0798\n",
      "Epoch 37/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4938.4414 - mae: 58.5383 - val_loss: 17044.2129 - val_mae: 94.0500\n",
      "Epoch 38/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4934.9067 - mae: 58.5083 - val_loss: 17038.5605 - val_mae: 94.0199\n",
      "Epoch 39/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4931.3711 - mae: 58.4779 - val_loss: 17032.7988 - val_mae: 93.9893\n",
      "Epoch 40/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 4927.6890 - mae: 58.4466 - val_loss: 17026.8711 - val_mae: 93.9577\n",
      "Epoch 41/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4924.0908 - mae: 58.4155 - val_loss: 17021.0488 - val_mae: 93.9267\n",
      "Epoch 42/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - loss: 4920.5156 - mae: 58.3850 - val_loss: 17015.3730 - val_mae: 93.8965\n",
      "Epoch 43/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 77ms/step - loss: 4917.0127 - mae: 58.3551 - val_loss: 17009.7871 - val_mae: 93.8668\n",
      "Epoch 44/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 4913.4678 - mae: 58.3247 - val_loss: 17004.0332 - val_mae: 93.8361\n",
      "Epoch 45/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 4909.9014 - mae: 58.2942 - val_loss: 16998.3105 - val_mae: 93.8056\n",
      "Epoch 46/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 4906.3447 - mae: 58.2637 - val_loss: 16992.5859 - val_mae: 93.7751\n",
      "Epoch 47/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 4902.8125 - mae: 58.2332 - val_loss: 16986.8652 - val_mae: 93.7446\n",
      "Epoch 48/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4899.2666 - mae: 58.2028 - val_loss: 16981.2148 - val_mae: 93.7145\n",
      "Epoch 49/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 4895.7280 - mae: 58.1725 - val_loss: 16975.5234 - val_mae: 93.6841\n",
      "Epoch 50/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - loss: 4892.1704 - mae: 58.1416 - val_loss: 16969.6797 - val_mae: 93.6529\n",
      "Epoch 51/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4888.5918 - mae: 58.1110 - val_loss: 16964.0195 - val_mae: 93.6227\n",
      "Epoch 52/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4885.1182 - mae: 58.0808 - val_loss: 16958.3809 - val_mae: 93.5926\n",
      "Epoch 53/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 4881.6187 - mae: 58.0510 - val_loss: 16952.8086 - val_mae: 93.5628\n",
      "Epoch 54/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 4878.1567 - mae: 58.0213 - val_loss: 16947.2109 - val_mae: 93.5329\n",
      "Epoch 55/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4874.6001 - mae: 57.9907 - val_loss: 16941.4570 - val_mae: 93.5021\n",
      "Epoch 56/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4871.1123 - mae: 57.9605 - val_loss: 16935.8535 - val_mae: 93.4721\n",
      "Epoch 57/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4867.5957 - mae: 57.9301 - val_loss: 16930.1738 - val_mae: 93.4417\n",
      "Epoch 58/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - loss: 4864.1025 - mae: 57.9003 - val_loss: 16924.6035 - val_mae: 93.4119\n",
      "Epoch 59/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4860.6182 - mae: 57.8698 - val_loss: 16918.8867 - val_mae: 93.3813\n",
      "Epoch 60/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4857.1182 - mae: 57.8394 - val_loss: 16913.2539 - val_mae: 93.3512\n",
      "Epoch 61/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4853.6348 - mae: 57.8094 - val_loss: 16907.6348 - val_mae: 93.3211\n",
      "Epoch 62/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4850.1147 - mae: 57.7791 - val_loss: 16901.9297 - val_mae: 93.2905\n",
      "Epoch 63/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 4846.6147 - mae: 57.7485 - val_loss: 16896.2559 - val_mae: 93.2601\n",
      "Epoch 64/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 4843.1221 - mae: 57.7188 - val_loss: 16890.7363 - val_mae: 93.2305\n",
      "Epoch 65/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - loss: 4839.6602 - mae: 57.6885 - val_loss: 16885.0391 - val_mae: 93.1999\n",
      "Epoch 66/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4836.1602 - mae: 57.6581 - val_loss: 16879.3965 - val_mae: 93.1696\n",
      "Epoch 67/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4832.6709 - mae: 57.6282 - val_loss: 16873.8320 - val_mae: 93.1398\n",
      "Epoch 68/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4829.2349 - mae: 57.5979 - val_loss: 16868.1621 - val_mae: 93.1093\n",
      "Epoch 69/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 4825.7397 - mae: 57.5677 - val_loss: 16862.5859 - val_mae: 93.0794\n",
      "Epoch 70/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 4822.2646 - mae: 57.5373 - val_loss: 16856.8965 - val_mae: 93.0488\n",
      "Epoch 71/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 4818.7529 - mae: 57.5070 - val_loss: 16851.2578 - val_mae: 93.0185\n",
      "Epoch 72/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4815.2681 - mae: 57.4767 - val_loss: 16845.6133 - val_mae: 92.9882\n",
      "Epoch 73/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 4811.8032 - mae: 57.4463 - val_loss: 16839.9785 - val_mae: 92.9578\n",
      "Epoch 74/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4808.3379 - mae: 57.4163 - val_loss: 16834.4434 - val_mae: 92.9281\n",
      "Epoch 75/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - loss: 4804.8760 - mae: 57.3858 - val_loss: 16828.7266 - val_mae: 92.8973\n",
      "Epoch 76/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - loss: 4801.3823 - mae: 57.3560 - val_loss: 16823.2344 - val_mae: 92.8677\n",
      "Epoch 77/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - loss: 4797.9692 - mae: 57.3257 - val_loss: 16817.6016 - val_mae: 92.8374\n",
      "Epoch 78/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 4794.5068 - mae: 57.2957 - val_loss: 16812.0410 - val_mae: 92.8074\n",
      "Epoch 79/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 4791.0977 - mae: 57.2655 - val_loss: 16806.4199 - val_mae: 92.7772\n",
      "Epoch 80/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 4787.6401 - mae: 57.2357 - val_loss: 16800.9102 - val_mae: 92.7475\n",
      "Epoch 81/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - loss: 4784.1777 - mae: 57.2056 - val_loss: 16795.3047 - val_mae: 92.7172\n",
      "Epoch 82/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4780.7021 - mae: 57.1752 - val_loss: 16789.6113 - val_mae: 92.6865\n",
      "Epoch 83/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 4777.1777 - mae: 57.1443 - val_loss: 16783.8770 - val_mae: 92.6556\n",
      "Epoch 84/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 4773.6992 - mae: 57.1141 - val_loss: 16778.3613 - val_mae: 92.6258\n",
      "Epoch 85/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 4770.2827 - mae: 57.0840 - val_loss: 16772.7383 - val_mae: 92.5955\n",
      "Epoch 86/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 4766.8335 - mae: 57.0533 - val_loss: 16767.1016 - val_mae: 92.5650\n",
      "Epoch 87/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 4763.3989 - mae: 57.0234 - val_loss: 16761.5781 - val_mae: 92.5352\n",
      "Epoch 88/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 4759.9780 - mae: 56.9934 - val_loss: 16756.0000 - val_mae: 92.5050\n",
      "Epoch 89/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - loss: 4756.5098 - mae: 56.9629 - val_loss: 16750.3262 - val_mae: 92.4744\n",
      "Epoch 90/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - loss: 4752.9897 - mae: 56.9323 - val_loss: 16744.6680 - val_mae: 92.4438\n",
      "Epoch 91/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 4749.5029 - mae: 56.9016 - val_loss: 16739.0137 - val_mae: 92.4132\n",
      "Epoch 92/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 4746.0693 - mae: 56.8715 - val_loss: 16733.4805 - val_mae: 92.3833\n",
      "Epoch 93/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - loss: 4742.6689 - mae: 56.8418 - val_loss: 16727.9707 - val_mae: 92.3534\n",
      "Epoch 94/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 4739.2065 - mae: 56.8111 - val_loss: 16722.2812 - val_mae: 92.3226\n",
      "Epoch 95/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - loss: 4735.7803 - mae: 56.7809 - val_loss: 16716.7422 - val_mae: 92.2926\n",
      "Epoch 96/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 4732.3755 - mae: 56.7509 - val_loss: 16711.1367 - val_mae: 92.2622\n",
      "Epoch 97/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4728.6499 - mae: 56.7179 - val_loss: 16704.9648 - val_mae: 92.2288\n",
      "Epoch 98/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 4725.0708 - mae: 56.6868 - val_loss: 16699.3457 - val_mae: 92.1983\n",
      "Epoch 99/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 4721.6484 - mae: 56.6563 - val_loss: 16693.7461 - val_mae: 92.1680\n",
      "Epoch 100/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4718.2739 - mae: 56.6265 - val_loss: 16688.2773 - val_mae: 92.1383\n",
      "Epoch 101/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4714.8877 - mae: 56.5968 - val_loss: 16682.8184 - val_mae: 92.1086\n",
      "Epoch 102/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4711.5151 - mae: 56.5668 - val_loss: 16677.2637 - val_mae: 92.0785\n",
      "Epoch 103/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 4708.1348 - mae: 56.5372 - val_loss: 16671.8184 - val_mae: 92.0489\n",
      "Epoch 104/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - loss: 4704.7651 - mae: 56.5070 - val_loss: 16666.2656 - val_mae: 92.0188\n",
      "Epoch 105/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 4701.4048 - mae: 56.4772 - val_loss: 16660.8066 - val_mae: 91.9891\n",
      "Epoch 106/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4698.0410 - mae: 56.4478 - val_loss: 16655.3633 - val_mae: 91.9595\n",
      "Epoch 107/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4694.5747 - mae: 56.4167 - val_loss: 16649.6094 - val_mae: 91.9282\n",
      "Epoch 108/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4691.1870 - mae: 56.3865 - val_loss: 16644.1250 - val_mae: 91.8984\n",
      "Epoch 109/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 4687.8228 - mae: 56.3570 - val_loss: 16638.7266 - val_mae: 91.8690\n",
      "Epoch 110/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 4684.4888 - mae: 56.3276 - val_loss: 16633.2949 - val_mae: 91.8394\n",
      "Epoch 111/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 4681.1064 - mae: 56.2973 - val_loss: 16627.6914 - val_mae: 91.8089\n",
      "Epoch 112/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4677.7412 - mae: 56.2673 - val_loss: 16622.2480 - val_mae: 91.7793\n",
      "Epoch 113/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4674.4136 - mae: 56.2378 - val_loss: 16616.8418 - val_mae: 91.7498\n",
      "Epoch 114/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4671.0435 - mae: 56.2083 - val_loss: 16611.3926 - val_mae: 91.7201\n",
      "Epoch 115/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - loss: 4667.7002 - mae: 56.1782 - val_loss: 16605.8398 - val_mae: 91.6898\n",
      "Epoch 116/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - loss: 4664.3140 - mae: 56.1479 - val_loss: 16600.2754 - val_mae: 91.6595\n",
      "Epoch 117/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4660.8789 - mae: 56.1178 - val_loss: 16594.7539 - val_mae: 91.6293\n",
      "Epoch 118/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4657.4985 - mae: 56.0872 - val_loss: 16589.1680 - val_mae: 91.5989\n",
      "Epoch 119/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4654.1030 - mae: 56.0570 - val_loss: 16583.6289 - val_mae: 91.5686\n",
      "Epoch 120/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 4650.7124 - mae: 56.0268 - val_loss: 16578.1055 - val_mae: 91.5385\n",
      "Epoch 121/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - loss: 4647.3413 - mae: 55.9968 - val_loss: 16572.6309 - val_mae: 91.5086\n",
      "Epoch 122/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4643.9609 - mae: 55.9669 - val_loss: 16567.1289 - val_mae: 91.4785\n",
      "Epoch 123/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 4640.6265 - mae: 55.9369 - val_loss: 16561.6582 - val_mae: 91.4486\n",
      "Epoch 124/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 4637.2671 - mae: 55.9066 - val_loss: 16556.0996 - val_mae: 91.4182\n",
      "Epoch 125/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 4633.8931 - mae: 55.8762 - val_loss: 16550.5723 - val_mae: 91.3879\n",
      "Epoch 126/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 4630.5264 - mae: 55.8466 - val_loss: 16545.1602 - val_mae: 91.3583\n",
      "Epoch 127/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4627.1616 - mae: 55.8162 - val_loss: 16539.5723 - val_mae: 91.3277\n",
      "Epoch 128/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 4623.7646 - mae: 55.7858 - val_loss: 16534.0391 - val_mae: 91.2974\n",
      "Epoch 129/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4620.4404 - mae: 55.7559 - val_loss: 16528.6133 - val_mae: 91.2677\n",
      "Epoch 130/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - loss: 4617.0625 - mae: 55.7259 - val_loss: 16523.1133 - val_mae: 91.2376\n",
      "Epoch 131/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 4613.7041 - mae: 55.6955 - val_loss: 16517.5586 - val_mae: 91.2071\n",
      "Epoch 132/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4610.3232 - mae: 55.6650 - val_loss: 16511.9844 - val_mae: 91.1766\n",
      "Epoch 133/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4606.9092 - mae: 55.6347 - val_loss: 16506.4570 - val_mae: 91.1463\n",
      "Epoch 134/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 4603.5532 - mae: 55.6043 - val_loss: 16500.9395 - val_mae: 91.1160\n",
      "Epoch 135/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4600.2007 - mae: 55.5744 - val_loss: 16495.4863 - val_mae: 91.0861\n",
      "Epoch 136/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4596.8735 - mae: 55.5443 - val_loss: 16489.9727 - val_mae: 91.0558\n",
      "Epoch 137/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 4593.4722 - mae: 55.5136 - val_loss: 16484.3633 - val_mae: 91.0250\n",
      "Epoch 138/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4590.0503 - mae: 55.4825 - val_loss: 16478.7402 - val_mae: 90.9941\n",
      "Epoch 139/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - loss: 4586.5933 - mae: 55.4517 - val_loss: 16473.1211 - val_mae: 90.9632\n",
      "Epoch 140/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 4583.1973 - mae: 55.4208 - val_loss: 16467.5137 - val_mae: 90.9324\n",
      "Epoch 141/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - loss: 4579.8779 - mae: 55.3911 - val_loss: 16462.1699 - val_mae: 90.9030\n",
      "Epoch 142/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 4576.5947 - mae: 55.3617 - val_loss: 16456.8145 - val_mae: 90.8735\n",
      "Epoch 143/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 4573.3013 - mae: 55.3316 - val_loss: 16451.3125 - val_mae: 90.8433\n",
      "Epoch 144/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - loss: 4569.9912 - mae: 55.3018 - val_loss: 16445.9023 - val_mae: 90.8135\n",
      "Epoch 145/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - loss: 4566.6768 - mae: 55.2722 - val_loss: 16440.5703 - val_mae: 90.7841\n",
      "Epoch 146/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4563.3774 - mae: 55.2422 - val_loss: 16435.0527 - val_mae: 90.7537\n",
      "Epoch 147/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - loss: 4560.0059 - mae: 55.2112 - val_loss: 16429.4102 - val_mae: 90.7226\n",
      "Epoch 148/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4556.6919 - mae: 55.1815 - val_loss: 16424.1152 - val_mae: 90.6934\n",
      "Epoch 149/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 4553.4248 - mae: 55.1517 - val_loss: 16418.6895 - val_mae: 90.6635\n",
      "Epoch 150/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - loss: 4550.1147 - mae: 55.1220 - val_loss: 16413.2793 - val_mae: 90.6337\n",
      "Epoch 151/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4546.8135 - mae: 55.0917 - val_loss: 16407.7832 - val_mae: 90.6034\n",
      "Epoch 152/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 4543.4692 - mae: 55.0612 - val_loss: 16402.2715 - val_mae: 90.5729\n",
      "Epoch 153/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 4540.1597 - mae: 55.0317 - val_loss: 16396.9746 - val_mae: 90.5437\n",
      "Epoch 154/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - loss: 4536.9146 - mae: 55.0021 - val_loss: 16391.5625 - val_mae: 90.5138\n",
      "Epoch 155/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 4533.6313 - mae: 54.9717 - val_loss: 16386.0605 - val_mae: 90.4834\n",
      "Epoch 156/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4530.2998 - mae: 54.9416 - val_loss: 16380.6162 - val_mae: 90.4533\n",
      "Epoch 157/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4527.0117 - mae: 54.9120 - val_loss: 16375.2744 - val_mae: 90.4238\n",
      "Epoch 158/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 4523.7412 - mae: 54.8819 - val_loss: 16369.8018 - val_mae: 90.3935\n",
      "Epoch 159/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - loss: 4520.3525 - mae: 54.8509 - val_loss: 16364.1709 - val_mae: 90.3624\n",
      "Epoch 160/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4516.9810 - mae: 54.8205 - val_loss: 16358.7158 - val_mae: 90.3322\n",
      "Epoch 161/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 4513.7231 - mae: 54.7905 - val_loss: 16353.3164 - val_mae: 90.3023\n",
      "Epoch 162/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4510.4053 - mae: 54.7607 - val_loss: 16347.9561 - val_mae: 90.2726\n",
      "Epoch 163/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 4507.1685 - mae: 54.7309 - val_loss: 16342.5518 - val_mae: 90.2426\n",
      "Epoch 164/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - loss: 4503.9009 - mae: 54.7008 - val_loss: 16337.0723 - val_mae: 90.2123\n",
      "Epoch 165/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 4500.6323 - mae: 54.6711 - val_loss: 16331.7793 - val_mae: 90.1829\n",
      "Epoch 166/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 4497.3560 - mae: 54.6412 - val_loss: 16326.3770 - val_mae: 90.1530\n",
      "Epoch 167/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - loss: 4494.0830 - mae: 54.6115 - val_loss: 16320.9854 - val_mae: 90.1231\n",
      "Epoch 168/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4490.7627 - mae: 54.5809 - val_loss: 16315.4443 - val_mae: 90.0923\n",
      "Epoch 169/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4487.4351 - mae: 54.5501 - val_loss: 16309.9004 - val_mae: 90.0615\n",
      "Epoch 170/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 4484.1157 - mae: 54.5200 - val_loss: 16304.5127 - val_mae: 90.0316\n",
      "Epoch 171/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 4480.8101 - mae: 54.4900 - val_loss: 16299.1104 - val_mae: 90.0016\n",
      "Epoch 172/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 4477.5176 - mae: 54.4594 - val_loss: 16293.5713 - val_mae: 89.9708\n",
      "Epoch 173/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4474.1934 - mae: 54.4289 - val_loss: 16288.1123 - val_mae: 89.9405\n",
      "Epoch 174/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4470.9229 - mae: 54.3988 - val_loss: 16282.7236 - val_mae: 89.9106\n",
      "Epoch 175/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4467.6504 - mae: 54.3690 - val_loss: 16277.3359 - val_mae: 89.8806\n",
      "Epoch 176/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 4464.3936 - mae: 54.3388 - val_loss: 16271.9287 - val_mae: 89.8505\n",
      "Epoch 177/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4461.1313 - mae: 54.3087 - val_loss: 16266.5303 - val_mae: 89.8204\n",
      "Epoch 178/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4457.8711 - mae: 54.2786 - val_loss: 16261.1025 - val_mae: 89.7902\n",
      "Epoch 179/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 4454.5576 - mae: 54.2483 - val_loss: 16255.6602 - val_mae: 89.7599\n",
      "Epoch 180/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - loss: 4451.2710 - mae: 54.2183 - val_loss: 16250.2471 - val_mae: 89.7298\n",
      "Epoch 181/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4448.0244 - mae: 54.1879 - val_loss: 16244.7949 - val_mae: 89.6994\n",
      "Epoch 182/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4444.6938 - mae: 54.1570 - val_loss: 16239.2354 - val_mae: 89.6684\n",
      "Epoch 183/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4441.4321 - mae: 54.1269 - val_loss: 16233.9326 - val_mae: 89.6388\n",
      "Epoch 184/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - loss: 4438.2261 - mae: 54.0973 - val_loss: 16228.5889 - val_mae: 89.6090\n",
      "Epoch 185/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 4434.9692 - mae: 54.0671 - val_loss: 16223.1875 - val_mae: 89.5788\n",
      "Epoch 186/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 4431.7212 - mae: 54.0372 - val_loss: 16217.8555 - val_mae: 89.5491\n",
      "Epoch 187/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 4428.5029 - mae: 54.0073 - val_loss: 16212.4541 - val_mae: 89.5189\n",
      "Epoch 188/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4425.2290 - mae: 53.9771 - val_loss: 16207.0645 - val_mae: 89.4888\n",
      "Epoch 189/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - loss: 4421.9951 - mae: 53.9473 - val_loss: 16201.7266 - val_mae: 89.4590\n",
      "Epoch 190/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 4418.7178 - mae: 53.9167 - val_loss: 16196.2266 - val_mae: 89.4282\n",
      "Epoch 191/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 4415.4326 - mae: 53.8863 - val_loss: 16190.8096 - val_mae: 89.3979\n",
      "Epoch 192/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4412.2197 - mae: 53.8564 - val_loss: 16185.4854 - val_mae: 89.3682\n",
      "Epoch 193/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4409.0205 - mae: 53.8269 - val_loss: 16180.2158 - val_mae: 89.3387\n",
      "Epoch 194/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4405.8228 - mae: 53.7972 - val_loss: 16174.9072 - val_mae: 89.3090\n",
      "Epoch 195/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 4402.6460 - mae: 53.7674 - val_loss: 16169.5225 - val_mae: 89.2788\n",
      "Epoch 196/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - loss: 4399.1606 - mae: 53.7347 - val_loss: 16163.5879 - val_mae: 89.2456\n",
      "Epoch 197/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 4395.8618 - mae: 53.7044 - val_loss: 16158.4062 - val_mae: 89.2165\n",
      "Epoch 198/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4392.7100 - mae: 53.6751 - val_loss: 16153.1318 - val_mae: 89.1870\n",
      "Epoch 199/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 4389.5615 - mae: 53.6457 - val_loss: 16147.8818 - val_mae: 89.1575\n",
      "Epoch 200/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - loss: 4386.3848 - mae: 53.6159 - val_loss: 16142.6191 - val_mae: 89.1280\n",
      "Epoch 201/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 4383.2256 - mae: 53.5865 - val_loss: 16137.3291 - val_mae: 89.0983\n",
      "Epoch 202/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4380.0596 - mae: 53.5573 - val_loss: 16132.1348 - val_mae: 89.0692\n",
      "Epoch 203/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4376.9077 - mae: 53.5277 - val_loss: 16126.8643 - val_mae: 89.0396\n",
      "Epoch 204/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - loss: 4373.7305 - mae: 53.4978 - val_loss: 16121.5215 - val_mae: 89.0096\n",
      "Epoch 205/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4370.5552 - mae: 53.4684 - val_loss: 16116.3105 - val_mae: 88.9803\n",
      "Epoch 206/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 4367.3857 - mae: 53.4385 - val_loss: 16110.9502 - val_mae: 88.9502\n",
      "Epoch 207/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - loss: 4364.1660 - mae: 53.4085 - val_loss: 16105.6338 - val_mae: 88.9203\n",
      "Epoch 208/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4361.0010 - mae: 53.3788 - val_loss: 16100.3789 - val_mae: 88.8907\n",
      "Epoch 209/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 4357.8394 - mae: 53.3490 - val_loss: 16095.0098 - val_mae: 88.8605\n",
      "Epoch 210/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4354.6392 - mae: 53.3196 - val_loss: 16089.8555 - val_mae: 88.8315\n",
      "Epoch 211/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4351.4902 - mae: 53.2899 - val_loss: 16084.5332 - val_mae: 88.8016\n",
      "Epoch 212/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 4348.2920 - mae: 53.2592 - val_loss: 16079.0791 - val_mae: 88.7709\n",
      "Epoch 213/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4345.0474 - mae: 53.2293 - val_loss: 16073.8135 - val_mae: 88.7412\n",
      "Epoch 214/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 4341.8853 - mae: 53.1993 - val_loss: 16068.4512 - val_mae: 88.7110\n",
      "Epoch 215/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - loss: 4338.6704 - mae: 53.1694 - val_loss: 16063.1602 - val_mae: 88.6811\n",
      "Epoch 216/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 4335.5024 - mae: 53.1396 - val_loss: 16057.8740 - val_mae: 88.6513\n",
      "Epoch 217/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 4332.2788 - mae: 53.1091 - val_loss: 16052.4287 - val_mae: 88.6206\n",
      "Epoch 218/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - loss: 4329.0610 - mae: 53.0785 - val_loss: 16047.0312 - val_mae: 88.5901\n",
      "Epoch 219/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4325.8579 - mae: 53.0487 - val_loss: 16041.7461 - val_mae: 88.5603\n",
      "Epoch 220/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4322.7183 - mae: 53.0190 - val_loss: 16036.5674 - val_mae: 88.5311\n",
      "Epoch 221/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 4319.5933 - mae: 52.9896 - val_loss: 16031.3535 - val_mae: 88.5016\n",
      "Epoch 222/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - loss: 4316.4790 - mae: 52.9601 - val_loss: 16026.0811 - val_mae: 88.4718\n",
      "Epoch 223/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4313.2319 - mae: 52.9292 - val_loss: 16020.5537 - val_mae: 88.4406\n",
      "Epoch 224/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4309.9902 - mae: 52.8990 - val_loss: 16015.2686 - val_mae: 88.4107\n",
      "Epoch 225/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 4306.8081 - mae: 52.8688 - val_loss: 16009.8994 - val_mae: 88.3803\n",
      "Epoch 226/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4303.6353 - mae: 52.8389 - val_loss: 16004.6504 - val_mae: 88.3506\n",
      "Epoch 227/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 4300.4634 - mae: 52.8086 - val_loss: 15999.2861 - val_mae: 88.3203\n",
      "Epoch 228/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - loss: 4297.3193 - mae: 52.7785 - val_loss: 15994.0264 - val_mae: 88.2905\n",
      "Epoch 229/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 4294.1831 - mae: 52.7488 - val_loss: 15988.7490 - val_mae: 88.2606\n",
      "Epoch 230/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4291.0005 - mae: 52.7189 - val_loss: 15983.4297 - val_mae: 88.2305\n",
      "Epoch 231/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 4287.7969 - mae: 52.6889 - val_loss: 15978.1455 - val_mae: 88.2005\n",
      "Epoch 232/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 4284.6523 - mae: 52.6592 - val_loss: 15972.8643 - val_mae: 88.1706\n",
      "Epoch 233/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 4281.5093 - mae: 52.6291 - val_loss: 15967.5635 - val_mae: 88.1405\n",
      "Epoch 234/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - loss: 4278.3418 - mae: 52.5991 - val_loss: 15962.2529 - val_mae: 88.1104\n",
      "Epoch 235/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - loss: 4275.1646 - mae: 52.5689 - val_loss: 15956.9209 - val_mae: 88.0801\n",
      "Epoch 236/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 4271.9512 - mae: 52.5385 - val_loss: 15951.5205 - val_mae: 88.0495\n",
      "Epoch 237/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 4268.7637 - mae: 52.5083 - val_loss: 15946.2021 - val_mae: 88.0192\n",
      "Epoch 238/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 4265.5962 - mae: 52.4783 - val_loss: 15940.9688 - val_mae: 87.9895\n",
      "Epoch 239/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 4262.4854 - mae: 52.4487 - val_loss: 15935.7451 - val_mae: 87.9598\n",
      "Epoch 240/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 4259.3862 - mae: 52.4191 - val_loss: 15930.5342 - val_mae: 87.9302\n",
      "Epoch 241/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - loss: 4256.1929 - mae: 52.3885 - val_loss: 15925.0723 - val_mae: 87.8991\n",
      "Epoch 242/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 4253.0142 - mae: 52.3582 - val_loss: 15919.7715 - val_mae: 87.8690\n",
      "Epoch 243/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 113ms/step - loss: 4249.8276 - mae: 52.3281 - val_loss: 15914.4443 - val_mae: 87.8386\n",
      "Epoch 244/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 4246.5850 - mae: 52.2970 - val_loss: 15908.9648 - val_mae: 87.8074\n",
      "Epoch 245/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 4243.4116 - mae: 52.2670 - val_loss: 15903.7363 - val_mae: 87.7777\n",
      "Epoch 246/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - loss: 4240.2954 - mae: 52.2372 - val_loss: 15898.5059 - val_mae: 87.7479\n",
      "Epoch 247/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - loss: 4237.1929 - mae: 52.2072 - val_loss: 15893.2119 - val_mae: 87.7177\n",
      "Epoch 248/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - loss: 4234.0566 - mae: 52.1777 - val_loss: 15888.0391 - val_mae: 87.6882\n",
      "Epoch 249/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 4230.9697 - mae: 52.1479 - val_loss: 15882.7822 - val_mae: 87.6582\n",
      "Epoch 250/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 4227.8047 - mae: 52.1176 - val_loss: 15877.4551 - val_mae: 87.6279\n",
      "Epoch 251/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 4224.5630 - mae: 52.0867 - val_loss: 15871.9775 - val_mae: 87.5966\n",
      "Epoch 252/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4221.3853 - mae: 52.0563 - val_loss: 15866.6875 - val_mae: 87.5664\n",
      "Epoch 253/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 4218.2510 - mae: 52.0259 - val_loss: 15861.3691 - val_mae: 87.5360\n",
      "Epoch 254/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 4215.1558 - mae: 51.9964 - val_loss: 15856.2305 - val_mae: 87.5067\n",
      "Epoch 255/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 4212.0952 - mae: 51.9673 - val_loss: 15851.1328 - val_mae: 87.4775\n",
      "Epoch 256/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4209.0347 - mae: 51.9376 - val_loss: 15845.8984 - val_mae: 87.4476\n",
      "Epoch 257/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4205.8926 - mae: 51.9074 - val_loss: 15840.6016 - val_mae: 87.4173\n",
      "Epoch 258/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 4202.7432 - mae: 51.8775 - val_loss: 15835.3633 - val_mae: 87.3873\n",
      "Epoch 259/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 4199.6777 - mae: 51.8478 - val_loss: 15830.1680 - val_mae: 87.3576\n",
      "Epoch 260/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 4196.6055 - mae: 51.8179 - val_loss: 15824.9678 - val_mae: 87.3278\n",
      "Epoch 261/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 4193.5420 - mae: 51.7885 - val_loss: 15819.7959 - val_mae: 87.2982\n",
      "Epoch 262/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 4190.4487 - mae: 51.7590 - val_loss: 15814.6299 - val_mae: 87.2686\n",
      "Epoch 263/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 4187.3096 - mae: 51.7284 - val_loss: 15809.2217 - val_mae: 87.2376\n",
      "Epoch 264/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 4184.1670 - mae: 51.6983 - val_loss: 15804.0459 - val_mae: 87.2080\n",
      "Epoch 265/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 4181.1294 - mae: 51.6691 - val_loss: 15798.9277 - val_mae: 87.1786\n",
      "Epoch 266/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 4178.0732 - mae: 51.6394 - val_loss: 15793.6982 - val_mae: 87.1486\n",
      "Epoch 267/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4174.9575 - mae: 51.6091 - val_loss: 15788.4287 - val_mae: 87.1184\n",
      "Epoch 268/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 4171.8169 - mae: 51.5790 - val_loss: 15783.1631 - val_mae: 87.0882\n",
      "Epoch 269/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - loss: 4168.7417 - mae: 51.5490 - val_loss: 15777.9531 - val_mae: 87.0582\n",
      "Epoch 270/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4165.6479 - mae: 51.5189 - val_loss: 15772.6641 - val_mae: 87.0278\n",
      "Epoch 271/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 4162.5288 - mae: 51.4889 - val_loss: 15767.4492 - val_mae: 86.9979\n",
      "Epoch 272/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 4159.4141 - mae: 51.4585 - val_loss: 15762.1191 - val_mae: 86.9673\n",
      "Epoch 273/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 4156.2671 - mae: 51.4280 - val_loss: 15756.8350 - val_mae: 86.9369\n",
      "Epoch 274/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 4153.2041 - mae: 51.3982 - val_loss: 15751.6787 - val_mae: 86.9072\n",
      "Epoch 275/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 4150.1782 - mae: 51.3687 - val_loss: 15746.5303 - val_mae: 86.8776\n",
      "Epoch 276/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 4147.0942 - mae: 51.3389 - val_loss: 15741.3447 - val_mae: 86.8477\n",
      "Epoch 277/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 4144.0049 - mae: 51.3091 - val_loss: 15736.1465 - val_mae: 86.8178\n",
      "Epoch 278/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4140.9805 - mae: 51.2794 - val_loss: 15731.0039 - val_mae: 86.7882\n",
      "Epoch 279/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 4137.9561 - mae: 51.2498 - val_loss: 15725.8740 - val_mae: 86.7586\n",
      "Epoch 280/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 4134.9106 - mae: 51.2206 - val_loss: 15720.7686 - val_mae: 86.7292\n",
      "Epoch 281/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 4131.8652 - mae: 51.1910 - val_loss: 15715.6045 - val_mae: 86.6994\n",
      "Epoch 282/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 4128.8276 - mae: 51.1614 - val_loss: 15710.4160 - val_mae: 86.6695\n",
      "Epoch 283/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 4125.7041 - mae: 51.1310 - val_loss: 15705.1338 - val_mae: 86.6390\n",
      "Epoch 284/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 4122.6553 - mae: 51.1011 - val_loss: 15699.9971 - val_mae: 86.6094\n",
      "Epoch 285/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 4119.5918 - mae: 51.0710 - val_loss: 15694.7461 - val_mae: 86.5790\n",
      "Epoch 286/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 4116.4775 - mae: 51.0404 - val_loss: 15689.4268 - val_mae: 86.5483\n",
      "Epoch 287/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 4113.3794 - mae: 51.0104 - val_loss: 15684.2393 - val_mae: 86.5183\n",
      "Epoch 288/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4110.3184 - mae: 50.9803 - val_loss: 15679.0605 - val_mae: 86.4884\n",
      "Epoch 289/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4107.3135 - mae: 50.9505 - val_loss: 15673.9141 - val_mae: 86.4586\n",
      "Epoch 290/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 4104.2246 - mae: 50.9207 - val_loss: 15668.6689 - val_mae: 86.4283\n",
      "Epoch 291/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 4101.1792 - mae: 50.8907 - val_loss: 15663.5273 - val_mae: 86.3986\n",
      "Epoch 292/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 4098.1807 - mae: 50.8611 - val_loss: 15658.4189 - val_mae: 86.3690\n",
      "Epoch 293/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - loss: 4095.0298 - mae: 50.8305 - val_loss: 15653.0479 - val_mae: 86.3379\n",
      "Epoch 294/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 4091.9829 - mae: 50.8005 - val_loss: 15647.9141 - val_mae: 86.3082\n",
      "Epoch 295/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 4088.9250 - mae: 50.7706 - val_loss: 15642.7490 - val_mae: 86.2782\n",
      "Epoch 296/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step - loss: 4085.8809 - mae: 50.7404 - val_loss: 15637.4990 - val_mae: 86.2478\n",
      "Epoch 297/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 4082.8347 - mae: 50.7103 - val_loss: 15632.3447 - val_mae: 86.2179\n",
      "Epoch 298/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - loss: 4079.7595 - mae: 50.6804 - val_loss: 15627.1396 - val_mae: 86.1877\n",
      "Epoch 299/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - loss: 4076.6946 - mae: 50.6503 - val_loss: 15621.9053 - val_mae: 86.1573\n",
      "Epoch 300/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 4073.6013 - mae: 50.6198 - val_loss: 15616.6592 - val_mae: 86.1269\n",
      "Epoch 301/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step - loss: 4070.5403 - mae: 50.5897 - val_loss: 15611.4336 - val_mae: 86.0966\n",
      "Epoch 302/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 4067.4766 - mae: 50.5592 - val_loss: 15606.2344 - val_mae: 86.0664\n",
      "Epoch 303/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 4064.4766 - mae: 50.5299 - val_loss: 15601.1807 - val_mae: 86.0370\n",
      "Epoch 304/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - loss: 4061.4009 - mae: 50.4993 - val_loss: 15595.8281 - val_mae: 86.0059\n",
      "Epoch 305/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 4058.3198 - mae: 50.4686 - val_loss: 15590.5908 - val_mae: 85.9754\n",
      "Epoch 306/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 4055.3140 - mae: 50.4391 - val_loss: 15585.5547 - val_mae: 85.9461\n",
      "Epoch 307/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 4052.2869 - mae: 50.4090 - val_loss: 15580.3184 - val_mae: 85.9157\n",
      "Epoch 308/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 4049.2444 - mae: 50.3793 - val_loss: 15575.2139 - val_mae: 85.8860\n",
      "Epoch 309/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 4046.1882 - mae: 50.3489 - val_loss: 15569.9268 - val_mae: 85.8552\n",
      "Epoch 310/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 4043.0854 - mae: 50.3181 - val_loss: 15564.6543 - val_mae: 85.8245\n",
      "Epoch 311/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 4040.0991 - mae: 50.2882 - val_loss: 15559.5859 - val_mae: 85.7949\n",
      "Epoch 312/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4037.0725 - mae: 50.2584 - val_loss: 15554.4277 - val_mae: 85.7649\n",
      "Epoch 313/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4034.0559 - mae: 50.2282 - val_loss: 15549.2471 - val_mae: 85.7346\n",
      "Epoch 314/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 4031.0337 - mae: 50.1984 - val_loss: 15544.1201 - val_mae: 85.7048\n",
      "Epoch 315/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4028.0232 - mae: 50.1684 - val_loss: 15538.9482 - val_mae: 85.6746\n",
      "Epoch 316/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 4025.0107 - mae: 50.1387 - val_loss: 15533.8770 - val_mae: 85.6450\n",
      "Epoch 317/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 4022.0381 - mae: 50.1083 - val_loss: 15528.6758 - val_mae: 85.6146\n",
      "Epoch 318/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4019.0312 - mae: 50.0790 - val_loss: 15523.6543 - val_mae: 85.5853\n",
      "Epoch 319/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4016.0527 - mae: 50.0492 - val_loss: 15518.5166 - val_mae: 85.5553\n",
      "Epoch 320/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 4013.0635 - mae: 50.0195 - val_loss: 15513.4287 - val_mae: 85.5255\n",
      "Epoch 321/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 4010.1250 - mae: 49.9902 - val_loss: 15508.4346 - val_mae: 85.4963\n",
      "Epoch 322/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - loss: 4007.1843 - mae: 49.9609 - val_loss: 15503.4375 - val_mae: 85.4671\n",
      "Epoch 323/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4004.2297 - mae: 49.9311 - val_loss: 15498.2451 - val_mae: 85.4367\n",
      "Epoch 324/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 4001.2141 - mae: 49.9007 - val_loss: 15493.0713 - val_mae: 85.4064\n",
      "Epoch 325/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 3998.1497 - mae: 49.8705 - val_loss: 15487.8789 - val_mae: 85.3760\n",
      "Epoch 326/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 3995.1560 - mae: 49.8403 - val_loss: 15482.7236 - val_mae: 85.3458\n",
      "Epoch 327/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 3992.1338 - mae: 49.8105 - val_loss: 15477.6133 - val_mae: 85.3159\n",
      "Epoch 328/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 3989.1077 - mae: 49.7800 - val_loss: 15472.3809 - val_mae: 85.2852\n",
      "Epoch 329/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 3986.1287 - mae: 49.7498 - val_loss: 15467.2686 - val_mae: 85.2552\n",
      "Epoch 330/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 3983.1079 - mae: 49.7194 - val_loss: 15462.0615 - val_mae: 85.2247\n",
      "Epoch 331/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 3980.0818 - mae: 49.6891 - val_loss: 15456.9131 - val_mae: 85.1945\n",
      "Epoch 332/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 3977.1411 - mae: 49.6597 - val_loss: 15451.9092 - val_mae: 85.1651\n",
      "Epoch 333/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 3974.1938 - mae: 49.6302 - val_loss: 15446.8477 - val_mae: 85.1354\n",
      "Epoch 334/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 3971.2351 - mae: 49.6002 - val_loss: 15441.7285 - val_mae: 85.1053\n",
      "Epoch 335/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 3968.2759 - mae: 49.5712 - val_loss: 15436.7959 - val_mae: 85.0763\n",
      "Epoch 336/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 3965.3325 - mae: 49.5410 - val_loss: 15431.6328 - val_mae: 85.0460\n",
      "Epoch 337/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 3962.3828 - mae: 49.5111 - val_loss: 15426.5547 - val_mae: 85.0161\n",
      "Epoch 338/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - loss: 3959.4473 - mae: 49.4816 - val_loss: 15421.5137 - val_mae: 84.9865\n",
      "Epoch 339/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 3956.5098 - mae: 49.4519 - val_loss: 15416.4893 - val_mae: 84.9569\n",
      "Epoch 340/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 3953.5327 - mae: 49.4218 - val_loss: 15411.3047 - val_mae: 84.9264\n",
      "Epoch 341/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 3950.5640 - mae: 49.3913 - val_loss: 15406.1445 - val_mae: 84.8960\n",
      "Epoch 342/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - loss: 3947.6008 - mae: 49.3620 - val_loss: 15401.1982 - val_mae: 84.8668\n",
      "Epoch 343/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 3944.6196 - mae: 49.3317 - val_loss: 15395.9531 - val_mae: 84.8359\n",
      "Epoch 344/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 3941.5896 - mae: 49.3010 - val_loss: 15390.7725 - val_mae: 84.8054\n",
      "Epoch 345/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - loss: 3938.5698 - mae: 49.2705 - val_loss: 15385.5723 - val_mae: 84.7747\n",
      "Epoch 346/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - loss: 3935.6145 - mae: 49.2405 - val_loss: 15380.4834 - val_mae: 84.7447\n",
      "Epoch 347/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - loss: 3932.6528 - mae: 49.2106 - val_loss: 15375.4512 - val_mae: 84.7150\n",
      "Epoch 348/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - loss: 3929.7595 - mae: 49.1811 - val_loss: 15370.4404 - val_mae: 84.6854\n",
      "Epoch 349/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 3926.8237 - mae: 49.1510 - val_loss: 15365.3105 - val_mae: 84.6552\n",
      "Epoch 350/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - loss: 3923.8230 - mae: 49.1211 - val_loss: 15360.2510 - val_mae: 84.6252\n",
      "Epoch 351/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - loss: 3920.9102 - mae: 49.0914 - val_loss: 15355.2207 - val_mae: 84.5955\n",
      "Epoch 352/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - loss: 3917.9727 - mae: 49.0615 - val_loss: 15350.1230 - val_mae: 84.5654\n",
      "Epoch 353/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - loss: 3915.0327 - mae: 49.0314 - val_loss: 15345.0332 - val_mae: 84.5353\n",
      "Epoch 354/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 3912.1184 - mae: 49.0020 - val_loss: 15340.0762 - val_mae: 84.5060\n",
      "Epoch 355/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 80ms/step - loss: 3909.1863 - mae: 48.9721 - val_loss: 15334.9736 - val_mae: 84.4758\n",
      "Epoch 356/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 3906.2659 - mae: 48.9420 - val_loss: 15329.9160 - val_mae: 84.4458\n",
      "Epoch 357/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 3903.3723 - mae: 48.9126 - val_loss: 15324.9805 - val_mae: 84.4166\n",
      "Epoch 358/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 3900.4641 - mae: 48.8828 - val_loss: 15319.8721 - val_mae: 84.3864\n",
      "Epoch 359/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 3897.5386 - mae: 48.8536 - val_loss: 15314.9385 - val_mae: 84.3571\n",
      "Epoch 360/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: 3894.6731 - mae: 48.8239 - val_loss: 15309.9121 - val_mae: 84.3273\n",
      "Epoch 361/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 3891.7598 - mae: 48.7945 - val_loss: 15304.9014 - val_mae: 84.2976\n",
      "Epoch 362/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 3888.8359 - mae: 48.7649 - val_loss: 15299.8145 - val_mae: 84.2674\n",
      "Epoch 363/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 3885.8904 - mae: 48.7348 - val_loss: 15294.7441 - val_mae: 84.2373\n",
      "Epoch 364/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 3883.0088 - mae: 48.7055 - val_loss: 15289.7812 - val_mae: 84.2079\n",
      "Epoch 365/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 3880.1008 - mae: 48.6757 - val_loss: 15284.6592 - val_mae: 84.1775\n",
      "Epoch 366/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 3877.1321 - mae: 48.6454 - val_loss: 15279.5518 - val_mae: 84.1471\n",
      "Epoch 367/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 3874.2200 - mae: 48.6161 - val_loss: 15274.6045 - val_mae: 84.1177\n",
      "Epoch 368/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 3871.3235 - mae: 48.5864 - val_loss: 15269.5176 - val_mae: 84.0875\n",
      "Epoch 369/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 3868.3518 - mae: 48.5560 - val_loss: 15264.3818 - val_mae: 84.0569\n",
      "Epoch 370/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 3865.3828 - mae: 48.5259 - val_loss: 15259.2900 - val_mae: 84.0266\n",
      "Epoch 371/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 3862.5071 - mae: 48.4966 - val_loss: 15254.3525 - val_mae: 83.9972\n",
      "Epoch 372/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step - loss: 3859.6536 - mae: 48.4674 - val_loss: 15249.3760 - val_mae: 83.9676\n",
      "Epoch 373/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 3856.7725 - mae: 48.4377 - val_loss: 15244.3359 - val_mae: 83.9376\n",
      "Epoch 374/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 3853.8311 - mae: 48.4077 - val_loss: 15239.2646 - val_mae: 83.9074\n",
      "Epoch 375/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 3850.9717 - mae: 48.3782 - val_loss: 15234.2959 - val_mae: 83.8778\n",
      "Epoch 376/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 3848.0688 - mae: 48.3484 - val_loss: 15229.2393 - val_mae: 83.8476\n",
      "Epoch 377/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - loss: 3845.1902 - mae: 48.3187 - val_loss: 15224.2461 - val_mae: 83.8178\n",
      "Epoch 378/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 3842.3123 - mae: 48.2894 - val_loss: 15219.2803 - val_mae: 83.7882\n",
      "Epoch 379/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 3839.4392 - mae: 48.2593 - val_loss: 15214.2002 - val_mae: 83.7579\n",
      "Epoch 380/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 3836.5427 - mae: 48.2303 - val_loss: 15209.2559 - val_mae: 83.7284\n",
      "Epoch 381/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 3833.6399 - mae: 48.2005 - val_loss: 15204.2275 - val_mae: 83.6983\n",
      "Epoch 382/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 3830.7483 - mae: 48.1705 - val_loss: 15199.1572 - val_mae: 83.6681\n",
      "Epoch 383/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - loss: 3827.8792 - mae: 48.1409 - val_loss: 15194.1904 - val_mae: 83.6384\n",
      "Epoch 384/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 3824.9297 - mae: 48.1103 - val_loss: 15188.9775 - val_mae: 83.6072\n",
      "Epoch 385/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - loss: 3821.9570 - mae: 48.0798 - val_loss: 15183.8936 - val_mae: 83.5768\n",
      "Epoch 386/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step - loss: 3819.0608 - mae: 48.0498 - val_loss: 15178.8369 - val_mae: 83.5465\n",
      "Epoch 387/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - loss: 3816.2114 - mae: 48.0201 - val_loss: 15173.8535 - val_mae: 83.5167\n",
      "Epoch 388/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 3813.3804 - mae: 47.9911 - val_loss: 15168.9756 - val_mae: 83.4875\n",
      "Epoch 389/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 3810.5125 - mae: 47.9616 - val_loss: 15163.9893 - val_mae: 83.4576\n",
      "Epoch 390/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - loss: 3807.6616 - mae: 47.9323 - val_loss: 15159.0781 - val_mae: 83.4282\n",
      "Epoch 391/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 3804.8423 - mae: 47.9025 - val_loss: 15154.0391 - val_mae: 83.3980\n",
      "Epoch 392/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 3801.9448 - mae: 47.8731 - val_loss: 15149.1035 - val_mae: 83.3684\n",
      "Epoch 393/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 3799.0474 - mae: 47.8431 - val_loss: 15144.0215 - val_mae: 83.3379\n",
      "Epoch 394/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 3796.1838 - mae: 47.8131 - val_loss: 15139.0264 - val_mae: 83.3079\n",
      "Epoch 395/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 3793.3486 - mae: 47.7836 - val_loss: 15134.0898 - val_mae: 83.2783\n",
      "Epoch 396/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 3790.5400 - mae: 47.7543 - val_loss: 15129.1602 - val_mae: 83.2487\n",
      "Epoch 397/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 3787.7041 - mae: 47.7251 - val_loss: 15124.2432 - val_mae: 83.2192\n",
      "Epoch 398/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 3784.7861 - mae: 47.6951 - val_loss: 15119.1357 - val_mae: 83.1885\n",
      "Epoch 399/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 3781.8630 - mae: 47.6643 - val_loss: 15113.9707 - val_mae: 83.1574\n",
      "Epoch 400/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 3778.9922 - mae: 47.6347 - val_loss: 15109.0713 - val_mae: 83.1280\n",
      "Epoch 401/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 3776.1375 - mae: 47.6052 - val_loss: 15104.1094 - val_mae: 83.0981\n",
      "Epoch 402/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 3773.3289 - mae: 47.5753 - val_loss: 15099.1328 - val_mae: 83.0681\n",
      "Epoch 403/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 3770.4102 - mae: 47.5450 - val_loss: 15094.0117 - val_mae: 83.0373\n",
      "Epoch 404/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 3767.5537 - mae: 47.5151 - val_loss: 15089.0371 - val_mae: 83.0073\n",
      "Epoch 405/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 3764.7053 - mae: 47.4854 - val_loss: 15084.0693 - val_mae: 82.9774\n",
      "Epoch 406/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 3761.8701 - mae: 47.4556 - val_loss: 15079.1104 - val_mae: 82.9475\n",
      "Epoch 407/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 3759.0649 - mae: 47.4265 - val_loss: 15074.2598 - val_mae: 82.9183\n",
      "Epoch 408/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 3756.2795 - mae: 47.3974 - val_loss: 15069.3721 - val_mae: 82.8888\n",
      "Epoch 409/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 3753.4104 - mae: 47.3676 - val_loss: 15064.3574 - val_mae: 82.8586\n",
      "Epoch 410/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 3750.6035 - mae: 47.3379 - val_loss: 15059.4219 - val_mae: 82.8288\n",
      "Epoch 411/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 3747.7974 - mae: 47.3088 - val_loss: 15054.6016 - val_mae: 82.7997\n",
      "Epoch 412/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 3745.0227 - mae: 47.2796 - val_loss: 15049.6729 - val_mae: 82.7699\n",
      "Epoch 413/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 3742.2134 - mae: 47.2502 - val_loss: 15044.7666 - val_mae: 82.7402\n",
      "Epoch 414/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 3739.4072 - mae: 47.2201 - val_loss: 15039.7314 - val_mae: 82.7098\n",
      "Epoch 415/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 3736.5664 - mae: 47.1905 - val_loss: 15034.8076 - val_mae: 82.6801\n",
      "Epoch 416/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 3733.7344 - mae: 47.1607 - val_loss: 15029.8467 - val_mae: 82.6500\n",
      "Epoch 417/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 3730.8889 - mae: 47.1312 - val_loss: 15024.8760 - val_mae: 82.6200\n",
      "Epoch 418/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 3728.0474 - mae: 47.1008 - val_loss: 15019.8076 - val_mae: 82.5893\n",
      "Epoch 419/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 3725.2412 - mae: 47.0719 - val_loss: 15015.0332 - val_mae: 82.5604\n",
      "Epoch 420/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 3722.4519 - mae: 47.0428 - val_loss: 15010.1279 - val_mae: 82.5307\n",
      "Epoch 421/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 3719.6616 - mae: 47.0133 - val_loss: 15005.2021 - val_mae: 82.5008\n",
      "Epoch 422/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 3716.8186 - mae: 46.9830 - val_loss: 15000.1758 - val_mae: 82.4704\n",
      "Epoch 423/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 3714.0034 - mae: 46.9534 - val_loss: 14995.2490 - val_mae: 82.4405\n",
      "Epoch 424/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 3711.2275 - mae: 46.9243 - val_loss: 14990.4023 - val_mae: 82.4111\n",
      "Epoch 425/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 3708.4329 - mae: 46.8949 - val_loss: 14985.4688 - val_mae: 82.3811\n",
      "Epoch 426/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 3705.6667 - mae: 46.8654 - val_loss: 14980.5381 - val_mae: 82.3512\n",
      "Epoch 427/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 3702.8074 - mae: 46.8356 - val_loss: 14975.5820 - val_mae: 82.3211\n",
      "Epoch 428/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 3700.0088 - mae: 46.8056 - val_loss: 14970.5322 - val_mae: 82.2904\n",
      "Epoch 429/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 3697.1216 - mae: 46.7749 - val_loss: 14965.4854 - val_mae: 82.2598\n",
      "Epoch 430/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - loss: 3694.3274 - mae: 46.7457 - val_loss: 14960.6113 - val_mae: 82.2301\n",
      "Epoch 431/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - loss: 3691.5166 - mae: 46.7162 - val_loss: 14955.6963 - val_mae: 82.2002\n",
      "Epoch 432/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 3688.7456 - mae: 46.6868 - val_loss: 14950.7793 - val_mae: 82.1703\n",
      "Epoch 433/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 3685.9595 - mae: 46.6576 - val_loss: 14945.9541 - val_mae: 82.1410\n",
      "Epoch 434/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 3683.2144 - mae: 46.6285 - val_loss: 14941.1602 - val_mae: 82.1118\n",
      "Epoch 435/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 3680.4893 - mae: 46.5989 - val_loss: 14936.1650 - val_mae: 82.0814\n",
      "Epoch 436/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 3677.6514 - mae: 46.5696 - val_loss: 14931.2988 - val_mae: 82.0517\n",
      "Epoch 437/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 3674.8513 - mae: 46.5398 - val_loss: 14926.3047 - val_mae: 82.0213\n",
      "Epoch 438/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 3672.0449 - mae: 46.5097 - val_loss: 14921.3105 - val_mae: 81.9908\n",
      "Epoch 439/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 3669.1646 - mae: 46.4794 - val_loss: 14916.2871 - val_mae: 81.9602\n",
      "Epoch 440/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - loss: 3666.3904 - mae: 46.4496 - val_loss: 14911.3896 - val_mae: 81.9303\n",
      "Epoch 441/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 3663.6619 - mae: 46.4200 - val_loss: 14906.5020 - val_mae: 81.9005\n",
      "Epoch 442/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 3660.8779 - mae: 46.3910 - val_loss: 14901.7305 - val_mae: 81.8713\n",
      "Epoch 443/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 3658.1560 - mae: 46.3618 - val_loss: 14896.8057 - val_mae: 81.8412\n",
      "Epoch 444/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 3655.3555 - mae: 46.3324 - val_loss: 14891.9482 - val_mae: 81.8116\n",
      "Epoch 445/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 3652.6389 - mae: 46.3031 - val_loss: 14887.0596 - val_mae: 81.7817\n",
      "Epoch 446/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 3649.8467 - mae: 46.2737 - val_loss: 14882.1953 - val_mae: 81.7519\n",
      "Epoch 447/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 3647.0835 - mae: 46.2440 - val_loss: 14877.2598 - val_mae: 81.7217\n",
      "Epoch 448/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 3644.2822 - mae: 46.2145 - val_loss: 14872.3652 - val_mae: 81.6918\n",
      "Epoch 449/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 3641.5476 - mae: 46.1854 - val_loss: 14867.5010 - val_mae: 81.6620\n",
      "Epoch 450/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 3638.7744 - mae: 46.1554 - val_loss: 14862.5332 - val_mae: 81.6316\n",
      "Epoch 451/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 3635.9973 - mae: 46.1257 - val_loss: 14857.6709 - val_mae: 81.6018\n",
      "Epoch 452/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 3633.2583 - mae: 46.0966 - val_loss: 14852.8408 - val_mae: 81.5722\n",
      "Epoch 453/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - loss: 3630.4717 - mae: 46.0671 - val_loss: 14847.8857 - val_mae: 81.5418\n",
      "Epoch 454/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 3627.6648 - mae: 46.0369 - val_loss: 14842.8809 - val_mae: 81.5111\n",
      "Epoch 455/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 3624.9148 - mae: 46.0074 - val_loss: 14838.0615 - val_mae: 81.4816\n",
      "Epoch 456/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 3622.1514 - mae: 45.9779 - val_loss: 14833.1055 - val_mae: 81.4512\n",
      "Epoch 457/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 3619.3245 - mae: 45.9475 - val_loss: 14828.0635 - val_mae: 81.4202\n",
      "Epoch 458/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 3616.5229 - mae: 45.9179 - val_loss: 14823.1504 - val_mae: 81.3900\n",
      "Epoch 459/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 3613.7590 - mae: 45.8885 - val_loss: 14818.3076 - val_mae: 81.3603\n",
      "Epoch 460/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 3611.0369 - mae: 45.8590 - val_loss: 14813.4072 - val_mae: 81.3301\n",
      "Epoch 461/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 3608.3152 - mae: 45.8297 - val_loss: 14808.5879 - val_mae: 81.3005\n",
      "Epoch 462/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 3605.5862 - mae: 45.8006 - val_loss: 14803.7236 - val_mae: 81.2706\n",
      "Epoch 463/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 3602.8535 - mae: 45.7717 - val_loss: 14798.9717 - val_mae: 81.2413\n",
      "Epoch 464/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 3600.1580 - mae: 45.7425 - val_loss: 14794.1172 - val_mae: 81.2115\n",
      "Epoch 465/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 3597.4436 - mae: 45.7137 - val_loss: 14789.4189 - val_mae: 81.1825\n",
      "Epoch 466/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 3594.7793 - mae: 45.6844 - val_loss: 14784.4990 - val_mae: 81.1522\n",
      "Epoch 467/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 3592.0349 - mae: 45.6555 - val_loss: 14779.7158 - val_mae: 81.1227\n",
      "Epoch 468/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 3589.3628 - mae: 45.6259 - val_loss: 14774.8389 - val_mae: 81.0927\n",
      "Epoch 469/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 3586.5955 - mae: 45.5963 - val_loss: 14769.9219 - val_mae: 81.0624\n",
      "Epoch 470/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 3583.7983 - mae: 45.5663 - val_loss: 14764.9629 - val_mae: 81.0318\n",
      "Epoch 471/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 3581.1033 - mae: 45.5372 - val_loss: 14760.1865 - val_mae: 81.0023\n",
      "Epoch 472/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 3578.3816 - mae: 45.5078 - val_loss: 14755.2822 - val_mae: 80.9720\n",
      "Epoch 473/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 3575.5933 - mae: 45.4784 - val_loss: 14750.4219 - val_mae: 80.9420\n",
      "Epoch 474/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 3572.9045 - mae: 45.4492 - val_loss: 14745.6602 - val_mae: 80.9126\n",
      "Epoch 475/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - loss: 3570.2463 - mae: 45.4203 - val_loss: 14740.8857 - val_mae: 80.8831\n",
      "Epoch 476/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 3567.5981 - mae: 45.3917 - val_loss: 14736.1328 - val_mae: 80.8537\n",
      "Epoch 477/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 3564.9082 - mae: 45.3625 - val_loss: 14731.3135 - val_mae: 80.8239\n",
      "Epoch 478/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 3562.1848 - mae: 45.3335 - val_loss: 14726.5137 - val_mae: 80.7942\n",
      "Epoch 479/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 3559.5110 - mae: 45.3046 - val_loss: 14721.7500 - val_mae: 80.7647\n",
      "Epoch 480/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 3556.8472 - mae: 45.2757 - val_loss: 14716.9346 - val_mae: 80.7349\n",
      "Epoch 481/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 3554.1252 - mae: 45.2464 - val_loss: 14712.1357 - val_mae: 80.7051\n",
      "Epoch 482/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 3551.4573 - mae: 45.2176 - val_loss: 14707.3701 - val_mae: 80.6756\n",
      "Epoch 483/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - loss: 3548.7759 - mae: 45.1884 - val_loss: 14702.5068 - val_mae: 80.6455\n",
      "Epoch 484/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 3546.1104 - mae: 45.1597 - val_loss: 14697.8027 - val_mae: 80.6163\n",
      "Epoch 485/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 3543.4609 - mae: 45.1310 - val_loss: 14693.0635 - val_mae: 80.5869\n",
      "Epoch 486/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 3540.7893 - mae: 45.1024 - val_loss: 14688.3262 - val_mae: 80.5575\n",
      "Epoch 487/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 3538.0320 - mae: 45.0727 - val_loss: 14683.3398 - val_mae: 80.5265\n",
      "Epoch 488/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 3535.3572 - mae: 45.0436 - val_loss: 14678.6016 - val_mae: 80.4971\n",
      "Epoch 489/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - loss: 3532.7212 - mae: 45.0146 - val_loss: 14673.8301 - val_mae: 80.4675\n",
      "Epoch 490/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 3530.0618 - mae: 44.9862 - val_loss: 14669.1279 - val_mae: 80.4382\n",
      "Epoch 491/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 3527.3760 - mae: 44.9571 - val_loss: 14664.3066 - val_mae: 80.4083\n",
      "Epoch 492/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 3524.7283 - mae: 44.9281 - val_loss: 14659.5459 - val_mae: 80.3787\n",
      "Epoch 493/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - loss: 3522.0381 - mae: 44.8990 - val_loss: 14654.6904 - val_mae: 80.3484\n",
      "Epoch 494/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 3519.3674 - mae: 44.8696 - val_loss: 14649.8242 - val_mae: 80.3182\n",
      "Epoch 495/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - loss: 3516.6924 - mae: 44.8411 - val_loss: 14645.1338 - val_mae: 80.2890\n",
      "Epoch 496/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 3513.9880 - mae: 44.8114 - val_loss: 14640.2119 - val_mae: 80.2583\n",
      "Epoch 497/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 3511.2810 - mae: 44.7819 - val_loss: 14635.3984 - val_mae: 80.2283\n",
      "Epoch 498/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 3508.6121 - mae: 44.7527 - val_loss: 14630.5898 - val_mae: 80.1983\n",
      "Epoch 499/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 3505.9236 - mae: 44.7239 - val_loss: 14625.8105 - val_mae: 80.1685\n",
      "Epoch 500/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 3503.1965 - mae: 44.6944 - val_loss: 14620.8799 - val_mae: 80.1378\n",
      "Epoch 501/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - loss: 3500.5474 - mae: 44.6649 - val_loss: 14616.1035 - val_mae: 80.1080\n",
      "Epoch 502/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - loss: 3497.9146 - mae: 44.6366 - val_loss: 14611.4355 - val_mae: 80.0788\n",
      "Epoch 503/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 3495.2944 - mae: 44.6082 - val_loss: 14606.7334 - val_mae: 80.0495\n",
      "Epoch 504/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - loss: 3492.6411 - mae: 44.5793 - val_loss: 14601.9775 - val_mae: 80.0198\n",
      "Epoch 505/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - loss: 3489.9829 - mae: 44.5504 - val_loss: 14597.1660 - val_mae: 79.9897\n",
      "Epoch 506/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - loss: 3487.3433 - mae: 44.5217 - val_loss: 14592.4678 - val_mae: 79.9603\n",
      "Epoch 507/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 3484.7061 - mae: 44.4924 - val_loss: 14587.6055 - val_mae: 79.9299\n",
      "Epoch 508/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - loss: 3482.0554 - mae: 44.4635 - val_loss: 14582.8984 - val_mae: 79.9004\n",
      "Epoch 509/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 3479.4146 - mae: 44.4351 - val_loss: 14578.1689 - val_mae: 79.8708\n",
      "Epoch 510/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 3476.7209 - mae: 44.4056 - val_loss: 14573.2852 - val_mae: 79.8403\n",
      "Epoch 511/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 3474.0364 - mae: 44.3760 - val_loss: 14568.4346 - val_mae: 79.8099\n",
      "Epoch 512/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 3471.3767 - mae: 44.3471 - val_loss: 14563.6602 - val_mae: 79.7800\n",
      "Epoch 513/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 3468.6995 - mae: 44.3180 - val_loss: 14558.8701 - val_mae: 79.7499\n",
      "Epoch 514/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - loss: 3466.0508 - mae: 44.2887 - val_loss: 14554.0713 - val_mae: 79.7199\n",
      "Epoch 515/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 3463.4241 - mae: 44.2594 - val_loss: 14549.2324 - val_mae: 79.6895\n",
      "Epoch 516/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 3460.7322 - mae: 44.2301 - val_loss: 14544.4512 - val_mae: 79.6595\n",
      "Epoch 517/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 3458.1338 - mae: 44.2017 - val_loss: 14539.7764 - val_mae: 79.6301\n",
      "Epoch 518/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 3455.5010 - mae: 44.1726 - val_loss: 14534.9795 - val_mae: 79.6000\n",
      "Epoch 519/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - loss: 3452.8103 - mae: 44.1435 - val_loss: 14530.1660 - val_mae: 79.5698\n",
      "Epoch 520/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - loss: 3450.2065 - mae: 44.1139 - val_loss: 14525.3555 - val_mae: 79.5396\n",
      "Epoch 521/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - loss: 3447.5266 - mae: 44.0852 - val_loss: 14520.6768 - val_mae: 79.5101\n",
      "Epoch 522/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 59ms/step - loss: 3444.9473 - mae: 44.0568 - val_loss: 14515.9893 - val_mae: 79.4806\n",
      "Epoch 523/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 3442.3450 - mae: 44.0280 - val_loss: 14511.2793 - val_mae: 79.4510\n",
      "Epoch 524/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 3439.7622 - mae: 43.9998 - val_loss: 14506.6377 - val_mae: 79.4218\n",
      "Epoch 525/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - loss: 3437.1816 - mae: 43.9710 - val_loss: 14501.9160 - val_mae: 79.3921\n",
      "Epoch 526/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 3434.5835 - mae: 43.9425 - val_loss: 14497.2471 - val_mae: 79.3627\n",
      "Epoch 527/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - loss: 3432.0129 - mae: 43.9141 - val_loss: 14492.6016 - val_mae: 79.3334\n",
      "Epoch 528/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 3429.4404 - mae: 43.8857 - val_loss: 14487.9561 - val_mae: 79.3041\n",
      "Epoch 529/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - loss: 3426.8638 - mae: 43.8571 - val_loss: 14483.2158 - val_mae: 79.2742\n",
      "Epoch 530/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - loss: 3424.2673 - mae: 43.8286 - val_loss: 14478.5283 - val_mae: 79.2446\n",
      "Epoch 531/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - loss: 3421.6907 - mae: 43.8001 - val_loss: 14473.8730 - val_mae: 79.2153\n",
      "Epoch 532/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 3419.0273 - mae: 43.7710 - val_loss: 14469.0283 - val_mae: 79.1847\n",
      "Epoch 533/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 3416.3875 - mae: 43.7412 - val_loss: 14464.1719 - val_mae: 79.1540\n",
      "Epoch 534/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 3413.7371 - mae: 43.7123 - val_loss: 14459.4834 - val_mae: 79.1244\n",
      "Epoch 535/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 3411.1418 - mae: 43.6839 - val_loss: 14454.7832 - val_mae: 79.0947\n",
      "Epoch 536/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 3408.5229 - mae: 43.6548 - val_loss: 14449.9707 - val_mae: 79.0642\n",
      "Epoch 537/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 3405.8867 - mae: 43.6257 - val_loss: 14445.2021 - val_mae: 79.0341\n",
      "Epoch 538/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 3403.2913 - mae: 43.5965 - val_loss: 14440.4307 - val_mae: 79.0039\n",
      "Epoch 539/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 3400.6685 - mae: 43.5674 - val_loss: 14435.6875 - val_mae: 78.9739\n",
      "Epoch 540/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 74ms/step - loss: 3398.0818 - mae: 43.5387 - val_loss: 14431.0010 - val_mae: 78.9442\n",
      "Epoch 541/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - loss: 3395.4944 - mae: 43.5105 - val_loss: 14426.3467 - val_mae: 78.9147\n",
      "Epoch 542/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - loss: 3392.9353 - mae: 43.4819 - val_loss: 14421.6875 - val_mae: 78.8852\n",
      "Epoch 543/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 3390.3372 - mae: 43.4531 - val_loss: 14416.9229 - val_mae: 78.8550\n",
      "Epoch 544/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - loss: 3387.7131 - mae: 43.4238 - val_loss: 14412.1494 - val_mae: 78.8247\n",
      "Epoch 545/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 3385.1360 - mae: 43.3952 - val_loss: 14407.4883 - val_mae: 78.7951\n",
      "Epoch 546/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 3382.5508 - mae: 43.3661 - val_loss: 14402.7188 - val_mae: 78.7648\n",
      "Epoch 547/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 3379.9561 - mae: 43.3376 - val_loss: 14398.0205 - val_mae: 78.7350\n",
      "Epoch 548/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 3377.3267 - mae: 43.3085 - val_loss: 14393.2480 - val_mae: 78.7047\n",
      "Epoch 549/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 3374.7632 - mae: 43.2795 - val_loss: 14388.4814 - val_mae: 78.6744\n",
      "Epoch 550/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 3372.1401 - mae: 43.2503 - val_loss: 14383.7324 - val_mae: 78.6442\n",
      "Epoch 551/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 3369.5588 - mae: 43.2219 - val_loss: 14379.0986 - val_mae: 78.6148\n",
      "Epoch 552/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 3367.0044 - mae: 43.1931 - val_loss: 14374.3945 - val_mae: 78.5848\n",
      "Epoch 553/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 3364.4290 - mae: 43.1645 - val_loss: 14369.6943 - val_mae: 78.5549\n",
      "Epoch 554/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 3361.8789 - mae: 43.1362 - val_loss: 14365.1016 - val_mae: 78.5257\n",
      "Epoch 555/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 3359.3555 - mae: 43.1080 - val_loss: 14360.4678 - val_mae: 78.4962\n",
      "Epoch 556/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 3356.8013 - mae: 43.0796 - val_loss: 14355.7783 - val_mae: 78.4663\n",
      "Epoch 557/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 3354.2539 - mae: 43.0514 - val_loss: 14351.2070 - val_mae: 78.4372\n",
      "Epoch 558/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 3351.7114 - mae: 43.0226 - val_loss: 14346.4307 - val_mae: 78.4067\n",
      "Epoch 559/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 3349.0940 - mae: 42.9933 - val_loss: 14341.6816 - val_mae: 78.3764\n",
      "Epoch 560/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 3346.5063 - mae: 42.9643 - val_loss: 14336.9570 - val_mae: 78.3463\n",
      "Epoch 561/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 3343.9263 - mae: 42.9356 - val_loss: 14332.3262 - val_mae: 78.3167\n",
      "Epoch 562/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 3341.3728 - mae: 42.9070 - val_loss: 14327.6055 - val_mae: 78.2866\n",
      "Epoch 563/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 3338.8223 - mae: 42.8779 - val_loss: 14322.8594 - val_mae: 78.2563\n",
      "Epoch 564/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 3336.1841 - mae: 42.8483 - val_loss: 14318.0381 - val_mae: 78.2255\n",
      "Epoch 565/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 3333.6460 - mae: 42.8197 - val_loss: 14313.3994 - val_mae: 78.1958\n",
      "Epoch 566/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 3331.0715 - mae: 42.7908 - val_loss: 14308.6816 - val_mae: 78.1656\n",
      "Epoch 567/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 3328.5000 - mae: 42.7622 - val_loss: 14304.0449 - val_mae: 78.1360\n",
      "Epoch 568/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - loss: 3325.9492 - mae: 42.7330 - val_loss: 14299.2422 - val_mae: 78.1052\n",
      "Epoch 569/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - loss: 3323.3804 - mae: 42.7047 - val_loss: 14294.6934 - val_mae: 78.0761\n",
      "Epoch 570/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 3320.8967 - mae: 42.6761 - val_loss: 14290.0391 - val_mae: 78.0463\n",
      "Epoch 571/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 3318.3901 - mae: 42.6480 - val_loss: 14285.5146 - val_mae: 78.0173\n",
      "Epoch 572/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 3315.8945 - mae: 42.6200 - val_loss: 14280.9092 - val_mae: 77.9878\n",
      "Epoch 573/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 3313.4006 - mae: 42.5916 - val_loss: 14276.2900 - val_mae: 77.9582\n",
      "Epoch 574/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 3310.8394 - mae: 42.5631 - val_loss: 14271.6006 - val_mae: 77.9281\n",
      "Epoch 575/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 3308.2896 - mae: 42.5339 - val_loss: 14266.8564 - val_mae: 77.8976\n",
      "Epoch 576/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 3305.6973 - mae: 42.5048 - val_loss: 14262.1309 - val_mae: 77.8673\n",
      "Epoch 577/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 3303.1599 - mae: 42.4758 - val_loss: 14257.4111 - val_mae: 77.8370\n",
      "Epoch 578/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 3300.6453 - mae: 42.4478 - val_loss: 14252.9424 - val_mae: 77.8083\n",
      "Epoch 579/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 3298.1321 - mae: 42.4193 - val_loss: 14248.2666 - val_mae: 77.7782\n",
      "Epoch 580/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - loss: 3295.6399 - mae: 42.3900 - val_loss: 14243.4707 - val_mae: 77.7474\n",
      "Epoch 581/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 3293.0598 - mae: 42.3616 - val_loss: 14238.8848 - val_mae: 77.7179\n",
      "Epoch 582/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 3290.5715 - mae: 42.3338 - val_loss: 14234.3447 - val_mae: 77.6887\n",
      "Epoch 583/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 3288.0652 - mae: 42.3050 - val_loss: 14229.6270 - val_mae: 77.6583\n",
      "Epoch 584/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 3285.5613 - mae: 42.2768 - val_loss: 14225.0928 - val_mae: 77.6291\n",
      "Epoch 585/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 3283.0701 - mae: 42.2486 - val_loss: 14220.4355 - val_mae: 77.5991\n",
      "Epoch 586/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 3280.5791 - mae: 42.2207 - val_loss: 14215.9229 - val_mae: 77.5700\n",
      "Epoch 587/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 3278.1047 - mae: 42.1925 - val_loss: 14211.3311 - val_mae: 77.5404\n",
      "Epoch 588/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 3275.6094 - mae: 42.1641 - val_loss: 14206.6885 - val_mae: 77.5105\n",
      "Epoch 589/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 3273.1208 - mae: 42.1359 - val_loss: 14202.1348 - val_mae: 77.4811\n",
      "Epoch 590/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 3270.6699 - mae: 42.1082 - val_loss: 14197.5947 - val_mae: 77.4518\n",
      "Epoch 591/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 3268.1404 - mae: 42.0797 - val_loss: 14192.9209 - val_mae: 77.4216\n",
      "Epoch 592/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 3265.6023 - mae: 42.0513 - val_loss: 14188.3115 - val_mae: 77.3918\n",
      "Epoch 593/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 3263.1514 - mae: 42.0232 - val_loss: 14183.7139 - val_mae: 77.3621\n",
      "Epoch 594/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 3260.6768 - mae: 41.9953 - val_loss: 14179.1680 - val_mae: 77.3327\n",
      "Epoch 595/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 3258.1482 - mae: 41.9665 - val_loss: 14174.4072 - val_mae: 77.3019\n",
      "Epoch 596/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 3255.6147 - mae: 41.9374 - val_loss: 14169.7383 - val_mae: 77.2717\n",
      "Epoch 597/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 3253.0466 - mae: 41.9083 - val_loss: 14164.9951 - val_mae: 77.2410\n",
      "Epoch 598/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 3250.5627 - mae: 41.8800 - val_loss: 14160.3770 - val_mae: 77.2111\n",
      "Epoch 599/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 3248.0173 - mae: 41.8506 - val_loss: 14155.6445 - val_mae: 77.1805\n",
      "Epoch 600/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 3245.4922 - mae: 41.8221 - val_loss: 14151.0303 - val_mae: 77.1506\n",
      "Epoch 601/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 3243.0259 - mae: 41.7943 - val_loss: 14146.5264 - val_mae: 77.1214\n",
      "Epoch 602/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 3240.6035 - mae: 41.7665 - val_loss: 14141.9922 - val_mae: 77.0920\n",
      "Epoch 603/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - loss: 3238.1694 - mae: 41.7384 - val_loss: 14137.4502 - val_mae: 77.0625\n",
      "Epoch 604/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 3235.7183 - mae: 41.7108 - val_loss: 14132.9551 - val_mae: 77.0334\n",
      "Epoch 605/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 3233.3086 - mae: 41.6829 - val_loss: 14128.5078 - val_mae: 77.0045\n",
      "Epoch 606/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step - loss: 3230.8904 - mae: 41.6549 - val_loss: 14123.8896 - val_mae: 76.9745\n",
      "Epoch 607/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - loss: 3228.4233 - mae: 41.6273 - val_loss: 14119.4385 - val_mae: 76.9456\n",
      "Epoch 608/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - loss: 3225.9797 - mae: 41.5988 - val_loss: 14114.7734 - val_mae: 76.9153\n",
      "Epoch 609/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - loss: 3223.4727 - mae: 41.5708 - val_loss: 14110.2773 - val_mae: 76.8860\n",
      "Epoch 610/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - loss: 3221.0195 - mae: 41.5420 - val_loss: 14105.5908 - val_mae: 76.8556\n",
      "Epoch 611/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - loss: 3218.5645 - mae: 41.5136 - val_loss: 14100.9844 - val_mae: 76.8256\n",
      "Epoch 612/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 3216.0322 - mae: 41.4850 - val_loss: 14096.3594 - val_mae: 76.7955\n",
      "Epoch 613/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 3213.5913 - mae: 41.4571 - val_loss: 14091.8340 - val_mae: 76.7660\n",
      "Epoch 614/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 3211.1555 - mae: 41.4293 - val_loss: 14087.3271 - val_mae: 76.7366\n",
      "Epoch 615/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 3208.6982 - mae: 41.4005 - val_loss: 14082.6338 - val_mae: 76.7060\n",
      "Epoch 616/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 3206.2009 - mae: 41.3723 - val_loss: 14078.0566 - val_mae: 76.6762\n",
      "Epoch 617/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - loss: 3203.7246 - mae: 41.3437 - val_loss: 14073.3936 - val_mae: 76.6458\n",
      "Epoch 618/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 3201.2710 - mae: 41.3151 - val_loss: 14068.7822 - val_mae: 76.6157\n",
      "Epoch 619/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - loss: 3198.8210 - mae: 41.2876 - val_loss: 14064.4014 - val_mae: 76.5871\n",
      "Epoch 620/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 3196.4224 - mae: 41.2592 - val_loss: 14059.7676 - val_mae: 76.5569\n",
      "Epoch 621/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 3193.9419 - mae: 41.2310 - val_loss: 14055.1650 - val_mae: 76.5268\n",
      "Epoch 622/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - loss: 3191.4883 - mae: 41.2021 - val_loss: 14050.5107 - val_mae: 76.4964\n",
      "Epoch 623/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - loss: 3189.0212 - mae: 41.1747 - val_loss: 14046.0469 - val_mae: 76.4672\n",
      "Epoch 624/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 3186.5940 - mae: 41.1462 - val_loss: 14041.4326 - val_mae: 76.4370\n",
      "Epoch 625/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 3184.1328 - mae: 41.1182 - val_loss: 14036.8760 - val_mae: 76.4072\n",
      "Epoch 626/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 3181.6838 - mae: 41.0897 - val_loss: 14032.2383 - val_mae: 76.3768\n",
      "Epoch 627/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 3179.2219 - mae: 41.0613 - val_loss: 14027.6816 - val_mae: 76.3470\n",
      "Epoch 628/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 3176.7930 - mae: 41.0329 - val_loss: 14023.0596 - val_mae: 76.3167\n",
      "Epoch 629/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 3174.3438 - mae: 41.0052 - val_loss: 14018.6611 - val_mae: 76.2879\n",
      "Epoch 630/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 3171.9553 - mae: 40.9774 - val_loss: 14014.0557 - val_mae: 76.2577\n",
      "Epoch 631/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 3169.5071 - mae: 40.9487 - val_loss: 14009.4961 - val_mae: 76.2278\n",
      "Epoch 632/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step - loss: 3167.0881 - mae: 40.9213 - val_loss: 14005.0723 - val_mae: 76.1988\n",
      "Epoch 633/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 3164.6619 - mae: 40.8930 - val_loss: 14000.4072 - val_mae: 76.1682\n",
      "Epoch 634/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 3162.2310 - mae: 40.8647 - val_loss: 13995.8896 - val_mae: 76.1385\n",
      "Epoch 635/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - loss: 3159.8047 - mae: 40.8363 - val_loss: 13991.2900 - val_mae: 76.1083\n",
      "Epoch 636/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - loss: 3157.4084 - mae: 40.8090 - val_loss: 13986.8818 - val_mae: 76.0793\n",
      "Epoch 637/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - loss: 3155.0552 - mae: 40.7811 - val_loss: 13982.4287 - val_mae: 76.0501\n",
      "Epoch 638/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 3152.6865 - mae: 40.7539 - val_loss: 13977.9668 - val_mae: 76.0207\n",
      "Epoch 639/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 3150.2744 - mae: 40.7262 - val_loss: 13973.4795 - val_mae: 75.9912\n",
      "Epoch 640/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 3147.8640 - mae: 40.6983 - val_loss: 13968.9238 - val_mae: 75.9612\n",
      "Epoch 641/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 3145.4917 - mae: 40.6709 - val_loss: 13964.5273 - val_mae: 75.9323\n",
      "Epoch 642/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 3143.0837 - mae: 40.6428 - val_loss: 13959.8994 - val_mae: 75.9018\n",
      "Epoch 643/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - loss: 3140.6562 - mae: 40.6149 - val_loss: 13955.4297 - val_mae: 75.8723\n",
      "Epoch 644/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 3138.2297 - mae: 40.5868 - val_loss: 13950.8311 - val_mae: 75.8420\n",
      "Epoch 645/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 3135.8191 - mae: 40.5586 - val_loss: 13946.3037 - val_mae: 75.8122\n",
      "Epoch 646/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - loss: 3133.4307 - mae: 40.5308 - val_loss: 13941.7686 - val_mae: 75.7823\n",
      "Epoch 647/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - loss: 3131.0403 - mae: 40.5032 - val_loss: 13937.3730 - val_mae: 75.7533\n",
      "Epoch 648/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 64ms/step - loss: 3128.6899 - mae: 40.4756 - val_loss: 13932.8594 - val_mae: 75.7235\n",
      "Epoch 649/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - loss: 3126.2791 - mae: 40.4478 - val_loss: 13928.3271 - val_mae: 75.6935\n",
      "Epoch 650/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - loss: 3123.9185 - mae: 40.4204 - val_loss: 13923.9551 - val_mae: 75.6646\n",
      "Epoch 651/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 3121.5386 - mae: 40.3926 - val_loss: 13919.3574 - val_mae: 75.6343\n",
      "Epoch 652/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 3119.0405 - mae: 40.3636 - val_loss: 13914.6279 - val_mae: 75.6030\n",
      "Epoch 653/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - loss: 3116.5903 - mae: 40.3352 - val_loss: 13910.0449 - val_mae: 75.5727\n",
      "Epoch 654/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - loss: 3114.1624 - mae: 40.3069 - val_loss: 13905.4844 - val_mae: 75.5425\n",
      "Epoch 655/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - loss: 3111.7935 - mae: 40.2792 - val_loss: 13901.0195 - val_mae: 75.5129\n",
      "Epoch 656/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step - loss: 3109.4165 - mae: 40.2514 - val_loss: 13896.5049 - val_mae: 75.4830\n",
      "Epoch 657/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 3107.0312 - mae: 40.2234 - val_loss: 13891.9824 - val_mae: 75.4531\n",
      "Epoch 658/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 3104.6628 - mae: 40.1960 - val_loss: 13887.5488 - val_mae: 75.4237\n",
      "Epoch 659/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 3102.2629 - mae: 40.1679 - val_loss: 13882.9883 - val_mae: 75.3935\n",
      "Epoch 660/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - loss: 3099.8757 - mae: 40.1403 - val_loss: 13878.5312 - val_mae: 75.3639\n",
      "Epoch 661/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 3097.5469 - mae: 40.1127 - val_loss: 13874.0762 - val_mae: 75.3343\n",
      "Epoch 662/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 3095.2131 - mae: 40.0857 - val_loss: 13869.7285 - val_mae: 75.3055\n",
      "Epoch 663/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 3092.9065 - mae: 40.0590 - val_loss: 13865.3525 - val_mae: 75.2764\n",
      "Epoch 664/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 3090.5837 - mae: 40.0315 - val_loss: 13860.9004 - val_mae: 75.2468\n",
      "Epoch 665/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step - loss: 3088.2534 - mae: 40.0048 - val_loss: 13856.5371 - val_mae: 75.2178\n",
      "Epoch 666/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - loss: 3085.9077 - mae: 39.9770 - val_loss: 13852.0117 - val_mae: 75.1877\n",
      "Epoch 667/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 112ms/step - loss: 3083.5588 - mae: 39.9502 - val_loss: 13847.6602 - val_mae: 75.1588\n",
      "Epoch 668/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 86ms/step - loss: 3081.2402 - mae: 39.9228 - val_loss: 13843.1484 - val_mae: 75.1288\n",
      "Epoch 669/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - loss: 3078.8699 - mae: 39.8953 - val_loss: 13838.6992 - val_mae: 75.0992\n",
      "Epoch 670/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step - loss: 3076.5232 - mae: 39.8678 - val_loss: 13834.2559 - val_mae: 75.0696\n",
      "Epoch 671/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - loss: 3074.1731 - mae: 39.8410 - val_loss: 13829.8369 - val_mae: 75.0401\n",
      "Epoch 672/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 3071.8193 - mae: 39.8132 - val_loss: 13825.2617 - val_mae: 75.0096\n",
      "Epoch 673/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 3069.4504 - mae: 39.7857 - val_loss: 13820.8389 - val_mae: 74.9801\n",
      "Epoch 674/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 3067.0715 - mae: 39.7580 - val_loss: 13816.3203 - val_mae: 74.9500\n",
      "Epoch 675/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - loss: 3064.7068 - mae: 39.7298 - val_loss: 13811.7441 - val_mae: 74.9195\n",
      "Epoch 676/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - loss: 3062.3623 - mae: 39.7030 - val_loss: 13807.3955 - val_mae: 74.8905\n",
      "Epoch 677/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 3060.0471 - mae: 39.6761 - val_loss: 13803.0215 - val_mae: 74.8612\n",
      "Epoch 678/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - loss: 3057.7588 - mae: 39.6491 - val_loss: 13798.5811 - val_mae: 74.8316\n",
      "Epoch 679/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 3055.3794 - mae: 39.6216 - val_loss: 13794.0996 - val_mae: 74.8016\n",
      "Epoch 680/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step - loss: 3052.8015 - mae: 39.5911 - val_loss: 13789.0010 - val_mae: 74.7675\n",
      "Epoch 681/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - loss: 3050.4075 - mae: 39.5628 - val_loss: 13784.6592 - val_mae: 74.7385\n",
      "Epoch 682/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 3048.1536 - mae: 39.5364 - val_loss: 13780.3291 - val_mae: 74.7095\n",
      "Epoch 683/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - loss: 3045.9014 - mae: 39.5096 - val_loss: 13776.0449 - val_mae: 74.6808\n",
      "Epoch 684/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - loss: 3043.6008 - mae: 39.4825 - val_loss: 13771.6025 - val_mae: 74.6511\n",
      "Epoch 685/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - loss: 3041.3198 - mae: 39.4564 - val_loss: 13767.3145 - val_mae: 74.6224\n",
      "Epoch 686/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - loss: 3038.9956 - mae: 39.4291 - val_loss: 13762.8623 - val_mae: 74.5926\n",
      "Epoch 687/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - loss: 3036.7200 - mae: 39.4024 - val_loss: 13758.5361 - val_mae: 74.5635\n",
      "Epoch 688/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 3034.4209 - mae: 39.3759 - val_loss: 13754.1426 - val_mae: 74.5341\n",
      "Epoch 689/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 3032.0950 - mae: 39.3482 - val_loss: 13749.6348 - val_mae: 74.5038\n",
      "Epoch 690/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - loss: 3029.8208 - mae: 39.3216 - val_loss: 13745.3467 - val_mae: 74.4750\n",
      "Epoch 691/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - loss: 3027.5657 - mae: 39.2954 - val_loss: 13741.0439 - val_mae: 74.4461\n",
      "Epoch 692/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - loss: 3025.3030 - mae: 39.2687 - val_loss: 13736.7383 - val_mae: 74.4172\n",
      "Epoch 693/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - loss: 3023.0129 - mae: 39.2414 - val_loss: 13732.1953 - val_mae: 74.3867\n",
      "Epoch 694/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - loss: 3020.6750 - mae: 39.2144 - val_loss: 13727.8379 - val_mae: 74.3574\n",
      "Epoch 695/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - loss: 3018.3835 - mae: 39.1877 - val_loss: 13723.5146 - val_mae: 74.3283\n",
      "Epoch 696/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - loss: 3016.1223 - mae: 39.1609 - val_loss: 13719.1064 - val_mae: 74.2986\n",
      "Epoch 697/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 3013.8281 - mae: 39.1338 - val_loss: 13714.7031 - val_mae: 74.2690\n",
      "Epoch 698/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 3011.4651 - mae: 39.1063 - val_loss: 13710.2100 - val_mae: 74.2388\n",
      "Epoch 699/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 3009.1746 - mae: 39.0797 - val_loss: 13705.8730 - val_mae: 74.2095\n",
      "Epoch 700/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 3006.9326 - mae: 39.0527 - val_loss: 13701.5195 - val_mae: 74.1802\n",
      "Epoch 701/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 3004.6658 - mae: 39.0265 - val_loss: 13697.2236 - val_mae: 74.1513\n",
      "Epoch 702/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 3002.4065 - mae: 38.9996 - val_loss: 13692.8223 - val_mae: 74.1216\n",
      "Epoch 703/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - loss: 3000.1184 - mae: 38.9726 - val_loss: 13688.4160 - val_mae: 74.0918\n",
      "Epoch 704/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 2997.8220 - mae: 38.9452 - val_loss: 13683.9355 - val_mae: 74.0616\n",
      "Epoch 705/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - loss: 2995.5010 - mae: 38.9186 - val_loss: 13679.5527 - val_mae: 74.0320\n",
      "Epoch 706/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - loss: 2993.1509 - mae: 38.8909 - val_loss: 13675.0020 - val_mae: 74.0013\n",
      "Epoch 707/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 2990.8281 - mae: 38.8635 - val_loss: 13670.5635 - val_mae: 73.9712\n",
      "Epoch 708/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 2988.5288 - mae: 38.8366 - val_loss: 13666.1934 - val_mae: 73.9417\n",
      "Epoch 709/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 2986.3076 - mae: 38.8105 - val_loss: 13661.8965 - val_mae: 73.9126\n",
      "Epoch 710/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 2984.0994 - mae: 38.7839 - val_loss: 13657.5820 - val_mae: 73.8835\n",
      "Epoch 711/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - loss: 2981.8298 - mae: 38.7573 - val_loss: 13653.2227 - val_mae: 73.8540\n",
      "Epoch 712/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - loss: 2979.5071 - mae: 38.7302 - val_loss: 13648.7549 - val_mae: 73.8237\n",
      "Epoch 713/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - loss: 2977.2512 - mae: 38.7035 - val_loss: 13644.4326 - val_mae: 73.7944\n",
      "Epoch 714/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 2974.9912 - mae: 38.6775 - val_loss: 13640.0859 - val_mae: 73.7650\n",
      "Epoch 715/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 2972.7808 - mae: 38.6512 - val_loss: 13635.8574 - val_mae: 73.7363\n",
      "Epoch 716/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 2970.5757 - mae: 38.6255 - val_loss: 13631.6631 - val_mae: 73.7078\n",
      "Epoch 717/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 2968.4023 - mae: 38.5999 - val_loss: 13627.3564 - val_mae: 73.6786\n",
      "Epoch 718/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 2966.2019 - mae: 38.5740 - val_loss: 13623.0850 - val_mae: 73.6496\n",
      "Epoch 719/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - loss: 2963.9700 - mae: 38.5486 - val_loss: 13618.8613 - val_mae: 73.6210\n",
      "Epoch 720/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 2961.7161 - mae: 38.5219 - val_loss: 13614.5234 - val_mae: 73.5915\n",
      "Epoch 721/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 2959.4851 - mae: 38.4955 - val_loss: 13610.1289 - val_mae: 73.5616\n",
      "Epoch 722/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - loss: 2957.2241 - mae: 38.4693 - val_loss: 13605.8232 - val_mae: 73.5324\n",
      "Epoch 723/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 2954.9805 - mae: 38.4427 - val_loss: 13601.4053 - val_mae: 73.5023\n",
      "Epoch 724/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 2952.7195 - mae: 38.4161 - val_loss: 13597.0537 - val_mae: 73.4727\n",
      "Epoch 725/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 2950.4875 - mae: 38.3895 - val_loss: 13592.6592 - val_mae: 73.4428\n",
      "Epoch 726/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 2948.2812 - mae: 38.3638 - val_loss: 13588.4736 - val_mae: 73.4143\n",
      "Epoch 727/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 2946.0662 - mae: 38.3380 - val_loss: 13584.2422 - val_mae: 73.3855\n",
      "Epoch 728/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 2943.8074 - mae: 38.3113 - val_loss: 13579.7148 - val_mae: 73.3546\n",
      "Epoch 729/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - loss: 2941.3035 - mae: 38.2813 - val_loss: 13574.7666 - val_mae: 73.3209\n",
      "Epoch 730/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - loss: 2939.0186 - mae: 38.2546 - val_loss: 13570.5381 - val_mae: 73.2920\n",
      "Epoch 731/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 2936.8411 - mae: 38.2293 - val_loss: 13566.3955 - val_mae: 73.2638\n",
      "Epoch 732/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 2934.6814 - mae: 38.2035 - val_loss: 13562.0762 - val_mae: 73.2343\n",
      "Epoch 733/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 2932.4158 - mae: 38.1772 - val_loss: 13557.7129 - val_mae: 73.2045\n",
      "Epoch 734/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - loss: 2930.2244 - mae: 38.1513 - val_loss: 13553.4668 - val_mae: 73.1755\n",
      "Epoch 735/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - loss: 2928.0344 - mae: 38.1257 - val_loss: 13549.2422 - val_mae: 73.1466\n",
      "Epoch 736/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step - loss: 2925.8027 - mae: 38.0994 - val_loss: 13544.9014 - val_mae: 73.1169\n",
      "Epoch 737/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step - loss: 2923.6677 - mae: 38.0741 - val_loss: 13540.7314 - val_mae: 73.0884\n",
      "Epoch 738/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 2921.5332 - mae: 38.0496 - val_loss: 13536.6494 - val_mae: 73.0605\n",
      "Epoch 739/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - loss: 2919.4236 - mae: 38.0246 - val_loss: 13532.5078 - val_mae: 73.0321\n",
      "Epoch 740/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step - loss: 2917.2668 - mae: 37.9987 - val_loss: 13528.1729 - val_mae: 73.0024\n",
      "Epoch 741/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 2915.0349 - mae: 37.9730 - val_loss: 13523.9033 - val_mae: 72.9732\n",
      "Epoch 742/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 2912.8533 - mae: 37.9476 - val_loss: 13519.6631 - val_mae: 72.9441\n",
      "Epoch 743/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step - loss: 2910.6614 - mae: 37.9220 - val_loss: 13515.3350 - val_mae: 72.9145\n",
      "Epoch 744/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step - loss: 2908.4609 - mae: 37.8966 - val_loss: 13511.1875 - val_mae: 72.8860\n",
      "Epoch 745/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step - loss: 2906.3325 - mae: 37.8714 - val_loss: 13506.9658 - val_mae: 72.8570\n",
      "Epoch 746/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - loss: 2904.1831 - mae: 37.8462 - val_loss: 13502.7852 - val_mae: 72.8284\n",
      "Epoch 747/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - loss: 2901.9790 - mae: 37.8202 - val_loss: 13498.3213 - val_mae: 72.7977\n",
      "Epoch 748/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - loss: 2899.7351 - mae: 37.7940 - val_loss: 13494.0049 - val_mae: 72.7680\n",
      "Epoch 749/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - loss: 2897.5273 - mae: 37.7682 - val_loss: 13489.7607 - val_mae: 72.7389\n",
      "Epoch 750/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 2895.4050 - mae: 37.7435 - val_loss: 13485.6270 - val_mae: 72.7104\n",
      "Epoch 751/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 2893.2542 - mae: 37.7186 - val_loss: 13481.3994 - val_mae: 72.6814\n",
      "Epoch 752/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 2891.1101 - mae: 37.6939 - val_loss: 13477.2715 - val_mae: 72.6530\n",
      "Epoch 753/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 2888.9565 - mae: 37.6683 - val_loss: 13472.9062 - val_mae: 72.6229\n",
      "Epoch 754/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 2886.7026 - mae: 37.6422 - val_loss: 13468.6113 - val_mae: 72.5933\n",
      "Epoch 755/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - loss: 2884.5664 - mae: 37.6168 - val_loss: 13464.4170 - val_mae: 72.5645\n",
      "Epoch 756/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 2882.4353 - mae: 37.5928 - val_loss: 13460.3457 - val_mae: 72.5364\n",
      "Epoch 757/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - loss: 2880.2800 - mae: 37.5670 - val_loss: 13455.9697 - val_mae: 72.5062\n",
      "Epoch 758/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 2878.1292 - mae: 37.5417 - val_loss: 13451.7998 - val_mae: 72.4775\n",
      "Epoch 759/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - loss: 2876.0173 - mae: 37.5168 - val_loss: 13447.6182 - val_mae: 72.4486\n",
      "Epoch 760/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - loss: 2873.8577 - mae: 37.4920 - val_loss: 13443.4561 - val_mae: 72.4199\n",
      "Epoch 761/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 2871.6804 - mae: 37.4659 - val_loss: 13439.0469 - val_mae: 72.3894\n",
      "Epoch 762/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 2869.5271 - mae: 37.4410 - val_loss: 13434.9346 - val_mae: 72.3610\n",
      "Epoch 763/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 2867.4265 - mae: 37.4161 - val_loss: 13430.7793 - val_mae: 72.3323\n",
      "Epoch 764/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 2865.3181 - mae: 37.3918 - val_loss: 13426.6650 - val_mae: 72.3039\n",
      "Epoch 765/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 2863.1619 - mae: 37.3661 - val_loss: 13422.4004 - val_mae: 72.2744\n",
      "Epoch 766/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 2861.0259 - mae: 37.3408 - val_loss: 13418.0986 - val_mae: 72.2446\n",
      "Epoch 767/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - loss: 2858.8215 - mae: 37.3149 - val_loss: 13413.8125 - val_mae: 72.2149\n",
      "Epoch 768/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - loss: 2856.6272 - mae: 37.2894 - val_loss: 13409.4805 - val_mae: 72.1849\n",
      "Epoch 769/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 2854.5068 - mae: 37.2644 - val_loss: 13405.3711 - val_mae: 72.1565\n",
      "Epoch 770/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 2852.3250 - mae: 37.2389 - val_loss: 13401.0186 - val_mae: 72.1263\n",
      "Epoch 771/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step - loss: 2850.2109 - mae: 37.2140 - val_loss: 13396.9141 - val_mae: 72.0978\n",
      "Epoch 772/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 2848.1077 - mae: 37.1893 - val_loss: 13392.7666 - val_mae: 72.0691\n",
      "Epoch 773/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - loss: 2845.9868 - mae: 37.1638 - val_loss: 13388.4814 - val_mae: 72.0393\n",
      "Epoch 774/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 2843.8201 - mae: 37.1395 - val_loss: 13384.4268 - val_mae: 72.0112\n",
      "Epoch 775/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - loss: 2841.7051 - mae: 37.1143 - val_loss: 13380.1143 - val_mae: 71.9812\n",
      "Epoch 776/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - loss: 2839.5532 - mae: 37.0883 - val_loss: 13375.7910 - val_mae: 71.9512\n",
      "Epoch 777/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - loss: 2837.3865 - mae: 37.0631 - val_loss: 13371.6172 - val_mae: 71.9222\n",
      "Epoch 778/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 2835.2593 - mae: 37.0384 - val_loss: 13367.4443 - val_mae: 71.8932\n",
      "Epoch 779/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - loss: 2833.1353 - mae: 37.0132 - val_loss: 13363.1807 - val_mae: 71.8635\n",
      "Epoch 780/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - loss: 2831.0029 - mae: 36.9875 - val_loss: 13358.9346 - val_mae: 71.8340\n",
      "Epoch 781/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step - loss: 2828.8679 - mae: 36.9626 - val_loss: 13354.7471 - val_mae: 71.8048\n",
      "Epoch 782/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step - loss: 2826.7502 - mae: 36.9377 - val_loss: 13350.5137 - val_mae: 71.7753\n",
      "Epoch 783/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step - loss: 2824.6338 - mae: 36.9128 - val_loss: 13346.3760 - val_mae: 71.7465\n",
      "Epoch 784/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - loss: 2822.4878 - mae: 36.8877 - val_loss: 13342.0957 - val_mae: 71.7167\n",
      "Epoch 785/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - loss: 2820.3154 - mae: 36.8623 - val_loss: 13337.8193 - val_mae: 71.6868\n",
      "Epoch 786/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - loss: 2818.2178 - mae: 36.8378 - val_loss: 13333.6777 - val_mae: 71.6580\n",
      "Epoch 787/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - loss: 2816.0928 - mae: 36.8130 - val_loss: 13329.4375 - val_mae: 71.6284\n",
      "Epoch 788/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - loss: 2813.9924 - mae: 36.7883 - val_loss: 13325.3076 - val_mae: 71.5995\n",
      "Epoch 789/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - loss: 2811.9070 - mae: 36.7641 - val_loss: 13321.1719 - val_mae: 71.5706\n",
      "Epoch 790/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - loss: 2809.7783 - mae: 36.7394 - val_loss: 13316.9346 - val_mae: 71.5410\n",
      "Epoch 791/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 60ms/step - loss: 2807.7002 - mae: 36.7144 - val_loss: 13312.7363 - val_mae: 71.5117\n",
      "Epoch 792/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - loss: 2805.6067 - mae: 36.6902 - val_loss: 13308.6162 - val_mae: 71.4828\n",
      "Epoch 793/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - loss: 2803.4680 - mae: 36.6653 - val_loss: 13304.4053 - val_mae: 71.4534\n",
      "Epoch 794/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 2801.3918 - mae: 36.6407 - val_loss: 13300.2549 - val_mae: 71.4244\n",
      "Epoch 795/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 2799.3330 - mae: 36.6164 - val_loss: 13296.1387 - val_mae: 71.3955\n",
      "Epoch 796/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 2797.2573 - mae: 36.5919 - val_loss: 13292.0312 - val_mae: 71.3668\n",
      "Epoch 797/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 2795.1702 - mae: 36.5678 - val_loss: 13287.8662 - val_mae: 71.3376\n",
      "Epoch 798/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 2793.0640 - mae: 36.5434 - val_loss: 13283.7021 - val_mae: 71.3084\n",
      "Epoch 799/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - loss: 2790.9714 - mae: 36.5191 - val_loss: 13279.5459 - val_mae: 71.2792\n",
      "Epoch 800/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 2788.9060 - mae: 36.4950 - val_loss: 13275.3877 - val_mae: 71.2501\n",
      "Epoch 801/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 2786.8027 - mae: 36.4702 - val_loss: 13271.1660 - val_mae: 71.2204\n",
      "Epoch 802/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - loss: 2784.6289 - mae: 36.4450 - val_loss: 13266.8037 - val_mae: 71.1898\n",
      "Epoch 803/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 2782.4758 - mae: 36.4198 - val_loss: 13262.5811 - val_mae: 71.1601\n",
      "Epoch 804/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 2780.4260 - mae: 36.3953 - val_loss: 13258.4277 - val_mae: 71.1310\n",
      "Epoch 805/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - loss: 2778.3672 - mae: 36.3718 - val_loss: 13254.4023 - val_mae: 71.1027\n",
      "Epoch 806/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 116ms/step - loss: 2776.3440 - mae: 36.3482 - val_loss: 13250.3506 - val_mae: 71.0741\n",
      "Epoch 807/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - loss: 2774.2971 - mae: 36.3244 - val_loss: 13246.2285 - val_mae: 71.0451\n",
      "Epoch 808/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - loss: 2772.1821 - mae: 36.2997 - val_loss: 13241.9668 - val_mae: 71.0151\n",
      "Epoch 809/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - loss: 2770.0635 - mae: 36.2753 - val_loss: 13237.7383 - val_mae: 70.9854\n",
      "Epoch 810/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 2768.0195 - mae: 36.2512 - val_loss: 13233.5664 - val_mae: 70.9560\n",
      "Epoch 811/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - loss: 2765.9453 - mae: 36.2272 - val_loss: 13229.5205 - val_mae: 70.9275\n",
      "Epoch 812/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 2763.8281 - mae: 36.2029 - val_loss: 13225.1904 - val_mae: 70.8969\n",
      "Epoch 813/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - loss: 2761.4836 - mae: 36.1752 - val_loss: 13220.3594 - val_mae: 70.8628\n",
      "Epoch 814/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - loss: 2759.3169 - mae: 36.1509 - val_loss: 13216.2549 - val_mae: 70.8339\n",
      "Epoch 815/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 118ms/step - loss: 2757.3062 - mae: 36.1275 - val_loss: 13212.2969 - val_mae: 70.8059\n",
      "Epoch 816/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - loss: 2755.2473 - mae: 36.1037 - val_loss: 13208.0791 - val_mae: 70.7761\n",
      "Epoch 817/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - loss: 2753.2002 - mae: 36.0800 - val_loss: 13204.0137 - val_mae: 70.7474\n",
      "Epoch 818/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - loss: 2751.1831 - mae: 36.0569 - val_loss: 13200.0186 - val_mae: 70.7192\n",
      "Epoch 819/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - loss: 2749.1843 - mae: 36.0332 - val_loss: 13195.8457 - val_mae: 70.6897\n",
      "Epoch 820/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - loss: 2747.1316 - mae: 36.0096 - val_loss: 13191.7832 - val_mae: 70.6609\n",
      "Epoch 821/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 130ms/step - loss: 2745.1399 - mae: 35.9869 - val_loss: 13187.7510 - val_mae: 70.6324\n",
      "Epoch 822/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - loss: 2743.1543 - mae: 35.9638 - val_loss: 13183.7256 - val_mae: 70.6039\n",
      "Epoch 823/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 2741.1121 - mae: 35.9410 - val_loss: 13179.7461 - val_mae: 70.5757\n",
      "Epoch 824/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 2739.0815 - mae: 35.9174 - val_loss: 13175.6211 - val_mae: 70.5465\n",
      "Epoch 825/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 2737.0410 - mae: 35.8938 - val_loss: 13171.4307 - val_mae: 70.5168\n",
      "Epoch 826/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - loss: 2735.0195 - mae: 35.8707 - val_loss: 13167.4355 - val_mae: 70.4884\n",
      "Epoch 827/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - loss: 2733.0554 - mae: 35.8482 - val_loss: 13163.4834 - val_mae: 70.4604\n",
      "Epoch 828/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - loss: 2731.0166 - mae: 35.8252 - val_loss: 13159.2842 - val_mae: 70.4306\n",
      "Epoch 829/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 2728.9807 - mae: 35.8021 - val_loss: 13155.2656 - val_mae: 70.4021\n",
      "Epoch 830/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 2726.9507 - mae: 35.7784 - val_loss: 13151.0898 - val_mae: 70.3724\n",
      "Epoch 831/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 2724.8289 - mae: 35.7541 - val_loss: 13146.7490 - val_mae: 70.3415\n",
      "Epoch 832/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - loss: 2722.7625 - mae: 35.7311 - val_loss: 13142.7334 - val_mae: 70.3130\n",
      "Epoch 833/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 2720.7908 - mae: 35.7084 - val_loss: 13138.6699 - val_mae: 70.2841\n",
      "Epoch 834/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 2718.7566 - mae: 35.6852 - val_loss: 13134.6562 - val_mae: 70.2555\n",
      "Epoch 835/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 2716.7805 - mae: 35.6626 - val_loss: 13130.6094 - val_mae: 70.2267\n",
      "Epoch 836/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 2714.8162 - mae: 35.6397 - val_loss: 13126.5986 - val_mae: 70.1982\n",
      "Epoch 837/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - loss: 2712.8306 - mae: 35.6168 - val_loss: 13122.5381 - val_mae: 70.1692\n",
      "Epoch 838/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 2710.8313 - mae: 35.5943 - val_loss: 13118.5244 - val_mae: 70.1406\n",
      "Epoch 839/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 2708.8010 - mae: 35.5709 - val_loss: 13114.3711 - val_mae: 70.1110\n",
      "Epoch 840/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 2706.8171 - mae: 35.5483 - val_loss: 13110.4150 - val_mae: 70.0828\n",
      "Epoch 841/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 2704.8142 - mae: 35.5255 - val_loss: 13106.3389 - val_mae: 70.0537\n",
      "Epoch 842/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 2702.7910 - mae: 35.5025 - val_loss: 13102.1807 - val_mae: 70.0240\n",
      "Epoch 843/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 2700.7585 - mae: 35.4793 - val_loss: 13098.0615 - val_mae: 69.9946\n",
      "Epoch 844/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 2698.7354 - mae: 35.4566 - val_loss: 13094.0781 - val_mae: 69.9661\n",
      "Epoch 845/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - loss: 2696.7468 - mae: 35.4331 - val_loss: 13089.9316 - val_mae: 69.9365\n",
      "Epoch 846/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 2694.7192 - mae: 35.4106 - val_loss: 13085.8760 - val_mae: 69.9075\n",
      "Epoch 847/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 2692.7095 - mae: 35.3875 - val_loss: 13081.7305 - val_mae: 69.8778\n",
      "Epoch 848/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 2690.7344 - mae: 35.3647 - val_loss: 13077.6855 - val_mae: 69.8489\n",
      "Epoch 849/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 2688.7068 - mae: 35.3421 - val_loss: 13073.5820 - val_mae: 69.8195\n",
      "Epoch 850/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 2686.6770 - mae: 35.3190 - val_loss: 13069.4297 - val_mae: 69.7898\n",
      "Epoch 851/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 2684.6685 - mae: 35.2962 - val_loss: 13065.3232 - val_mae: 69.7604\n",
      "Epoch 852/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 2682.6089 - mae: 35.2730 - val_loss: 13061.1162 - val_mae: 69.7302\n",
      "Epoch 853/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 2680.5911 - mae: 35.2503 - val_loss: 13057.0654 - val_mae: 69.7011\n",
      "Epoch 854/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 2678.5498 - mae: 35.2271 - val_loss: 13052.9180 - val_mae: 69.6714\n",
      "Epoch 855/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 2676.5808 - mae: 35.2044 - val_loss: 13048.7559 - val_mae: 69.6415\n",
      "Epoch 856/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 2674.5979 - mae: 35.1823 - val_loss: 13044.9316 - val_mae: 69.6140\n",
      "Epoch 857/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 2672.6467 - mae: 35.1600 - val_loss: 13040.8281 - val_mae: 69.5846\n",
      "Epoch 858/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 2670.6680 - mae: 35.1376 - val_loss: 13036.7832 - val_mae: 69.5555\n",
      "Epoch 859/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - loss: 2668.6479 - mae: 35.1148 - val_loss: 13032.6436 - val_mae: 69.5257\n",
      "Epoch 860/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 2666.6545 - mae: 35.0920 - val_loss: 13028.5977 - val_mae: 69.4966\n",
      "Epoch 861/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 2664.7122 - mae: 35.0701 - val_loss: 13024.6230 - val_mae: 69.4680\n",
      "Epoch 862/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 2662.7454 - mae: 35.0476 - val_loss: 13020.6035 - val_mae: 69.4391\n",
      "Epoch 863/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 2660.7788 - mae: 35.0252 - val_loss: 13016.5811 - val_mae: 69.4101\n",
      "Epoch 864/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 2658.8230 - mae: 35.0030 - val_loss: 13012.6426 - val_mae: 69.3817\n",
      "Epoch 865/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 2656.8787 - mae: 34.9810 - val_loss: 13008.5947 - val_mae: 69.3526\n",
      "Epoch 866/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 2654.8542 - mae: 34.9580 - val_loss: 13004.4150 - val_mae: 69.3224\n",
      "Epoch 867/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 2652.8428 - mae: 34.9350 - val_loss: 13000.2803 - val_mae: 69.2926\n",
      "Epoch 868/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 2650.8760 - mae: 34.9129 - val_loss: 12996.2568 - val_mae: 69.2636\n",
      "Epoch 869/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 2648.9268 - mae: 34.8910 - val_loss: 12992.3047 - val_mae: 69.2350\n",
      "Epoch 870/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 2646.9692 - mae: 34.8691 - val_loss: 12988.3301 - val_mae: 69.2063\n",
      "Epoch 871/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 2645.0085 - mae: 34.8471 - val_loss: 12984.2656 - val_mae: 69.1769\n",
      "Epoch 872/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 2642.9871 - mae: 34.8244 - val_loss: 12980.0488 - val_mae: 69.1465\n",
      "Epoch 873/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 2641.0281 - mae: 34.8024 - val_loss: 12976.1191 - val_mae: 69.1180\n",
      "Epoch 874/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 2639.1128 - mae: 34.7806 - val_loss: 12972.1182 - val_mae: 69.0891\n",
      "Epoch 875/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 2637.1882 - mae: 34.7589 - val_loss: 12968.1074 - val_mae: 69.0601\n",
      "Epoch 876/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 2635.2539 - mae: 34.7378 - val_loss: 12964.2305 - val_mae: 69.0320\n",
      "Epoch 877/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 2633.3516 - mae: 34.7160 - val_loss: 12960.2393 - val_mae: 69.0031\n",
      "Epoch 878/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 2631.3943 - mae: 34.6946 - val_loss: 12956.2373 - val_mae: 68.9741\n",
      "Epoch 879/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 2629.4312 - mae: 34.6723 - val_loss: 12952.1797 - val_mae: 68.9446\n",
      "Epoch 880/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - loss: 2627.4695 - mae: 34.6504 - val_loss: 12948.1504 - val_mae: 68.9154\n",
      "Epoch 881/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 2625.5220 - mae: 34.6289 - val_loss: 12944.0840 - val_mae: 68.8859\n",
      "Epoch 882/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 2623.5398 - mae: 34.6066 - val_loss: 12939.9658 - val_mae: 68.8560\n",
      "Epoch 883/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 2621.5994 - mae: 34.5850 - val_loss: 12936.0596 - val_mae: 68.8276\n",
      "Epoch 884/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 2619.7095 - mae: 34.5643 - val_loss: 12932.0869 - val_mae: 68.7988\n",
      "Epoch 885/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 2617.7595 - mae: 34.5420 - val_loss: 12927.9941 - val_mae: 68.7690\n",
      "Epoch 886/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 2615.7466 - mae: 34.5200 - val_loss: 12923.8477 - val_mae: 68.7389\n",
      "Epoch 887/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - loss: 2613.7747 - mae: 34.4977 - val_loss: 12919.8193 - val_mae: 68.7096\n",
      "Epoch 888/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 2611.8457 - mae: 34.4764 - val_loss: 12915.8389 - val_mae: 68.6806\n",
      "Epoch 889/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 2609.9570 - mae: 34.4553 - val_loss: 12911.9531 - val_mae: 68.6523\n",
      "Epoch 890/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - loss: 2608.0493 - mae: 34.4343 - val_loss: 12907.9873 - val_mae: 68.6234\n",
      "Epoch 891/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 2606.1116 - mae: 34.4124 - val_loss: 12903.9238 - val_mae: 68.5938\n",
      "Epoch 892/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 2604.1484 - mae: 34.3906 - val_loss: 12899.8564 - val_mae: 68.5641\n",
      "Epoch 893/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - loss: 2602.2219 - mae: 34.3691 - val_loss: 12895.8818 - val_mae: 68.5351\n",
      "Epoch 894/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 2600.3376 - mae: 34.3479 - val_loss: 12891.9980 - val_mae: 68.5068\n",
      "Epoch 895/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 121ms/step - loss: 2598.4353 - mae: 34.3269 - val_loss: 12888.0068 - val_mae: 68.4777\n",
      "Epoch 896/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 119ms/step - loss: 2596.4963 - mae: 34.3055 - val_loss: 12883.9336 - val_mae: 68.4479\n",
      "Epoch 897/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 117ms/step - loss: 2594.5728 - mae: 34.2841 - val_loss: 12880.0332 - val_mae: 68.4194\n",
      "Epoch 898/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - loss: 2592.5950 - mae: 34.2619 - val_loss: 12875.8369 - val_mae: 68.3887\n",
      "Epoch 899/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 2590.6479 - mae: 34.2396 - val_loss: 12871.7451 - val_mae: 68.3588\n",
      "Epoch 900/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - loss: 2588.6951 - mae: 34.2182 - val_loss: 12867.6865 - val_mae: 68.3291\n",
      "Epoch 901/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - loss: 2586.4917 - mae: 34.1937 - val_loss: 12863.0078 - val_mae: 68.2949\n",
      "Epoch 902/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 2584.5647 - mae: 34.1716 - val_loss: 12859.1152 - val_mae: 68.2664\n",
      "Epoch 903/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 2582.6626 - mae: 34.1512 - val_loss: 12855.3057 - val_mae: 68.2385\n",
      "Epoch 904/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 2580.7993 - mae: 34.1303 - val_loss: 12851.3643 - val_mae: 68.2096\n",
      "Epoch 905/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 2578.9197 - mae: 34.1094 - val_loss: 12847.4531 - val_mae: 68.1809\n",
      "Epoch 906/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - loss: 2577.0457 - mae: 34.0885 - val_loss: 12843.5029 - val_mae: 68.1519\n",
      "Epoch 907/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 2575.2170 - mae: 34.0681 - val_loss: 12839.7422 - val_mae: 68.1243\n",
      "Epoch 908/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - loss: 2573.4019 - mae: 34.0482 - val_loss: 12835.9180 - val_mae: 68.0963\n",
      "Epoch 909/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - loss: 2571.5679 - mae: 34.0275 - val_loss: 12832.1104 - val_mae: 68.0683\n",
      "Epoch 910/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - loss: 2569.7483 - mae: 34.0072 - val_loss: 12828.2129 - val_mae: 68.0397\n",
      "Epoch 911/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - loss: 2567.9150 - mae: 33.9870 - val_loss: 12824.4756 - val_mae: 68.0122\n",
      "Epoch 912/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 2566.0815 - mae: 33.9667 - val_loss: 12820.6396 - val_mae: 67.9840\n",
      "Epoch 913/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - loss: 2564.1497 - mae: 33.9447 - val_loss: 12816.3975 - val_mae: 67.9528\n",
      "Epoch 914/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - loss: 2562.2603 - mae: 33.9241 - val_loss: 12812.6074 - val_mae: 67.9249\n",
      "Epoch 915/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 2560.4258 - mae: 33.9033 - val_loss: 12808.7881 - val_mae: 67.8968\n",
      "Epoch 916/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - loss: 2558.5852 - mae: 33.8832 - val_loss: 12804.8770 - val_mae: 67.8680\n",
      "Epoch 917/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - loss: 2556.7075 - mae: 33.8623 - val_loss: 12800.9492 - val_mae: 67.8390\n",
      "Epoch 918/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - loss: 2554.8438 - mae: 33.8414 - val_loss: 12796.9766 - val_mae: 67.8097\n",
      "Epoch 919/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 2552.9646 - mae: 33.8202 - val_loss: 12793.0283 - val_mae: 67.7806\n",
      "Epoch 920/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 2551.1243 - mae: 33.8001 - val_loss: 12789.3037 - val_mae: 67.7531\n",
      "Epoch 921/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 2549.3025 - mae: 33.7796 - val_loss: 12785.3496 - val_mae: 67.7240\n",
      "Epoch 922/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 2547.4734 - mae: 33.7595 - val_loss: 12781.5078 - val_mae: 67.6956\n",
      "Epoch 923/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - loss: 2545.6274 - mae: 33.7395 - val_loss: 12777.7109 - val_mae: 67.6675\n",
      "Epoch 924/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 2543.7659 - mae: 33.7184 - val_loss: 12773.6689 - val_mae: 67.6377\n",
      "Epoch 925/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - loss: 2541.8601 - mae: 33.6976 - val_loss: 12769.6924 - val_mae: 67.6082\n",
      "Epoch 926/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 2540.0376 - mae: 33.6770 - val_loss: 12765.8398 - val_mae: 67.5798\n",
      "Epoch 927/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 2538.2075 - mae: 33.6568 - val_loss: 12761.9609 - val_mae: 67.5510\n",
      "Epoch 928/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 2536.3435 - mae: 33.6367 - val_loss: 12758.1162 - val_mae: 67.5226\n",
      "Epoch 929/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 2534.4631 - mae: 33.6159 - val_loss: 12754.0898 - val_mae: 67.4928\n",
      "Epoch 930/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - loss: 2532.6165 - mae: 33.5958 - val_loss: 12750.2158 - val_mae: 67.4641\n",
      "Epoch 931/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 2530.7817 - mae: 33.5753 - val_loss: 12746.2900 - val_mae: 67.4350\n",
      "Epoch 932/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 2528.9592 - mae: 33.5560 - val_loss: 12742.5811 - val_mae: 67.4074\n",
      "Epoch 933/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 2527.1389 - mae: 33.5357 - val_loss: 12738.6035 - val_mae: 67.3779\n",
      "Epoch 934/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 2525.3154 - mae: 33.5162 - val_loss: 12734.8311 - val_mae: 67.3499\n",
      "Epoch 935/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 2523.4836 - mae: 33.4962 - val_loss: 12730.9980 - val_mae: 67.3215\n",
      "Epoch 936/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - loss: 2521.6709 - mae: 33.4761 - val_loss: 12727.0195 - val_mae: 67.2919\n",
      "Epoch 937/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 2519.8586 - mae: 33.4559 - val_loss: 12723.2148 - val_mae: 67.2637\n",
      "Epoch 938/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 2518.0652 - mae: 33.4369 - val_loss: 12719.4844 - val_mae: 67.2359\n",
      "Epoch 939/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 2516.2458 - mae: 33.4168 - val_loss: 12715.6309 - val_mae: 67.2073\n",
      "Epoch 940/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 2514.4690 - mae: 33.3970 - val_loss: 12711.7607 - val_mae: 67.1785\n",
      "Epoch 941/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 2512.6179 - mae: 33.3766 - val_loss: 12707.8389 - val_mae: 67.1493\n",
      "Epoch 942/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 2510.7969 - mae: 33.3572 - val_loss: 12704.0635 - val_mae: 67.1211\n",
      "Epoch 943/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 2508.9727 - mae: 33.3367 - val_loss: 12700.1104 - val_mae: 67.0917\n",
      "Epoch 944/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 2507.1313 - mae: 33.3170 - val_loss: 12696.2285 - val_mae: 67.0628\n",
      "Epoch 945/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 2505.3103 - mae: 33.2962 - val_loss: 12692.2764 - val_mae: 67.0333\n",
      "Epoch 946/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 2503.4905 - mae: 33.2768 - val_loss: 12688.5566 - val_mae: 67.0055\n",
      "Epoch 947/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 2501.6951 - mae: 33.2561 - val_loss: 12684.5693 - val_mae: 66.9758\n",
      "Epoch 948/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - loss: 2499.8679 - mae: 33.2366 - val_loss: 12680.7979 - val_mae: 66.9476\n",
      "Epoch 949/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 2498.0730 - mae: 33.2163 - val_loss: 12676.8936 - val_mae: 66.9184\n",
      "Epoch 950/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 2496.2502 - mae: 33.1968 - val_loss: 12673.0684 - val_mae: 66.8898\n",
      "Epoch 951/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 2494.1758 - mae: 33.1737 - val_loss: 12668.4648 - val_mae: 66.8554\n",
      "Epoch 952/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 2492.2708 - mae: 33.1530 - val_loss: 12664.6113 - val_mae: 66.8266\n",
      "Epoch 953/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - loss: 2490.4231 - mae: 33.1322 - val_loss: 12660.5605 - val_mae: 66.7963\n",
      "Epoch 954/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 2488.5737 - mae: 33.1122 - val_loss: 12656.7969 - val_mae: 66.7681\n",
      "Epoch 955/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 2486.8464 - mae: 33.0933 - val_loss: 12653.0332 - val_mae: 66.7399\n",
      "Epoch 956/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 2485.1160 - mae: 33.0742 - val_loss: 12649.3545 - val_mae: 66.7124\n",
      "Epoch 957/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 2483.3748 - mae: 33.0549 - val_loss: 12645.5771 - val_mae: 66.6840\n",
      "Epoch 958/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 2481.6045 - mae: 33.0359 - val_loss: 12641.8604 - val_mae: 66.6562\n",
      "Epoch 959/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 2479.8596 - mae: 33.0164 - val_loss: 12638.0498 - val_mae: 66.6276\n",
      "Epoch 960/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 2478.0969 - mae: 32.9974 - val_loss: 12634.3193 - val_mae: 66.5996\n",
      "Epoch 961/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - loss: 2476.3091 - mae: 32.9778 - val_loss: 12630.4814 - val_mae: 66.5707\n",
      "Epoch 962/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 2474.5391 - mae: 32.9584 - val_loss: 12626.7871 - val_mae: 66.5430\n",
      "Epoch 963/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 2472.8208 - mae: 32.9395 - val_loss: 12622.9277 - val_mae: 66.5140\n",
      "Epoch 964/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 2470.9924 - mae: 32.9197 - val_loss: 12619.1016 - val_mae: 66.4852\n",
      "Epoch 965/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 2469.2058 - mae: 32.9003 - val_loss: 12615.2354 - val_mae: 66.4562\n",
      "Epoch 966/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 2467.4397 - mae: 32.8816 - val_loss: 12611.4102 - val_mae: 66.4274\n",
      "Epoch 967/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 2465.6492 - mae: 32.8622 - val_loss: 12607.5693 - val_mae: 66.3984\n",
      "Epoch 968/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - loss: 2463.9124 - mae: 32.8433 - val_loss: 12603.8506 - val_mae: 66.3704\n",
      "Epoch 969/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 2462.1748 - mae: 32.8245 - val_loss: 12600.1621 - val_mae: 66.3426\n",
      "Epoch 970/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 2460.4744 - mae: 32.8061 - val_loss: 12596.4600 - val_mae: 66.3147\n",
      "Epoch 971/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 2458.7285 - mae: 32.7874 - val_loss: 12592.7197 - val_mae: 66.2865\n",
      "Epoch 972/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 2456.9551 - mae: 32.7685 - val_loss: 12588.9033 - val_mae: 66.2577\n",
      "Epoch 973/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 2455.1890 - mae: 32.7491 - val_loss: 12584.9678 - val_mae: 66.2280\n",
      "Epoch 974/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 2453.4014 - mae: 32.7300 - val_loss: 12581.1816 - val_mae: 66.1994\n",
      "Epoch 975/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 2451.6602 - mae: 32.7116 - val_loss: 12577.4902 - val_mae: 66.1716\n",
      "Epoch 976/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 2449.9636 - mae: 32.6930 - val_loss: 12573.7832 - val_mae: 66.1435\n",
      "Epoch 977/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 2448.2332 - mae: 32.6745 - val_loss: 12570.0508 - val_mae: 66.1153\n",
      "Epoch 978/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 2446.4817 - mae: 32.6555 - val_loss: 12566.2275 - val_mae: 66.0864\n",
      "Epoch 979/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 2444.7380 - mae: 32.6368 - val_loss: 12562.4873 - val_mae: 66.0581\n",
      "Epoch 980/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - loss: 2442.9919 - mae: 32.6185 - val_loss: 12558.7031 - val_mae: 66.0294\n",
      "Epoch 981/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 2441.2644 - mae: 32.5998 - val_loss: 12554.9199 - val_mae: 66.0008\n",
      "Epoch 982/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 2439.5381 - mae: 32.5815 - val_loss: 12551.2646 - val_mae: 65.9731\n",
      "Epoch 983/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 2437.7598 - mae: 32.5632 - val_loss: 12547.3594 - val_mae: 65.9435\n",
      "Epoch 984/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 2436.0095 - mae: 32.5446 - val_loss: 12543.5791 - val_mae: 65.9148\n",
      "Epoch 985/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 2434.2339 - mae: 32.5258 - val_loss: 12539.6768 - val_mae: 65.8852\n",
      "Epoch 986/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 2432.4331 - mae: 32.5070 - val_loss: 12535.7832 - val_mae: 65.8556\n",
      "Epoch 987/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 2430.7024 - mae: 32.4885 - val_loss: 12532.0898 - val_mae: 65.8276\n",
      "Epoch 988/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 2428.8845 - mae: 32.4697 - val_loss: 12528.0371 - val_mae: 65.7968\n",
      "Epoch 989/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 2427.0522 - mae: 32.4503 - val_loss: 12524.0059 - val_mae: 65.7662\n",
      "Epoch 990/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 2425.2109 - mae: 32.4313 - val_loss: 12520.1758 - val_mae: 65.7370\n",
      "Epoch 991/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 2423.5088 - mae: 32.4130 - val_loss: 12516.3672 - val_mae: 65.7081\n",
      "Epoch 992/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 2421.7854 - mae: 32.3958 - val_loss: 12512.7295 - val_mae: 65.6804\n",
      "Epoch 993/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 2420.0491 - mae: 32.3775 - val_loss: 12508.8535 - val_mae: 65.6509\n",
      "Epoch 994/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - loss: 2418.2629 - mae: 32.3593 - val_loss: 12505.0137 - val_mae: 65.6216\n",
      "Epoch 995/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 2416.5266 - mae: 32.3410 - val_loss: 12501.1240 - val_mae: 65.5920\n",
      "Epoch 996/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 2414.7732 - mae: 32.3231 - val_loss: 12497.4600 - val_mae: 65.5640\n",
      "Epoch 997/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 2413.1116 - mae: 32.3055 - val_loss: 12493.6504 - val_mae: 65.5350\n",
      "Epoch 998/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 123ms/step - loss: 2411.4036 - mae: 32.2884 - val_loss: 12490.0732 - val_mae: 65.5077\n",
      "Epoch 999/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step - loss: 2409.7485 - mae: 32.2709 - val_loss: 12486.4189 - val_mae: 65.4798\n",
      "Epoch 1000/1000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 2408.0649 - mae: 32.2541 - val_loss: 12482.7715 - val_mae: 65.4519\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7bf80aaa5240>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "model = Sequential()\n",
    "model.add(layers.LSTM(units=100, return_sequences=True))\n",
    "model.add(layers.LSTM(units=100))\n",
    "model.add(layers.Dense(50, activation=\"relu\")) \n",
    "model.add(layers.Dense(1, activation=\"linear\"))\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "model.fit(X_train, y_train, epochs=1000, callbacks=[es], validation_split=0.2, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "075be9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# es = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "# model = Sequential()\n",
    "# model.add(layers.Dense(28, input_shape=(X_train.shape[-1],), activation=\"relu\"))\n",
    "# model.add(layers.Dense(21, activation=\"relu\"))\n",
    "# model.add(layers.Dense(14, activation=\"relu\"))\n",
    "# model.add(layers.Dense(7, activation=\"relu\"))\n",
    "# model.add(layers.Dense(1, activation=\"linear\"))\n",
    "\n",
    "# model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "# model.fit(X_train, y_train, epochs=1000, callbacks=[es], validation_split=0.2, batch_size=32, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b3aab4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 44409.7695 - mae: 65.2660\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[44409.76953125, 65.26595306396484]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "06c83dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1f9f2d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 7.76177673e+01]],\n",
       "\n",
       "       [[ 3.83011475e+01]],\n",
       "\n",
       "       [[ 5.04469490e+01]],\n",
       "\n",
       "       [[ 1.02338814e+02]],\n",
       "\n",
       "       [[ 1.19228432e+02]],\n",
       "\n",
       "       [[ 1.04816162e+02]],\n",
       "\n",
       "       [[ 8.95793686e+01]],\n",
       "\n",
       "       [[ 9.74678421e+01]],\n",
       "\n",
       "       [[ 1.18487839e+02]],\n",
       "\n",
       "       [[ 1.05462212e+02]],\n",
       "\n",
       "       [[ 1.05011803e+02]],\n",
       "\n",
       "       [[ 1.19823074e+02]],\n",
       "\n",
       "       [[ 1.18379013e+02]],\n",
       "\n",
       "       [[ 9.71664200e+01]],\n",
       "\n",
       "       [[ 1.22425995e+02]],\n",
       "\n",
       "       [[ 1.04026062e+02]],\n",
       "\n",
       "       [[ 1.37131516e+02]],\n",
       "\n",
       "       [[ 1.39516388e+02]],\n",
       "\n",
       "       [[ 1.19574341e+02]],\n",
       "\n",
       "       [[ 1.11540756e+02]],\n",
       "\n",
       "       [[ 1.38907532e+02]],\n",
       "\n",
       "       [[ 1.61506821e+02]],\n",
       "\n",
       "       [[ 1.09293770e+02]],\n",
       "\n",
       "       [[ 6.76155930e+01]],\n",
       "\n",
       "       [[ 9.26159821e+01]],\n",
       "\n",
       "       [[ 6.73332520e+01]],\n",
       "\n",
       "       [[ 8.36948929e+01]],\n",
       "\n",
       "       [[ 8.49782791e+01]],\n",
       "\n",
       "       [[ 3.52242622e+01]],\n",
       "\n",
       "       [[ 1.46735878e+01]],\n",
       "\n",
       "       [[ 8.16376190e+01]],\n",
       "\n",
       "       [[ 7.06853027e+01]],\n",
       "\n",
       "       [[ 8.54524689e+01]],\n",
       "\n",
       "       [[ 1.01004051e+02]],\n",
       "\n",
       "       [[ 1.15429535e+02]],\n",
       "\n",
       "       [[ 1.10813469e+02]],\n",
       "\n",
       "       [[ 8.34935913e+01]],\n",
       "\n",
       "       [[ 7.97800140e+01]],\n",
       "\n",
       "       [[ 9.37551346e+01]],\n",
       "\n",
       "       [[ 1.18656357e+02]],\n",
       "\n",
       "       [[ 1.20861214e+02]],\n",
       "\n",
       "       [[ 9.92869949e+01]],\n",
       "\n",
       "       [[ 9.02129974e+01]],\n",
       "\n",
       "       [[ 8.61640854e+01]],\n",
       "\n",
       "       [[ 8.68845139e+01]],\n",
       "\n",
       "       [[ 1.11121269e+02]],\n",
       "\n",
       "       [[ 1.04942398e+02]],\n",
       "\n",
       "       [[ 1.22086838e+02]],\n",
       "\n",
       "       [[ 9.74994583e+01]],\n",
       "\n",
       "       [[ 1.05290977e+02]],\n",
       "\n",
       "       [[ 1.00102928e+02]],\n",
       "\n",
       "       [[ 8.78716507e+01]],\n",
       "\n",
       "       [[ 8.71012650e+01]],\n",
       "\n",
       "       [[ 9.90007477e+01]],\n",
       "\n",
       "       [[ 7.35396957e+01]],\n",
       "\n",
       "       [[ 8.56304092e+01]],\n",
       "\n",
       "       [[ 8.84723663e+01]],\n",
       "\n",
       "       [[ 7.46166000e+01]],\n",
       "\n",
       "       [[ 5.68641624e+01]],\n",
       "\n",
       "       [[ 7.91824570e+01]],\n",
       "\n",
       "       [[ 8.90252457e+01]],\n",
       "\n",
       "       [[ 1.22200668e+02]],\n",
       "\n",
       "       [[ 1.65779709e+02]],\n",
       "\n",
       "       [[ 1.01660378e+02]],\n",
       "\n",
       "       [[ 9.62922440e+01]],\n",
       "\n",
       "       [[ 8.19209213e+01]],\n",
       "\n",
       "       [[ 8.41753922e+01]],\n",
       "\n",
       "       [[ 9.09353333e+01]],\n",
       "\n",
       "       [[ 9.33447495e+01]],\n",
       "\n",
       "       [[ 1.15427254e+02]],\n",
       "\n",
       "       [[ 1.11204247e+02]],\n",
       "\n",
       "       [[ 9.88322678e+01]],\n",
       "\n",
       "       [[ 1.10215179e+02]],\n",
       "\n",
       "       [[ 1.42338058e+02]],\n",
       "\n",
       "       [[ 1.49080627e+02]],\n",
       "\n",
       "       [[ 1.21615425e+02]],\n",
       "\n",
       "       [[ 1.01788925e+02]],\n",
       "\n",
       "       [[ 7.90190201e+01]],\n",
       "\n",
       "       [[ 7.47296677e+01]],\n",
       "\n",
       "       [[ 9.09014740e+01]],\n",
       "\n",
       "       [[ 1.04632225e+02]],\n",
       "\n",
       "       [[ 1.15778038e+02]],\n",
       "\n",
       "       [[ 9.55695648e+01]],\n",
       "\n",
       "       [[ 9.18845139e+01]],\n",
       "\n",
       "       [[ 1.01734741e+02]],\n",
       "\n",
       "       [[ 8.67252731e+01]],\n",
       "\n",
       "       [[ 7.66722031e+01]],\n",
       "\n",
       "       [[ 1.05451836e+02]],\n",
       "\n",
       "       [[ 1.11573364e+02]],\n",
       "\n",
       "       [[ 1.19624725e+02]],\n",
       "\n",
       "       [[ 9.65019684e+01]],\n",
       "\n",
       "       [[ 9.77670822e+01]],\n",
       "\n",
       "       [[ 8.32670822e+01]],\n",
       "\n",
       "       [[ 8.92481918e+01]],\n",
       "\n",
       "       [[ 1.04471260e+02]],\n",
       "\n",
       "       [[ 1.10715973e+02]],\n",
       "\n",
       "       [[ 1.00428253e+02]],\n",
       "\n",
       "       [[ 7.72089005e+01]],\n",
       "\n",
       "       [[ 9.35289078e+01]],\n",
       "\n",
       "       [[ 1.10505676e+02]],\n",
       "\n",
       "       [[ 1.16462814e+02]],\n",
       "\n",
       "       [[ 1.12209267e+02]],\n",
       "\n",
       "       [[ 1.09761696e+02]],\n",
       "\n",
       "       [[ 9.81782608e+01]],\n",
       "\n",
       "       [[ 1.16329330e+02]],\n",
       "\n",
       "       [[ 1.06294151e+02]],\n",
       "\n",
       "       [[ 7.11047440e+01]],\n",
       "\n",
       "       [[ 2.44553814e+01]],\n",
       "\n",
       "       [[ 7.11261978e+01]],\n",
       "\n",
       "       [[ 9.29455490e+01]],\n",
       "\n",
       "       [[ 8.68150024e+01]],\n",
       "\n",
       "       [[ 1.25308105e+02]],\n",
       "\n",
       "       [[ 1.77861816e+02]],\n",
       "\n",
       "       [[ 1.13367035e+02]],\n",
       "\n",
       "       [[ 8.01887894e+01]],\n",
       "\n",
       "       [[ 9.52257233e+01]],\n",
       "\n",
       "       [[ 1.18644417e+02]],\n",
       "\n",
       "       [[ 1.99184326e+02]],\n",
       "\n",
       "       [[ 1.01859718e+02]],\n",
       "\n",
       "       [[ 9.29580307e+01]],\n",
       "\n",
       "       [[ 1.01632950e+02]],\n",
       "\n",
       "       [[ 1.01449234e+02]],\n",
       "\n",
       "       [[ 1.01403885e+02]],\n",
       "\n",
       "       [[ 1.04722557e+02]],\n",
       "\n",
       "       [[ 1.17707726e+02]],\n",
       "\n",
       "       [[ 7.55554657e+01]],\n",
       "\n",
       "       [[ 6.59324417e+01]],\n",
       "\n",
       "       [[ 6.92504578e+01]],\n",
       "\n",
       "       [[ 6.56122513e+01]],\n",
       "\n",
       "       [[ 9.68171768e+01]],\n",
       "\n",
       "       [[ 5.52385445e+01]],\n",
       "\n",
       "       [[ 7.75783539e+01]],\n",
       "\n",
       "       [[ 1.02310898e+02]],\n",
       "\n",
       "       [[ 1.21460716e+02]],\n",
       "\n",
       "       [[ 9.17235260e+01]],\n",
       "\n",
       "       [[ 8.41664963e+01]],\n",
       "\n",
       "       [[ 8.48979950e+01]],\n",
       "\n",
       "       [[ 7.85609818e+01]],\n",
       "\n",
       "       [[ 9.22020111e+01]],\n",
       "\n",
       "       [[ 1.27380302e+02]],\n",
       "\n",
       "       [[ 1.21020996e+02]],\n",
       "\n",
       "       [[ 8.34935226e+01]],\n",
       "\n",
       "       [[ 6.57664032e+01]],\n",
       "\n",
       "       [[ 8.50779572e+01]],\n",
       "\n",
       "       [[ 1.04907784e+02]],\n",
       "\n",
       "       [[ 9.18515472e+01]],\n",
       "\n",
       "       [[ 9.64137421e+01]],\n",
       "\n",
       "       [[ 1.16305672e+02]],\n",
       "\n",
       "       [[ 8.89966431e+01]],\n",
       "\n",
       "       [[ 7.52730255e+01]],\n",
       "\n",
       "       [[ 1.88197647e+02]],\n",
       "\n",
       "       [[ 2.22438416e+02]],\n",
       "\n",
       "       [[ 1.78079681e+02]],\n",
       "\n",
       "       [[ 1.75352890e+02]],\n",
       "\n",
       "       [[ 1.37850021e+02]],\n",
       "\n",
       "       [[ 8.07862549e+01]],\n",
       "\n",
       "       [[ 8.66079025e+01]],\n",
       "\n",
       "       [[ 1.76689957e+02]],\n",
       "\n",
       "       [[ 2.78777740e+02]],\n",
       "\n",
       "       [[ 1.54473145e+02]],\n",
       "\n",
       "       [[ 4.54964502e+03]],\n",
       "\n",
       "       [[ 9.06437256e+02]],\n",
       "\n",
       "       [[ 1.63733627e+02]],\n",
       "\n",
       "       [[ 8.63962250e+01]],\n",
       "\n",
       "       [[ 1.50948486e+02]],\n",
       "\n",
       "       [[ 1.83400208e+02]],\n",
       "\n",
       "       [[ 1.88742416e+02]],\n",
       "\n",
       "       [[ 1.07521233e+02]],\n",
       "\n",
       "       [[ 1.17682739e+02]],\n",
       "\n",
       "       [[ 2.40954529e+02]],\n",
       "\n",
       "       [[ 1.86917633e+02]],\n",
       "\n",
       "       [[ 1.10151230e+02]],\n",
       "\n",
       "       [[ 9.27013397e+01]],\n",
       "\n",
       "       [[ 1.43455063e+02]],\n",
       "\n",
       "       [[ 1.47687912e+02]],\n",
       "\n",
       "       [[ 1.07849159e+02]],\n",
       "\n",
       "       [[ 4.41763115e+01]],\n",
       "\n",
       "       [[ 8.64357758e+01]],\n",
       "\n",
       "       [[ 9.77150879e+01]],\n",
       "\n",
       "       [[ 5.73382111e+01]],\n",
       "\n",
       "       [[ 8.59814835e+01]],\n",
       "\n",
       "       [[ 1.08290718e+02]],\n",
       "\n",
       "       [[ 1.14768501e+02]],\n",
       "\n",
       "       [[ 1.27480278e+02]],\n",
       "\n",
       "       [[ 1.10489594e+02]],\n",
       "\n",
       "       [[ 1.04463455e+02]],\n",
       "\n",
       "       [[ 1.05098175e+02]],\n",
       "\n",
       "       [[ 1.11046021e+02]],\n",
       "\n",
       "       [[ 1.14360573e+02]],\n",
       "\n",
       "       [[ 1.03044060e+02]],\n",
       "\n",
       "       [[ 7.70314026e+01]],\n",
       "\n",
       "       [[ 8.71809158e+01]],\n",
       "\n",
       "       [[ 1.39955978e+02]],\n",
       "\n",
       "       [[ 1.15939606e+02]],\n",
       "\n",
       "       [[ 1.23787537e+02]],\n",
       "\n",
       "       [[ 1.60938446e+02]],\n",
       "\n",
       "       [[ 1.28479993e+03]],\n",
       "\n",
       "       [[ 1.32492584e+02]],\n",
       "\n",
       "       [[ 1.53784302e+02]],\n",
       "\n",
       "       [[ 1.35206451e+02]],\n",
       "\n",
       "       [[ 1.18261948e+02]],\n",
       "\n",
       "       [[ 7.59811935e+01]],\n",
       "\n",
       "       [[ 1.03197556e+02]],\n",
       "\n",
       "       [[ 1.06938805e+02]],\n",
       "\n",
       "       [[ 9.50784225e+01]],\n",
       "\n",
       "       [[ 7.43749313e+01]],\n",
       "\n",
       "       [[ 8.14720383e+01]],\n",
       "\n",
       "       [[ 7.69715500e+01]],\n",
       "\n",
       "       [[ 1.22238579e+02]],\n",
       "\n",
       "       [[ 1.26785957e+02]],\n",
       "\n",
       "       [[ 1.02608513e+02]],\n",
       "\n",
       "       [[ 9.52511902e+01]],\n",
       "\n",
       "       [[ 1.03957191e+02]],\n",
       "\n",
       "       [[ 1.11149796e+02]],\n",
       "\n",
       "       [[ 1.13644775e+02]],\n",
       "\n",
       "       [[ 1.27276596e+02]],\n",
       "\n",
       "       [[ 1.15460167e+02]],\n",
       "\n",
       "       [[ 1.11240707e+02]],\n",
       "\n",
       "       [[ 9.89547348e+01]],\n",
       "\n",
       "       [[ 9.78575974e+01]],\n",
       "\n",
       "       [[ 8.02929306e+01]],\n",
       "\n",
       "       [[ 9.55678940e+01]],\n",
       "\n",
       "       [[ 1.14161369e+02]],\n",
       "\n",
       "       [[ 1.08460274e+02]],\n",
       "\n",
       "       [[ 8.39895401e+01]],\n",
       "\n",
       "       [[ 8.51659317e+01]],\n",
       "\n",
       "       [[ 9.43895874e+01]],\n",
       "\n",
       "       [[ 1.31333527e+02]],\n",
       "\n",
       "       [[ 1.19397552e+02]],\n",
       "\n",
       "       [[ 1.12908066e+02]],\n",
       "\n",
       "       [[ 1.16756485e+02]],\n",
       "\n",
       "       [[ 9.85303040e+01]],\n",
       "\n",
       "       [[ 1.03272995e+02]],\n",
       "\n",
       "       [[ 8.68026886e+01]],\n",
       "\n",
       "       [[ 1.13134056e+02]],\n",
       "\n",
       "       [[ 1.06335945e+02]],\n",
       "\n",
       "       [[ 1.21875137e+02]],\n",
       "\n",
       "       [[ 1.26889603e+02]],\n",
       "\n",
       "       [[ 1.28723206e+02]],\n",
       "\n",
       "       [[ 1.22442162e+02]],\n",
       "\n",
       "       [[ 1.11731644e+02]],\n",
       "\n",
       "       [[ 1.13112175e+02]],\n",
       "\n",
       "       [[ 1.00005615e+02]],\n",
       "\n",
       "       [[ 1.00109642e+02]],\n",
       "\n",
       "       [[ 8.48338623e+01]],\n",
       "\n",
       "       [[ 7.81424484e+01]],\n",
       "\n",
       "       [[ 6.68748169e+01]],\n",
       "\n",
       "       [[ 8.00762558e+01]],\n",
       "\n",
       "       [[ 9.51073837e+01]],\n",
       "\n",
       "       [[ 8.96289825e+01]],\n",
       "\n",
       "       [[ 8.67492905e+01]],\n",
       "\n",
       "       [[ 6.26057320e+01]],\n",
       "\n",
       "       [[ 6.09183807e+01]],\n",
       "\n",
       "       [[ 7.96200867e+01]],\n",
       "\n",
       "       [[ 8.60228577e+01]],\n",
       "\n",
       "       [[ 1.21029640e+02]],\n",
       "\n",
       "       [[ 9.69517059e+01]],\n",
       "\n",
       "       [[ 9.05638351e+01]],\n",
       "\n",
       "       [[ 1.03586617e+02]],\n",
       "\n",
       "       [[ 1.01163002e+02]],\n",
       "\n",
       "       [[ 1.03383499e+02]],\n",
       "\n",
       "       [[ 1.08095558e+02]],\n",
       "\n",
       "       [[ 1.12505585e+02]],\n",
       "\n",
       "       [[ 9.47537155e+01]],\n",
       "\n",
       "       [[ 1.14528450e+02]],\n",
       "\n",
       "       [[ 1.13630333e+02]],\n",
       "\n",
       "       [[ 9.31352768e+01]],\n",
       "\n",
       "       [[ 8.70365295e+01]],\n",
       "\n",
       "       [[ 8.73773804e+01]],\n",
       "\n",
       "       [[ 7.71573792e+01]],\n",
       "\n",
       "       [[ 8.98226929e+01]],\n",
       "\n",
       "       [[ 1.00644775e+02]],\n",
       "\n",
       "       [[ 9.89429626e+01]],\n",
       "\n",
       "       [[ 9.67838593e+01]],\n",
       "\n",
       "       [[ 8.64417877e+01]],\n",
       "\n",
       "       [[ 7.62941666e+01]],\n",
       "\n",
       "       [[ 9.33443680e+01]],\n",
       "\n",
       "       [[ 1.04557808e+02]],\n",
       "\n",
       "       [[ 1.15894455e+02]],\n",
       "\n",
       "       [[ 8.83871384e+01]],\n",
       "\n",
       "       [[ 9.15471268e+01]],\n",
       "\n",
       "       [[ 7.42629318e+01]],\n",
       "\n",
       "       [[ 6.39030838e+01]],\n",
       "\n",
       "       [[ 8.76110153e+01]],\n",
       "\n",
       "       [[ 7.99647751e+01]],\n",
       "\n",
       "       [[ 8.50039444e+01]],\n",
       "\n",
       "       [[ 1.00015007e+02]],\n",
       "\n",
       "       [[ 1.00707748e+02]],\n",
       "\n",
       "       [[ 1.13244637e+02]],\n",
       "\n",
       "       [[ 1.08922585e+02]],\n",
       "\n",
       "       [[ 1.15725273e+02]],\n",
       "\n",
       "       [[ 1.49118439e+02]],\n",
       "\n",
       "       [[ 1.55988327e+02]],\n",
       "\n",
       "       [[ 1.62491989e+02]],\n",
       "\n",
       "       [[ 1.39200821e+02]],\n",
       "\n",
       "       [[ 1.02416084e+02]],\n",
       "\n",
       "       [[ 6.95613022e+01]],\n",
       "\n",
       "       [[ 4.74878731e+01]],\n",
       "\n",
       "       [[ 7.30780792e+01]],\n",
       "\n",
       "       [[ 8.57926865e+01]],\n",
       "\n",
       "       [[ 7.95162506e+01]],\n",
       "\n",
       "       [[ 8.66554642e+01]],\n",
       "\n",
       "       [[ 1.04431847e+02]],\n",
       "\n",
       "       [[ 1.11276733e+02]],\n",
       "\n",
       "       [[ 1.08746696e+02]],\n",
       "\n",
       "       [[ 1.01395378e+02]],\n",
       "\n",
       "       [[ 1.57026047e+02]],\n",
       "\n",
       "       [[ 1.41979584e+02]],\n",
       "\n",
       "       [[ 1.23809715e+02]],\n",
       "\n",
       "       [[ 1.11609863e+02]],\n",
       "\n",
       "       [[ 1.27601906e+02]],\n",
       "\n",
       "       [[ 1.17518959e+02]],\n",
       "\n",
       "       [[ 1.24051277e+02]],\n",
       "\n",
       "       [[ 1.06199142e+02]],\n",
       "\n",
       "       [[ 7.76711273e+01]],\n",
       "\n",
       "       [[ 6.18151550e+01]],\n",
       "\n",
       "       [[ 5.29124031e+01]],\n",
       "\n",
       "       [[ 5.19011497e+01]],\n",
       "\n",
       "       [[ 6.71954651e+01]],\n",
       "\n",
       "       [[ 9.31106720e+01]],\n",
       "\n",
       "       [[ 9.72880020e+01]],\n",
       "\n",
       "       [[ 8.59017792e+01]],\n",
       "\n",
       "       [[ 7.87190628e+01]],\n",
       "\n",
       "       [[ 6.49929047e+01]],\n",
       "\n",
       "       [[ 8.77471848e+01]],\n",
       "\n",
       "       [[ 1.01944168e+02]],\n",
       "\n",
       "       [[ 1.25099030e+02]],\n",
       "\n",
       "       [[ 8.47806931e+01]],\n",
       "\n",
       "       [[ 8.92177048e+01]],\n",
       "\n",
       "       [[ 9.09696960e+01]],\n",
       "\n",
       "       [[ 8.21900711e+01]],\n",
       "\n",
       "       [[ 8.17849960e+01]],\n",
       "\n",
       "       [[ 9.22699966e+01]],\n",
       "\n",
       "       [[ 1.14600449e+02]],\n",
       "\n",
       "       [[ 9.15458832e+01]],\n",
       "\n",
       "       [[ 8.59188766e+01]],\n",
       "\n",
       "       [[ 1.01573196e+02]],\n",
       "\n",
       "       [[ 5.25253181e+01]],\n",
       "\n",
       "       [[ 6.01565895e+01]],\n",
       "\n",
       "       [[ 6.45853424e+01]],\n",
       "\n",
       "       [[ 6.98647995e+01]],\n",
       "\n",
       "       [[ 8.72563019e+01]],\n",
       "\n",
       "       [[ 8.40762100e+01]],\n",
       "\n",
       "       [[ 9.52332153e+01]],\n",
       "\n",
       "       [[ 9.43758698e+01]],\n",
       "\n",
       "       [[ 8.57120743e+01]],\n",
       "\n",
       "       [[ 9.58737564e+01]],\n",
       "\n",
       "       [[ 1.10470062e+02]],\n",
       "\n",
       "       [[ 1.53266846e+02]],\n",
       "\n",
       "       [[ 9.89670029e+01]],\n",
       "\n",
       "       [[ 1.06568214e+02]],\n",
       "\n",
       "       [[ 1.21532356e+02]],\n",
       "\n",
       "       [[ 1.27881172e+02]],\n",
       "\n",
       "       [[ 1.08496925e+02]],\n",
       "\n",
       "       [[ 8.94278030e+01]],\n",
       "\n",
       "       [[ 1.29262527e+02]],\n",
       "\n",
       "       [[ 9.54541931e+01]],\n",
       "\n",
       "       [[ 1.19957481e+02]],\n",
       "\n",
       "       [[ 9.46688080e+01]],\n",
       "\n",
       "       [[ 9.81010971e+01]],\n",
       "\n",
       "       [[ 1.63361633e+02]],\n",
       "\n",
       "       [[ 2.67347656e+02]],\n",
       "\n",
       "       [[ 1.23852142e+02]],\n",
       "\n",
       "       [[ 7.63509827e+01]],\n",
       "\n",
       "       [[ 1.00011589e+02]],\n",
       "\n",
       "       [[ 7.54914169e+01]],\n",
       "\n",
       "       [[ 5.20244942e+01]],\n",
       "\n",
       "       [[ 8.17236252e+01]],\n",
       "\n",
       "       [[ 6.19265480e+01]],\n",
       "\n",
       "       [[ 5.43868790e+01]],\n",
       "\n",
       "       [[ 8.91391449e+01]],\n",
       "\n",
       "       [[ 9.10989304e+01]],\n",
       "\n",
       "       [[ 5.51498985e+01]],\n",
       "\n",
       "       [[ 1.19305595e+02]],\n",
       "\n",
       "       [[ 1.42184814e+02]],\n",
       "\n",
       "       [[ 1.31107422e+02]],\n",
       "\n",
       "       [[ 1.40942017e+02]],\n",
       "\n",
       "       [[ 1.35246857e+02]],\n",
       "\n",
       "       [[ 1.85205276e+02]],\n",
       "\n",
       "       [[ 7.93471985e+01]],\n",
       "\n",
       "       [[ 7.55127411e+01]],\n",
       "\n",
       "       [[ 9.26910858e+01]],\n",
       "\n",
       "       [[ 1.14886261e+02]],\n",
       "\n",
       "       [[ 1.44393433e+02]],\n",
       "\n",
       "       [[ 9.93454437e+01]],\n",
       "\n",
       "       [[ 6.79727554e+01]],\n",
       "\n",
       "       [[ 7.34679642e+01]],\n",
       "\n",
       "       [[ 6.53764420e+01]],\n",
       "\n",
       "       [[ 9.26139679e+01]],\n",
       "\n",
       "       [[ 1.50027954e+02]],\n",
       "\n",
       "       [[ 8.30521469e+01]],\n",
       "\n",
       "       [[ 7.65266190e+01]],\n",
       "\n",
       "       [[ 1.08214020e+02]],\n",
       "\n",
       "       [[ 1.06461052e+02]],\n",
       "\n",
       "       [[ 8.76300735e+01]],\n",
       "\n",
       "       [[ 1.37467361e+02]],\n",
       "\n",
       "       [[ 1.49599182e+02]],\n",
       "\n",
       "       [[ 1.51255798e+02]],\n",
       "\n",
       "       [[ 8.24382401e+01]],\n",
       "\n",
       "       [[ 1.06279312e+02]],\n",
       "\n",
       "       [[ 1.12257744e+02]],\n",
       "\n",
       "       [[ 1.03231018e+02]],\n",
       "\n",
       "       [[ 1.09732674e+02]],\n",
       "\n",
       "       [[ 1.48229828e+02]],\n",
       "\n",
       "       [[ 1.92485031e+02]],\n",
       "\n",
       "       [[ 1.39499451e+02]],\n",
       "\n",
       "       [[ 6.96409607e+01]],\n",
       "\n",
       "       [[ 1.32262115e+02]],\n",
       "\n",
       "       [[ 9.04013214e+01]],\n",
       "\n",
       "       [[ 1.07822739e+02]],\n",
       "\n",
       "       [[ 1.21390892e+02]],\n",
       "\n",
       "       [[ 1.08661674e+02]],\n",
       "\n",
       "       [[ 1.08843910e+02]],\n",
       "\n",
       "       [[ 1.36362000e+02]],\n",
       "\n",
       "       [[ 8.80490646e+01]],\n",
       "\n",
       "       [[ 5.47201157e+01]],\n",
       "\n",
       "       [[ 1.12075592e+02]],\n",
       "\n",
       "       [[ 1.30671829e+02]],\n",
       "\n",
       "       [[ 1.31585037e+02]],\n",
       "\n",
       "       [[ 1.67981140e+02]],\n",
       "\n",
       "       [[ 1.14696976e+02]],\n",
       "\n",
       "       [[ 9.31202240e+01]],\n",
       "\n",
       "       [[ 9.01642380e+01]],\n",
       "\n",
       "       [[ 1.19013794e+02]],\n",
       "\n",
       "       [[ 1.23261330e+02]],\n",
       "\n",
       "       [[ 1.16340668e+02]],\n",
       "\n",
       "       [[ 9.24980469e+01]],\n",
       "\n",
       "       [[ 7.63183289e+01]],\n",
       "\n",
       "       [[ 6.70144196e+01]],\n",
       "\n",
       "       [[ 8.75332108e+01]],\n",
       "\n",
       "       [[ 1.11666870e+02]],\n",
       "\n",
       "       [[ 1.11449844e+02]],\n",
       "\n",
       "       [[ 9.97361298e+01]],\n",
       "\n",
       "       [[ 8.23700943e+01]],\n",
       "\n",
       "       [[ 7.08846893e+01]],\n",
       "\n",
       "       [[ 4.48492851e+01]],\n",
       "\n",
       "       [[ 8.78082428e+01]],\n",
       "\n",
       "       [[ 1.04091606e+02]],\n",
       "\n",
       "       [[ 1.30198685e+02]],\n",
       "\n",
       "       [[ 1.30318314e+02]],\n",
       "\n",
       "       [[ 8.96596375e+01]],\n",
       "\n",
       "       [[ 9.03188400e+01]],\n",
       "\n",
       "       [[ 9.07989120e+01]],\n",
       "\n",
       "       [[ 7.23988419e+01]],\n",
       "\n",
       "       [[ 7.90964050e+01]],\n",
       "\n",
       "       [[ 6.64768143e+01]],\n",
       "\n",
       "       [[ 5.84519501e+01]],\n",
       "\n",
       "       [[ 6.38976860e+01]],\n",
       "\n",
       "       [[ 8.10775452e+01]],\n",
       "\n",
       "       [[ 7.04748840e+01]],\n",
       "\n",
       "       [[ 8.66390076e+01]],\n",
       "\n",
       "       [[ 6.64913254e+01]],\n",
       "\n",
       "       [[ 5.70698471e+01]],\n",
       "\n",
       "       [[ 6.45193863e+01]],\n",
       "\n",
       "       [[ 7.44362640e+01]],\n",
       "\n",
       "       [[ 7.20655975e+01]],\n",
       "\n",
       "       [[ 6.71872177e+01]],\n",
       "\n",
       "       [[ 6.64118652e+01]],\n",
       "\n",
       "       [[ 7.22810745e+01]],\n",
       "\n",
       "       [[ 8.19558945e+01]],\n",
       "\n",
       "       [[ 8.37632523e+01]],\n",
       "\n",
       "       [[ 7.28153076e+01]],\n",
       "\n",
       "       [[ 6.54129791e+01]],\n",
       "\n",
       "       [[ 4.97942924e+01]],\n",
       "\n",
       "       [[ 6.07554436e+01]],\n",
       "\n",
       "       [[ 7.10011139e+01]],\n",
       "\n",
       "       [[ 4.73505936e+01]],\n",
       "\n",
       "       [[ 5.90446739e+01]],\n",
       "\n",
       "       [[ 7.02025528e+01]],\n",
       "\n",
       "       [[ 7.96057281e+01]],\n",
       "\n",
       "       [[ 5.10587425e+01]],\n",
       "\n",
       "       [[ 3.31548653e+01]],\n",
       "\n",
       "       [[ 2.90703831e+01]],\n",
       "\n",
       "       [[ 1.91709499e+01]],\n",
       "\n",
       "       [[ 3.94150467e+01]],\n",
       "\n",
       "       [[ 4.58303261e+01]],\n",
       "\n",
       "       [[ 4.13803596e+01]],\n",
       "\n",
       "       [[ 4.71230698e+01]],\n",
       "\n",
       "       [[ 4.36904106e+01]],\n",
       "\n",
       "       [[ 6.27187920e+01]],\n",
       "\n",
       "       [[ 4.83361664e+01]],\n",
       "\n",
       "       [[ 5.77304840e+01]],\n",
       "\n",
       "       [[ 4.16475449e+01]],\n",
       "\n",
       "       [[ 5.85173721e+01]],\n",
       "\n",
       "       [[ 6.57652512e+01]],\n",
       "\n",
       "       [[ 5.09367447e+01]],\n",
       "\n",
       "       [[ 7.36017456e+01]],\n",
       "\n",
       "       [[ 8.30840836e+01]],\n",
       "\n",
       "       [[ 9.58029709e+01]],\n",
       "\n",
       "       [[ 7.19535141e+01]],\n",
       "\n",
       "       [[ 2.43810165e+02]],\n",
       "\n",
       "       [[ 4.49411354e+01]],\n",
       "\n",
       "       [[ 3.46787071e+01]],\n",
       "\n",
       "       [[ 4.71982346e+01]],\n",
       "\n",
       "       [[ 5.73313065e+01]],\n",
       "\n",
       "       [[ 4.22572403e+01]],\n",
       "\n",
       "       [[ 4.12559853e+01]],\n",
       "\n",
       "       [[ 5.61224518e+01]],\n",
       "\n",
       "       [[ 6.32954483e+01]],\n",
       "\n",
       "       [[ 8.76866913e+01]],\n",
       "\n",
       "       [[ 2.95829193e+02]],\n",
       "\n",
       "       [[ 4.62514839e+01]],\n",
       "\n",
       "       [[ 5.09218025e+01]],\n",
       "\n",
       "       [[ 6.95462265e+01]],\n",
       "\n",
       "       [[ 6.54359207e+01]],\n",
       "\n",
       "       [[ 2.80422306e+01]],\n",
       "\n",
       "       [[ 1.88754349e+01]],\n",
       "\n",
       "       [[ 5.61044006e+01]],\n",
       "\n",
       "       [[ 5.94237900e+01]],\n",
       "\n",
       "       [[ 5.33132286e+01]],\n",
       "\n",
       "       [[ 8.69808655e+01]],\n",
       "\n",
       "       [[ 6.54859848e+01]],\n",
       "\n",
       "       [[ 2.27845402e+01]],\n",
       "\n",
       "       [[ 4.56279144e+01]],\n",
       "\n",
       "       [[ 7.69524765e+01]],\n",
       "\n",
       "       [[ 8.88476562e+01]],\n",
       "\n",
       "       [[ 6.37304726e+01]],\n",
       "\n",
       "       [[ 3.34044266e+01]],\n",
       "\n",
       "       [[ 3.95007629e+01]],\n",
       "\n",
       "       [[ 5.20531540e+01]],\n",
       "\n",
       "       [[ 6.03470573e+01]],\n",
       "\n",
       "       [[ 4.53593369e+01]],\n",
       "\n",
       "       [[ 5.35648689e+01]],\n",
       "\n",
       "       [[ 4.51556892e+01]],\n",
       "\n",
       "       [[-1.76142347e+00]],\n",
       "\n",
       "       [[ 4.94794350e+01]],\n",
       "\n",
       "       [[ 5.01328239e+01]],\n",
       "\n",
       "       [[ 5.14150276e+01]],\n",
       "\n",
       "       [[ 5.44530830e+01]],\n",
       "\n",
       "       [[ 6.71738281e+01]],\n",
       "\n",
       "       [[ 7.82723389e+01]],\n",
       "\n",
       "       [[ 1.04444727e+03]],\n",
       "\n",
       "       [[ 2.80943750e+03]],\n",
       "\n",
       "       [[ 8.69086075e+01]],\n",
       "\n",
       "       [[ 1.42356358e+01]],\n",
       "\n",
       "       [[ 1.53107719e+01]],\n",
       "\n",
       "       [[ 2.52120934e+01]],\n",
       "\n",
       "       [[ 4.21168747e+01]],\n",
       "\n",
       "       [[ 4.27660332e+01]],\n",
       "\n",
       "       [[ 4.53324242e+01]],\n",
       "\n",
       "       [[ 2.85709076e+01]],\n",
       "\n",
       "       [[ 2.15554924e+01]],\n",
       "\n",
       "       [[ 5.08092422e+01]],\n",
       "\n",
       "       [[ 6.12898407e+01]],\n",
       "\n",
       "       [[ 5.32435265e+01]],\n",
       "\n",
       "       [[ 7.90184937e+01]],\n",
       "\n",
       "       [[ 7.95026474e+01]],\n",
       "\n",
       "       [[ 5.56749611e+01]],\n",
       "\n",
       "       [[ 4.70047340e+01]],\n",
       "\n",
       "       [[ 6.41090240e+01]],\n",
       "\n",
       "       [[ 6.05955467e+01]],\n",
       "\n",
       "       [[ 5.14420357e+01]],\n",
       "\n",
       "       [[ 5.83612900e+01]],\n",
       "\n",
       "       [[ 4.86582031e+01]],\n",
       "\n",
       "       [[ 4.22360954e+01]],\n",
       "\n",
       "       [[ 5.13184547e+01]],\n",
       "\n",
       "       [[ 5.72340851e+01]],\n",
       "\n",
       "       [[ 6.05547028e+01]],\n",
       "\n",
       "       [[ 4.50615234e+01]],\n",
       "\n",
       "       [[ 4.85426674e+01]],\n",
       "\n",
       "       [[ 5.03063545e+01]],\n",
       "\n",
       "       [[ 5.12896080e+01]],\n",
       "\n",
       "       [[ 4.37459221e+01]],\n",
       "\n",
       "       [[ 5.27283936e+01]],\n",
       "\n",
       "       [[ 4.97810898e+01]],\n",
       "\n",
       "       [[ 5.85228691e+01]],\n",
       "\n",
       "       [[ 5.70363808e+01]],\n",
       "\n",
       "       [[ 5.37365150e+01]],\n",
       "\n",
       "       [[ 2.80204659e+01]],\n",
       "\n",
       "       [[ 1.79164162e+01]],\n",
       "\n",
       "       [[ 2.20414524e+01]],\n",
       "\n",
       "       [[ 4.80597229e+01]],\n",
       "\n",
       "       [[ 4.43699379e+01]],\n",
       "\n",
       "       [[ 3.91275673e+01]],\n",
       "\n",
       "       [[ 4.01271362e+01]],\n",
       "\n",
       "       [[ 2.91373253e+01]],\n",
       "\n",
       "       [[ 3.16578407e+01]],\n",
       "\n",
       "       [[ 4.66708832e+01]],\n",
       "\n",
       "       [[ 5.24511986e+01]],\n",
       "\n",
       "       [[ 5.12914429e+01]],\n",
       "\n",
       "       [[ 4.75626602e+01]],\n",
       "\n",
       "       [[ 4.07643661e+01]],\n",
       "\n",
       "       [[ 4.74586983e+01]],\n",
       "\n",
       "       [[ 3.46180534e+01]],\n",
       "\n",
       "       [[ 4.46394386e+01]],\n",
       "\n",
       "       [[ 4.94476891e+01]],\n",
       "\n",
       "       [[ 4.37208366e+01]],\n",
       "\n",
       "       [[ 4.55212440e+01]],\n",
       "\n",
       "       [[ 4.93185005e+01]],\n",
       "\n",
       "       [[ 4.79632721e+01]],\n",
       "\n",
       "       [[ 4.23386269e+01]],\n",
       "\n",
       "       [[ 5.75029755e+01]],\n",
       "\n",
       "       [[ 5.76901932e+01]],\n",
       "\n",
       "       [[ 6.29675140e+01]],\n",
       "\n",
       "       [[ 6.15370216e+01]],\n",
       "\n",
       "       [[ 4.86367798e+01]],\n",
       "\n",
       "       [[ 3.83759689e+01]],\n",
       "\n",
       "       [[ 2.37809086e+01]],\n",
       "\n",
       "       [[ 6.47558517e+01]],\n",
       "\n",
       "       [[ 5.52343826e+01]],\n",
       "\n",
       "       [[ 4.47293968e+01]],\n",
       "\n",
       "       [[ 4.37380447e+01]],\n",
       "\n",
       "       [[ 3.84355049e+01]],\n",
       "\n",
       "       [[ 1.99341049e+01]],\n",
       "\n",
       "       [[ 4.59820518e+01]],\n",
       "\n",
       "       [[ 5.66574173e+01]],\n",
       "\n",
       "       [[ 4.08385544e+01]],\n",
       "\n",
       "       [[ 2.77059956e+01]],\n",
       "\n",
       "       [[ 2.98014488e+01]],\n",
       "\n",
       "       [[ 3.07827034e+01]],\n",
       "\n",
       "       [[ 4.26608658e+01]],\n",
       "\n",
       "       [[ 3.57034912e+01]],\n",
       "\n",
       "       [[ 2.56158886e+01]],\n",
       "\n",
       "       [[ 3.30358849e+01]],\n",
       "\n",
       "       [[ 3.78817520e+01]],\n",
       "\n",
       "       [[ 2.04965992e+01]],\n",
       "\n",
       "       [[ 2.66411800e+01]],\n",
       "\n",
       "       [[ 1.76401749e+01]],\n",
       "\n",
       "       [[ 1.35630560e+01]],\n",
       "\n",
       "       [[ 3.30314102e+01]],\n",
       "\n",
       "       [[ 1.95666885e+01]],\n",
       "\n",
       "       [[ 3.22651901e+01]],\n",
       "\n",
       "       [[ 3.04134636e+01]],\n",
       "\n",
       "       [[ 2.41228161e+01]],\n",
       "\n",
       "       [[ 2.28756447e+01]],\n",
       "\n",
       "       [[ 3.64885330e+01]],\n",
       "\n",
       "       [[ 5.30634422e+01]],\n",
       "\n",
       "       [[ 3.35157013e+01]],\n",
       "\n",
       "       [[ 1.70648785e+01]],\n",
       "\n",
       "       [[ 1.52379017e+01]],\n",
       "\n",
       "       [[ 3.06982193e+01]],\n",
       "\n",
       "       [[ 2.24615517e+01]],\n",
       "\n",
       "       [[ 3.64563980e+01]],\n",
       "\n",
       "       [[ 3.78578987e+01]],\n",
       "\n",
       "       [[ 2.94336243e+01]],\n",
       "\n",
       "       [[ 4.34388695e+01]],\n",
       "\n",
       "       [[ 4.81187248e+01]],\n",
       "\n",
       "       [[ 5.10777740e+01]],\n",
       "\n",
       "       [[ 4.65290947e+01]],\n",
       "\n",
       "       [[ 4.71572800e+01]],\n",
       "\n",
       "       [[ 4.44910851e+01]],\n",
       "\n",
       "       [[ 4.43748055e+01]],\n",
       "\n",
       "       [[ 4.00568924e+01]],\n",
       "\n",
       "       [[ 5.80748749e+01]],\n",
       "\n",
       "       [[ 6.96612244e+01]],\n",
       "\n",
       "       [[ 4.06051636e+01]],\n",
       "\n",
       "       [[ 3.99931030e+01]],\n",
       "\n",
       "       [[ 4.67848892e+01]],\n",
       "\n",
       "       [[ 4.67074661e+01]],\n",
       "\n",
       "       [[ 5.04431076e+01]],\n",
       "\n",
       "       [[ 6.53603821e+01]],\n",
       "\n",
       "       [[ 5.01429443e+01]],\n",
       "\n",
       "       [[ 2.17552967e+01]],\n",
       "\n",
       "       [[ 3.02999306e+01]],\n",
       "\n",
       "       [[ 3.51509590e+01]],\n",
       "\n",
       "       [[ 4.03045464e+01]],\n",
       "\n",
       "       [[ 4.61783371e+01]],\n",
       "\n",
       "       [[ 5.24117432e+01]],\n",
       "\n",
       "       [[ 6.20334435e+01]],\n",
       "\n",
       "       [[ 5.18052979e+01]],\n",
       "\n",
       "       [[ 5.25677452e+01]],\n",
       "\n",
       "       [[ 6.49355240e+01]],\n",
       "\n",
       "       [[ 1.34747620e+02]],\n",
       "\n",
       "       [[ 1.21177666e+02]],\n",
       "\n",
       "       [[ 9.99143143e+01]],\n",
       "\n",
       "       [[ 5.02505722e+01]],\n",
       "\n",
       "       [[ 3.14482708e+01]],\n",
       "\n",
       "       [[ 2.48796463e+01]],\n",
       "\n",
       "       [[ 2.60068798e+01]],\n",
       "\n",
       "       [[ 3.42309113e+01]],\n",
       "\n",
       "       [[ 6.67836227e+01]],\n",
       "\n",
       "       [[ 3.51429291e+01]],\n",
       "\n",
       "       [[ 3.05187073e+01]],\n",
       "\n",
       "       [[ 3.40593338e+01]],\n",
       "\n",
       "       [[ 3.69943771e+01]],\n",
       "\n",
       "       [[ 4.58961906e+01]],\n",
       "\n",
       "       [[ 4.89867439e+01]],\n",
       "\n",
       "       [[ 5.67214355e+01]],\n",
       "\n",
       "       [[ 3.99100418e+01]],\n",
       "\n",
       "       [[ 5.55369606e+01]],\n",
       "\n",
       "       [[ 4.83032837e+01]],\n",
       "\n",
       "       [[ 4.91987610e+01]],\n",
       "\n",
       "       [[ 4.27245522e+01]],\n",
       "\n",
       "       [[ 3.24633751e+01]],\n",
       "\n",
       "       [[ 3.31584320e+01]],\n",
       "\n",
       "       [[ 3.73664246e+01]],\n",
       "\n",
       "       [[ 4.35120201e+01]],\n",
       "\n",
       "       [[ 5.00261002e+01]],\n",
       "\n",
       "       [[ 5.07877121e+01]],\n",
       "\n",
       "       [[ 7.59606247e+01]],\n",
       "\n",
       "       [[ 8.83362427e+01]],\n",
       "\n",
       "       [[ 8.03881989e+01]],\n",
       "\n",
       "       [[ 7.07162781e+01]],\n",
       "\n",
       "       [[ 6.77398834e+01]],\n",
       "\n",
       "       [[ 8.19298859e+01]],\n",
       "\n",
       "       [[ 6.17194824e+01]],\n",
       "\n",
       "       [[ 6.11231689e+01]],\n",
       "\n",
       "       [[ 6.11123352e+01]],\n",
       "\n",
       "       [[ 6.85698090e+01]],\n",
       "\n",
       "       [[ 8.66582184e+01]],\n",
       "\n",
       "       [[ 7.38155060e+01]],\n",
       "\n",
       "       [[ 4.35343170e+01]],\n",
       "\n",
       "       [[ 3.12087688e+01]],\n",
       "\n",
       "       [[ 5.80201187e+01]],\n",
       "\n",
       "       [[ 9.51377792e+01]],\n",
       "\n",
       "       [[ 9.38750305e+01]],\n",
       "\n",
       "       [[ 1.03916023e+02]],\n",
       "\n",
       "       [[ 6.14491386e+01]],\n",
       "\n",
       "       [[ 7.20830078e+01]],\n",
       "\n",
       "       [[ 6.99228287e+01]],\n",
       "\n",
       "       [[ 5.94864273e+01]],\n",
       "\n",
       "       [[ 5.90447769e+01]],\n",
       "\n",
       "       [[ 6.75298309e+01]],\n",
       "\n",
       "       [[ 6.32232170e+01]],\n",
       "\n",
       "       [[ 7.30822754e+01]],\n",
       "\n",
       "       [[ 4.79158936e+01]],\n",
       "\n",
       "       [[ 5.02479553e+01]],\n",
       "\n",
       "       [[ 4.76889725e+01]],\n",
       "\n",
       "       [[ 4.91895943e+01]],\n",
       "\n",
       "       [[ 5.54647141e+01]],\n",
       "\n",
       "       [[ 6.94337082e+01]],\n",
       "\n",
       "       [[ 6.01194305e+01]],\n",
       "\n",
       "       [[ 5.35186615e+01]],\n",
       "\n",
       "       [[ 4.55221710e+01]],\n",
       "\n",
       "       [[ 5.34204330e+01]],\n",
       "\n",
       "       [[ 4.23286514e+01]],\n",
       "\n",
       "       [[ 5.86290092e+01]],\n",
       "\n",
       "       [[ 5.23201561e+01]],\n",
       "\n",
       "       [[ 6.98418350e+01]],\n",
       "\n",
       "       [[ 5.25150452e+01]],\n",
       "\n",
       "       [[ 5.84251976e+01]],\n",
       "\n",
       "       [[ 6.07105713e+01]],\n",
       "\n",
       "       [[ 4.40241661e+01]],\n",
       "\n",
       "       [[ 6.21232071e+01]],\n",
       "\n",
       "       [[ 5.41135521e+01]],\n",
       "\n",
       "       [[ 4.85083046e+01]],\n",
       "\n",
       "       [[ 5.60654907e+01]],\n",
       "\n",
       "       [[ 5.39379501e+01]],\n",
       "\n",
       "       [[ 1.34488419e+02]],\n",
       "\n",
       "       [[ 8.27442093e+01]],\n",
       "\n",
       "       [[ 9.06668854e+01]],\n",
       "\n",
       "       [[ 4.55643730e+01]],\n",
       "\n",
       "       [[ 6.16088486e+01]],\n",
       "\n",
       "       [[ 1.52922421e+01]],\n",
       "\n",
       "       [[ 9.42101860e+00]],\n",
       "\n",
       "       [[ 5.85526276e+01]],\n",
       "\n",
       "       [[ 5.22816238e+01]],\n",
       "\n",
       "       [[ 3.17696953e+01]],\n",
       "\n",
       "       [[ 2.55671139e+01]],\n",
       "\n",
       "       [[ 3.70425682e+01]],\n",
       "\n",
       "       [[ 4.97384529e+01]],\n",
       "\n",
       "       [[ 2.65171604e+01]],\n",
       "\n",
       "       [[ 1.89075241e+01]],\n",
       "\n",
       "       [[ 3.59019547e+01]],\n",
       "\n",
       "       [[ 3.70154343e+01]],\n",
       "\n",
       "       [[ 3.11528168e+01]],\n",
       "\n",
       "       [[ 2.23864689e+01]],\n",
       "\n",
       "       [[ 3.42641678e+01]],\n",
       "\n",
       "       [[ 2.83276901e+01]],\n",
       "\n",
       "       [[ 4.18521271e+01]],\n",
       "\n",
       "       [[ 4.43053589e+01]],\n",
       "\n",
       "       [[ 3.49684105e+01]],\n",
       "\n",
       "       [[ 3.46966782e+01]],\n",
       "\n",
       "       [[ 4.08370438e+01]],\n",
       "\n",
       "       [[ 2.42653084e+01]],\n",
       "\n",
       "       [[ 2.30513344e+01]],\n",
       "\n",
       "       [[ 3.42536430e+01]],\n",
       "\n",
       "       [[ 2.59253235e+01]],\n",
       "\n",
       "       [[ 3.42117958e+01]],\n",
       "\n",
       "       [[ 5.17793579e+01]],\n",
       "\n",
       "       [[ 5.34292107e+01]],\n",
       "\n",
       "       [[ 3.34447098e+01]],\n",
       "\n",
       "       [[ 5.79200287e+01]],\n",
       "\n",
       "       [[ 5.97460518e+01]],\n",
       "\n",
       "       [[ 4.21201782e+01]],\n",
       "\n",
       "       [[ 3.80576401e+01]],\n",
       "\n",
       "       [[ 3.46546707e+01]],\n",
       "\n",
       "       [[-6.07602835e+00]],\n",
       "\n",
       "       [[-1.98347068e+00]],\n",
       "\n",
       "       [[ 2.50086136e+01]],\n",
       "\n",
       "       [[ 3.67647018e+01]],\n",
       "\n",
       "       [[ 7.57710571e+01]]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c58bc1c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  1.       ,   0.       ,  62.27267  , ...,   5.1      ,\n",
       "          15.6      , 112.501495 ],\n",
       "        [  1.       ,   0.       , 112.501495 , ...,   5.9      ,\n",
       "           7.2      , 110.05619  ],\n",
       "        [  1.       ,   0.       , 110.05619  , ...,   9.       ,\n",
       "           0.4      ,  91.289345 ],\n",
       "        ...,\n",
       "        [  1.       ,   0.       ,  79.75906  , ...,   8.6      ,\n",
       "           0.       ,  69.127754 ],\n",
       "        [  1.       ,   0.       ,  69.127754 , ...,  13.2      ,\n",
       "           0.2      ,  67.79661  ],\n",
       "        [  1.       ,   0.       ,  67.79661  , ...,  10.1      ,\n",
       "           0.       ,  95.267105 ]],\n",
       "\n",
       "       [[  1.       ,   0.       , 112.501495 , ...,   5.9      ,\n",
       "           7.2      , 110.05619  ],\n",
       "        [  1.       ,   0.       , 110.05619  , ...,   9.       ,\n",
       "           0.4      ,  91.289345 ],\n",
       "        [  1.       ,   0.       ,  91.289345 , ...,   8.       ,\n",
       "           0.2      , 141.55698  ],\n",
       "        ...,\n",
       "        [  1.       ,   0.       ,  69.127754 , ...,  13.2      ,\n",
       "           0.2      ,  67.79661  ],\n",
       "        [  1.       ,   0.       ,  67.79661  , ...,  10.1      ,\n",
       "           0.       ,  95.267105 ],\n",
       "        [  1.       ,   0.       ,  95.267105 , ...,   9.2      ,\n",
       "           0.2      ,  77.61777  ]],\n",
       "\n",
       "       [[  1.       ,   0.       , 110.05619  , ...,   9.       ,\n",
       "           0.4      ,  91.289345 ],\n",
       "        [  1.       ,   0.       ,  91.289345 , ...,   8.       ,\n",
       "           0.2      , 141.55698  ],\n",
       "        [  1.       ,   0.       , 141.55698  , ...,   9.1      ,\n",
       "           0.       , 131.56213  ],\n",
       "        ...,\n",
       "        [  1.       ,   0.       ,  67.79661  , ...,  10.1      ,\n",
       "           0.       ,  95.267105 ],\n",
       "        [  1.       ,   0.       ,  95.267105 , ...,   9.2      ,\n",
       "           0.2      ,  77.61777  ],\n",
       "        [  1.       ,   0.       ,  77.61777  , ...,  12.2      ,\n",
       "           0.       ,  38.301147 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[  0.       ,   0.       ,  49.189594 , ...,  10.       ,\n",
       "           1.2      ,  55.464714 ],\n",
       "        [  0.       ,   0.       ,  55.464714 , ...,   9.6      ,\n",
       "           5.8      ,  69.43371  ],\n",
       "        [  0.       ,   0.       ,  69.43371  , ...,  10.8      ,\n",
       "           0.8      ,  60.11943  ],\n",
       "        ...,\n",
       "        [  0.       ,   0.       ,  38.05764  , ...,  13.       ,\n",
       "           0.6      ,  34.65467  ],\n",
       "        [  0.       ,   0.       ,  34.65467  , ...,  21.2      ,\n",
       "           1.8      ,  -6.0760283],\n",
       "        [  0.       ,   0.       ,  -6.0760283, ...,  22.       ,\n",
       "           0.       ,  -1.9834707]],\n",
       "\n",
       "       [[  0.       ,   0.       ,  55.464714 , ...,   9.6      ,\n",
       "           5.8      ,  69.43371  ],\n",
       "        [  0.       ,   0.       ,  69.43371  , ...,  10.8      ,\n",
       "           0.8      ,  60.11943  ],\n",
       "        [  0.       ,   0.       ,  60.11943  , ...,   5.8      ,\n",
       "           0.       ,  53.51866  ],\n",
       "        ...,\n",
       "        [  0.       ,   0.       ,  34.65467  , ...,  21.2      ,\n",
       "           1.8      ,  -6.0760283],\n",
       "        [  0.       ,   0.       ,  -6.0760283, ...,  22.       ,\n",
       "           0.       ,  -1.9834707],\n",
       "        [  0.       ,   0.       ,  -1.9834707, ...,  19.8      ,\n",
       "           0.       ,  25.008614 ]],\n",
       "\n",
       "       [[  0.       ,   0.       ,  69.43371  , ...,  10.8      ,\n",
       "           0.8      ,  60.11943  ],\n",
       "        [  0.       ,   0.       ,  60.11943  , ...,   5.8      ,\n",
       "           0.       ,  53.51866  ],\n",
       "        [  0.       ,   0.       ,  53.51866  , ...,   7.2      ,\n",
       "           6.8      ,  45.52217  ],\n",
       "        ...,\n",
       "        [  0.       ,   0.       ,  -6.0760283, ...,  22.       ,\n",
       "           0.       ,  -1.9834707],\n",
       "        [  0.       ,   0.       ,  -1.9834707, ...,  19.8      ,\n",
       "           0.       ,  25.008614 ],\n",
       "        [  0.       ,   0.       ,  25.008614 , ...,   8.4      ,\n",
       "           0.       ,  36.7647   ]]], shape=(782, 60, 9), dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fc827276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8353591d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e402bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874],\n",
       "       [29.725874]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7e6f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "84fff4e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Assume y_test is a pandas Series with date index, X_test is a DataFrame with date index\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# If y_test is not a Series with index, adjust accordingly\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Convert y_pred to Series with same index as y_test\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m y_pred_series \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(y_pred\u001b[38;5;241m.\u001b[39mflatten(), index\u001b[38;5;241m=\u001b[39m\u001b[43my_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m y_test_series \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(y_test, index\u001b[38;5;241m=\u001b[39my_test\u001b[38;5;241m.\u001b[39mindex, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mActual\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Combine into a DataFrame\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assume y_test is a pandas Series with date index, X_test is a DataFrame with date index\n",
    "# If y_test is not a Series with index, adjust accordingly\n",
    "\n",
    "# Convert y_pred to Series with same index as y_test\n",
    "y_pred_series = pd.Series(y_pred.flatten(), index=y_test.index, name=\"Predicted\")\n",
    "y_test_series = pd.Series(y_test, index=y_test.index, name=\"Actual\")\n",
    "\n",
    "# Combine into a DataFrame\n",
    "df_pred = pd.DataFrame({\"Actual\": y_test_series, \"Predicted\": y_pred_series})\n",
    "\n",
    "# Plot both against date\n",
    "df_pred.plot(figsize=(12, 6))\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"RRP\")\n",
    "plt.title(\"Actual vs Predicted RRP\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e636410f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.033061867627804054\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(f\"R2: {r2_score(y_test, y_pred)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "electricity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
